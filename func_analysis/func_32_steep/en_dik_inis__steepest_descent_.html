<h1>En Dik İniş (Steepest Descent)</h1>
<!DOCTYPE html>
<html>
  <head>
    <title>En Dik İniş (Steepest Descent)
</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {inlineMath: [["$","$"]]}
      });
    MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
      MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
        cancel: ["Extension","cancel"],
        bcancel: ["Extension","cancel"],
        xcancel: ["Extension","cancel"],
        cancelto: ["Extension","cancel"]
      });
    });
    </script>
<script type="text/javascript"
   src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML-full">
</script>
</head>

<p>Daha önce gradyan inişi konusunda işlediğimiz üzere bir $f$ fonksiyonu için
hesaplanan $-\nabla f(x)$ gradyanı $x$ noktasında fonksiyon için en yüksek
iniş (descent) olacak yönü gösteriyordu [1, sf. 151]. Fakat dikkat, {\em
  yön} kelimesini kullandık, o yönde ne kadar adım atılacağını
belirtmedik. Gradyanın temel hesabı türeve dayalı olduğu için ve türev
hesapladığı noktaya yakın bir yerde doğru bir yaklaşıklama olacağı için o
yönde atılan adımın büyüklüğüne göre minimizasyon iyi ya da kötü sonuçlar
verebilir. Bu sebeple gradyan inişi algoritmaları, ki</p>
<p>$$
x^{x+1} = x^k + \alpha_k \nabla f(x^k)
$$</p>
<p>ile kodlanırlar, çoğunlukla ufak ve pek çok adım atarlar, yani $\alpha_k$
sabitleri ufak seçilir. En Dik İniş (SD) algoritmasi bu noktada bir
ilerleme. Her $\alpha$, yani $\alpha_k$ öyle seçilir ki
$\phi(\alpha) \equiv f(x^k - \alpha \nabla f(x^k))$ kesinlikle minimize
edilsin / belli bir yöndeki en minimum noktaya vardıracak büyüklükte adım
atılsın. Ya da</p>
<p>$$
\alpha_k = \arg\min_{\alpha \ge 0} f(x^k - \alpha \nabla f(x^k))
$$</p>
<p>Yani gradyanın işaret ettiği yönde bir tür "arama" yapmış oluyoruz, adım
büyüklüğünü öyle seçiyoruz ki fonksiyon o yönde o kadar adım atıldığında en
fazla inişi gerçekleştirmiş olsun. Bu sebeple bu metota çizgi araması (line
search) metotu deniyor.</p>
<p>Tabii arama derken akla ikinci bir döngü içinde yine ufak ufak adımlar
atarak çizgi üzerinde gelinen yere bakıp büyüklük hesabını böyle yapmak
gelebilir, bu sonuçsal olarak, kabaca doğru, ama asıl adım hesabı bazı
cebirsel temellerle, ya da onu çözen yaklaşıksal şekilde yapılıyor. </p>
<p>En basiti atılan adım $\alpha$'yi pür cebirsel olarak çözmek, altta bir
örnek [3, sf. 101].</p>
<p>Soru</p>
<p>$f(x) = 9x_1^2 + 4x_1x_2 + 7x_2^2$ fonksiyonunun optimal noktasını bul.</p>
<p>Çözüm</p>
<p>Gradyanın öğeleri</p>
<p>$\frac{\partial f}{\partial x_1} = 18 x_1 + 4x_2$ ve 
$\frac{\partial  f}{\partial x_2} = 4 x_1 + 14 x_2$. Şimdi SD yöntemini
uygulayalım, başlangıç noktası $x^0 = [\begin{array}{cc} 1 &amp; 1 \end{array}]^T$
olsun. Bu durumda $f(x^0) = 20$, ve $\nabla f(x_0) = [\begin{array}{cc} 22 &amp; 18 \end{array}]^T$. 
Adım denklemine göre, </p>
<p>$$
x^1 = x^0 - \alpha_0 \nabla f(x^0)
$$</p>
<p>ya da </p>
<p>$$
\left[\begin{array}{c}
x_1 \\ x_2
\end{array}\right]
= 
\left[\begin{array}{c}
1 \\ 1
\end{array}\right]
\alpha_0 
\left[\begin{array}{c}
22 \\ 18
\end{array}\right]
$$</p>
<p>Şimdi öyle bir $\alpha_0$ seçmeliyiz ki $f(x^1)$ minimum olsun. Üstteki
değerlerin bize verdiği $x_1$ ve $x_2$ değerleri (ki $\alpha_0$ bazlı
olacaklar) ana formüle yeni $x$ olarak sokarsak, $\alpha_o$ bazlı bir
denklem edeceğiz,</p>
<p>$$
f(\alpha_0) = 20 - 808 \alpha_0 + 8208 (\alpha_0)^2
$$</p>
<p>$\frac{\mathrm{d} f(\alpha_0)}{\mathrm{d} \alpha_0} = 0$ üzerinden $\alpha_0$'nun optimum
değeri $0.05$'tır. Yani adımı şu şekilde atmalıyız,</p>
<p>$$
x^1 = 
\left[\begin{array}{c}
x_1 \\ x_2
\end{array}\right]
= 
\left[\begin{array}{c}
1 \\ 1
\end{array}\right]
0.05
\left[\begin{array}{c}
22 \\ 18
\end{array}\right]
$$</p>
<p>ki bu hesap bize $f(x^1) = 0.12$ verir. Bu şekilde özyineli döngüye devam
edersek nihai optimum noktayı buluruz.</p>
<p>Sekant Yöntemi</p>
<p>Basit cebirsel numaralar ile üstte adımı bulduk. Daha çetrefil durumlar
için sekant yöntemini kullanabiliriz. Bu yöntemi [2]'de işledik, ayrıca bkz
[1, sf. 120]. Sonuçta aradığımız $d$ yönündeki minimum</p>
<p>$$
\phi_k(\alpha) = f(x^k + \alpha d^k)
$$</p>
<p>değerini bulmaktır. Üstteki formülün $\alpha$ üzerinden türevi</p>
<p>$$
\phi_k'(\alpha) = {d^k}^T \nabla f(x^k + \alpha d^k) 
$$</p>
<p>O zaman minimum $\alpha$ icin </p>
<p>$$
0 = {d^k}^T \nabla f(x^k + \alpha d^k) 
$$</p>
<p>denklemini çözen $\alpha$ gerekli. Bu bir kök bulma problemi ve sekant
yöntemini kullanabiliriz. </p>
<pre><code class="python">def linesearch_secant(grad, d, x):
    epsilon=10**(-8)
    max = 500
    alpha_curr=0
    alpha=10**-8
    dphi_zero=np.dot(np.array(grad(x)).T,d)

    dphi_curr=dphi_zero
    i=0;
    while np.abs(dphi_curr)&gt;epsilon*np.abs(dphi_zero):
        alpha_old=alpha_curr
        alpha_curr=alpha
        dphi_old=dphi_curr        
        dphi_curr=np.dot(np.array(grad(x+alpha_curr*d)).T,d)
        alpha=(dphi_curr*alpha_old-dphi_old*alpha_curr)/(dphi_curr-dphi_old);
        i += 1
        if (i &gt;= max) and (np.abs(dphi_curr)&gt;epsilon*np.abs(dphi_zero)):
            print('Line search terminating with number of iterations:')
            print(i)
            print(alpha)
            break

    return alpha
</code></pre>

<p>Örnek</p>
<p>$f(x_1,x_2,x_3) = (x_1 - 4)^4 + (x_2 - 3)^2 + 4(x_3 + 5)^4$ fonksiyonunun
minimize edicisini bul.</p>
<p>Başlangıç noktamız $\left[\begin{array}{ccc} 4 &amp; 2 &amp; -1 \end{array}\right]^T$
olacak. </p>
<p>Üstteki fonksiyonun gradyanı</p>
<p>$$
\nabla f(x) = \left[\begin{array}{ccc}<br />
4(x_1-4)^3 &amp; 2(x_2-3) &amp; 16(x_3+5)^3
\end{array}\right]^T
$$</p>
<p>Kod olarak,</p>
<pre><code class="python">def g(x): return np.array([4*(x[0]-4)**3, 2*(x[1]-3), 16*(x[2]+5)**3])
</code></pre>

<p>$x^1$ hesaplamak için </p>
<p>$$
\alpha_0 = \arg\min_{\alpha \ge 0} f(x^0 - \alpha \nabla f(x^0))
$$</p>
<p>lazım, tam açılmış haliyle, </p>
<p>$$
= \arg\min_{\alpha \ge 0} (0 + (2+2\alpha-3)^2 + 4(-1-1024\alpha+5)^4
$$</p>
<p>Ama üstteki cebirle boğuşmaya gerek yok, gradyan fonksiyonu ve gidiş yönü
üzerinden kök bulup bize döndürecek üstteki çizgi araması kodunu
kullanabiliriz,</p>
<pre><code class="python">x0 = np.array([4,2,-1])
print (g(x0))
d0 = -g(x0)
alpha0 = linesearch_secant(g, d0, x0)
alpha0 = np.round(alpha0, 5)
print ('alpha0 =',alpha0)
x1 = x0 - alpha0*g(x0)
print ('x1',x1)
</code></pre>

<pre><code>[   0   -2 1024]
alpha0 = 0.00397
x1 [ 4.       2.00794 -5.06528]
</code></pre>

<p>Arka arkaya iki adım daha atarsak,</p>
<pre><code class="python">print ('g1',g(x1))
d1 = -g(x1)
alpha1 = linesearch_secant(g, d1, x1)
print (alpha1)
x2 = x1 - alpha1*g(x1)
print ('x2',x2)
print ('\n')
print ('g2',g(x2))
d2 = -g(x2)
alpha3 = linesearch_secant(g, d2, x2)
print (alpha3)
x3 = x2 - alpha3*g(x2)
print ('x3',x3)
</code></pre>

<pre><code>g1 [ 0.         -1.98412    -0.00445103]
0.5000022675782785
x2 [ 4.          3.0000045  -5.06305448]


g2 [ 0.00000000e+00  8.99829483e-06 -4.01113920e-03]
14.894217818923421
x3 [ 4.          2.99987048 -5.00331169]
</code></pre>

<p>Optimal noktaya erişmiş olduk.</p>
<p>Duruş Şartları </p>
<p>Optimizasyonda minimum varlığı için birinci-derecen gerekli şart
(first-order necessary condition -FONC-) minimumda $\nabla f(x) = 0$
olması. Eğer böyle bir noktaya erişmişsek, diyelim $x^k$ için
$\nabla f(x^k) = 0$ olmuş, bu nokta FONC'yi tatmin eder çünkü o zaman
$x^{k+1} = x^k$ olur, ve minimumdayız demektir. Bu teorik bilgiyi
algoritmamızın ne zaman duracağını anlaması için bir şart olarak kullanamaz
mıyız?</p>
<p>Ne yazık ki sayısal hesaplarda, yani pratikte $\nabla f(x^k) = 0$ hesabı
nadiren ortaya çıkar. Bir çözüm gradyanın normu $|| \nabla f(x) ||$ sıfır
olmasına bakmak. </p>
<p>Ya da $| f(x^{k+1}) - f(x^k) |$ mutlak değerine bakmak, yani hedef
fonksiyonun iki nokta arasındaki farkının mutlak değerine, bu değer eğer
daha önceden belirlenmiş bir eşik değeri $\epsilon$'un altına düşmüşse
durmak. Aynı şeyi $x^{n+1}$ ve $x^n$ değerlerinin kendisi için de
yapabiliriz.</p>
<p>Fakat bu yöntemler ölçek açısından problemli olabilir. Mesela 1 ve 1000
arasında gidip gelen $f(x)$'lerle 0 ve 1 arasında gidip gelen $f(x)$'lerin
kullanacağı $\epsilon$ farklı olabilir. Bir tanesi için $\epsilon = 100$
iyidir, diğeri için belki $\epsilon = 0.001$. Bu sebeple izafi bir hesap
daha faydalı olur, mesela</p>
<p>$$
\frac{|f(x^{k+1} - f(x^k))|}{|f(x^k)|} &lt; \epsilon
$$</p>
<p>ya da </p>
<p>$$
\frac{||x^{k+1} - x^k||}{||x^k||} &lt; \epsilon
$$</p>
<p>Üstteki yaklaşım "ölçekten bağımsız" olduğu için daha tercih edilir
yaklaşım, bir problemden diğerine geçtiğimizde farklı bir $\epsilon$
kullanmamız gerekmez.</p>
<p>Kaynaklar </p>
<p>[1] Zak, <em>An Introduction to Optimization, 4th Edition</em></p>
<p>[2] Bayramli, <em>Diferansiyel Denklemler, Kök Bulmak, Karesel Formül (Root Finding, Quadratic Formula)</em></p>
<p>[3] Dutta, <em>Optimization in Chemical Engineering</em></p>