<h1>En Dik İniş (Steepest Descent)</h1>
<!DOCTYPE html>
<html>
  <head>
    <title>En Dik İniş (Steepest Descent)
</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {inlineMath: [["$","$"]]}
      });
    MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
      MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
        cancel: ["Extension","cancel"],
        bcancel: ["Extension","cancel"],
        xcancel: ["Extension","cancel"],
        cancelto: ["Extension","cancel"]
      });
    });
    </script>
<script type="text/javascript"
   src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML-full">
</script>
</head>

<p>Daha önce gradyan inişi konusunda işlediğimiz üzere bir $f$ fonksiyonu için
hesaplanan $-\nabla f(x)$ gradyanı $x$ noktasında fonksiyon için en yüksek
iniş (descent) olacak yönü gösterir [1, sf. 151]. Dikkat, <em>yön</em>
kelimesini kullandık, o yönde ne kadar adım atılacağını
belirtmedik. Gradyanın temel hesabı türeve dayalı olduğu için ve türev
hesapladığı noktaya yakın bir yerde doğru bir yaklaşıklama olacağı için o
yönde atılan adımın büyüklüğüne göre minimizasyon doğru ya da yanlış yöne
gidiyor olabilir. Bu sebeple gradyan inişi algoritmaları, ki</p>
<p>$$
x^{x+1} = x^k + \alpha_k \nabla f(x^k)
$$</p>
<p>ile kodlanırlar, çoğunlukla pek çok ufak adım atarlar, yani $\alpha_k$
sabitleri ufak seçilir.</p>
<p>En Dik İniş (SD) algoritmasi bu noktada bir ilerleme sağlar. Her $\alpha$,
yani $\alpha_k$ öyle seçilir ki
$\phi(\alpha) \equiv f(x^k - \alpha \nabla f(x^k))$ minimize edilsin. Ya da</p>
<p>$$
\alpha_k = \arg\min_{\alpha \ge 0} f(x^k - \alpha \nabla f(x^k))
$$</p>
<p>Yani gradyanın işaret ettiği yönde bir tür "arama" yapmış oluyoruz, adım
büyüklüğünü öyle seçiyoruz ki fonksiyon o yönde o kadar adım atıldığında en
fazla inişi gerçekleştirmiş olsun. </p>
<p>Arama derken akla ikinci bir döngü içinde yine ufak ufak adımlar atarak
adım büyüklük hesabı yapmak geliyor, bu kabaca doğru, ama bize yardımcı
olacak bazı cebirsel numaralar var. En basiti atılan adım $\alpha$'yi
cebirsel olarak çözmek, altta bir örnek[3, sf. 101] var. </p>
<p>Soru</p>
<p>$f(x) = 9x_1^2 + 4x_1x_2 + 7x_2^2$ fonksiyonunun optimal noktasını bul.</p>
<p>Çözüm</p>
<p>Gradyanın öğeleri</p>
<p>$\frac{\partial f}{\partial x_1} = 18 x_1 + 4x_2$ ve 
$\frac{\partial  f}{\partial x_2} = 4 x_1 + 14 x_2$. Şimdi SD yöntemini
uygulayalım, başlangıç noktası $x^0 = \left[\begin{array}{cc} 1 &amp; 1 \end{array}\right]^T$
olsun. Bu durumda $f(x^0) = 20$, ve $\nabla f(x_0) = \left[\begin{array}{cc} 22 18 \end{array}\right]^T$. 
Adim denklemine gore, </p>
<p>$$
x^1 = x^0 - \alpha_0 \nabla f(x^0)
$$</p>
<p>ya da </p>
<p>$$
\left[\begin{array}{c}
x_1 \\ x_2
\end{array}\right]
= 
\left[\begin{array}{c}
1 \\ 1
\end{array}\right]
\alpha_0 
\left[\begin{array}{c}
22 \\ 18
\end{array}\right]
$$</p>
<p>Simdi oyle bir $\alpha_0$ secmeliyiz ki $f(x^1)$ minimum olsun. Ustteki
degerlerin bize verdigi $x_1$ ve $x_2$ degerleri (ki $\alpha_0$ bazli
degerler olacaklar) ana formule yeni $x$ olarak sokarsak, $\alpha_o$  bazli
bir denklem edecegiz, </p>
<p>$$
f(\alpha_0) = 20 - 808 \alpha_0 + 8208 (\alpha_0)^2
$$</p>
<p>Bu denkleme ve $\frac{\mathrm{d} f(\alpha_0)}{\mathrm{d} \alpha_0} = 0$ üzerinden
$\alpha_0$'nun optimum değeri $0.05$'tır. Yani adımı şu şekilde atmalıyız,</p>
<p>$$
x^1 = 
\left[\begin{array}{c}
x_1 \\ x_2
\end{array}\right]
= 
\left[\begin{array}{c}
1 \\ 1
\end{array}\right]
0.05
\left[\begin{array}{c}
22 \\ 18
\end{array}\right]
$$</p>
<p>ki bu hesap bize $f(x^1) = 0.12$ verir. Bu şekilde özyineli döngüye devam
edersek nihai optimum noktayı buluruz.</p>
<p>Sekant Yöntemi</p>
<p>Basit cebirsel numaralar ile üstte adımı bulduk. Daha çetrefil durumlar
için sekant yöntemini kullanabiliriz. Bu yöntemi [2]'de işledik, ayrıca bkz
[1, sf. 120]. Sonuçta aradığımız $d$ yönündeki minimum</p>
<p>$$
\phi_k(\alpha) = f(x^k + \alpha d^k)
$$</p>
<p>değerini bulmaktır. Üstteki formülün $\alpha$ üzerinden türevi</p>
<p>$$
\phi_k'(\alpha) = {d^k}^T \nabla f(x^k + \alpha d^k) 
$$</p>
<p>O zaman minimum $\alpha$ icin </p>
<p>$$
0 = {d^k}^T \nabla f(x^k + \alpha d^k) 
$$</p>
<p>denklemini çözen $\alpha$ gerekli. Bu bir kök bulma problemi ve sekant
yöntemini kullanabiliriz. </p>
<pre><code class="python">def linesearch_secant(grad, d, x):
    epsilon=10**(-8)
    max = 500
    alpha_curr=0
    alpha=10**-8
    dphi_zero=np.dot(np.array(grad(x)).T,d)

    dphi_curr=dphi_zero
    i=0;
    while np.abs(dphi_curr)&gt;epsilon*np.abs(dphi_zero):
        alpha_old=alpha_curr
        alpha_curr=alpha
        dphi_old=dphi_curr        
        dphi_curr=np.dot(np.array(grad(x+alpha_curr*d)).T,d)
        alpha=(dphi_curr*alpha_old-dphi_old*alpha_curr)/(dphi_curr-dphi_old);
        i += 1
        if (i &gt;= max) and (np.abs(dphi_curr)&gt;epsilon*np.abs(dphi_zero)):
            print('Line search terminating with number of iterations:')
            print(i)
            print(alpha)
            break

    return alpha
</code></pre>

<p>Örnek</p>
<p>$f(x_1,x_2,x_3) = (x_1 - 4)^4 + (x_2 - 3)^2 + 4(x_3 + 5)^4$ fonksiyonunun
minimize edicisini bul.</p>
<p>Başlangıç noktamız $\left[\begin{array}{ccc} 4 &amp; 2 &amp; -1 \end{array}\right]^T$
olacak. </p>
<p>Genel gradyan formülünü</p>
<p>$$
\nabla f(x) = \left[\begin{array}{ccc}<br />
4(x_1-4)^3 &amp; 2(x_2-3) &amp; 16(x_3+5)^3
\end{array}\right]^T
$$</p>
<p>olarak bulduk.</p>
<pre><code class="python">def g(x): return np.array([4*(x[0]-4)**3, 2*(x[1]-3), 16*(x[2]+5)**3])
</code></pre>

<p>$x^1$ hesaplamak için </p>
<p>$$
\alpha_0 = \arg\min_{\alpha \ge 0} f(x^0 - \alpha \nabla f(x^0))
$$</p>
<p>lazım, tam acilmis haliyle, </p>
<p>$$
= \arg\min_{\alpha \ge 0} (0 + (2+2\alpha-3)^2 + 4(-1-1024\alpha+5)^4
$$</p>
<p>Ama üstteki cebirle boğuşmaya gerek yok, gradyan fonksiyonu ve gidiş yönü
üzerinden kok bulup bize döndürecek üstteki hesabı kullanabiliriz,</p>
<pre><code class="python">x0 = np.array([4,2,-1])
print (g(x0))
d0 = -g(x0)
alpha0 = linesearch_secant(g, d0, x0)
alpha0 = np.round(alpha0, 5)
print ('alpha0 =',alpha0)
x1 = x0 - alpha0*g(x0)
print ('x1',x1)
</code></pre>

<pre><code>[   0   -2 1024]
alpha0 = 0.00397
x1 [ 4.       2.00794 -5.06528]
</code></pre>

<p>Arka arkaya iki adım daha atarsak,</p>
<pre><code class="python">print ('g1',g(x1))
d1 = -g(x1)
alpha1 = linesearch_secant(g, d1, x1)
print (alpha1)
x2 = x1 - alpha1*g(x1)
print ('x2',x2)
print ('\n')
print ('g2',g(x2))
d2 = -g(x2)
alpha3 = linesearch_secant(g, d2, x2)
print (alpha3)
x3 = x2 - alpha3*g(x2)
print ('x3',x3)
</code></pre>

<pre><code>g1 [ 0.         -1.98412    -0.00445103]
0.5000022675782785
x2 [ 4.          3.0000045  -5.06305448]


g2 [ 0.00000000e+00  8.99829483e-06 -4.01113920e-03]
14.894217818923421
x3 [ 4.          2.99987048 -5.00331169]
</code></pre>

<p>Optimal noktaya erişmiş olduk.</p>
<p>Kaynaklar </p>
<p>[1] Zak, <em>An Introduction to Optimization, 4th Edition</em></p>
<p>[2] Bayramli, <em>Diferansiyel Denklemler, Kök Bulmak, Karesel Formül (Root Finding, Quadratic Formula)</em></p>
<p>[3] Dutta, <em>Optimization in Chemical Engineering</em></p>