\documentclass[12pt,fleqn]{article}\usepackage{../../common}
\begin{document}
Dýþbükey Optimizasyonuna (Convex Optimization) Giriþ

Yapay öðrenme (machine learning) ve optimizasyonda sürekli optimizasyonu
görürüz. Diðer disiplenlerde de görülür tabii ama bu ikisi benim ana
konularým o yüzden o konulardan bu derste daha fazla bahsedeceðiz. Derste
belirli bir amaç için gereken optimizasyon problemini çözmekten çok
optimizasyon mekanizmasýnýn detaylarýný inceleyeceðiz. Optimallik
þartlarýna bakmak, varýlan çözümün niteliðine bakmak bu detaylardan
bazýlarý.

Þimdi aklýnýza gelen bazý optimizasyon örneklerini verin bana [öðrenciler
söylüyor]

1) Regresyon - En Az Kareler. Evet. Hata karelerinin toplamý minimize
edilir burada, bir hedef $y$ vardýr, onu bir formül üzerinden katsayýlarý
olan bir denklem vardýr, ve model uyum iyiliðini hata kare toplamý
üzerinden ölçeriz.

$$
\min_\beta \sum (y_i - x_i^T \beta)^2 
$$

Baþka ne tür regresyon þekilleri var?  

2) Regülarize Edilmiþ Regresyon - Lasso. Burada yine hata karelerin toplamý
var, ama üstüne katsayýlarýn L1 norm'unu minimize etmeye çalýþýrýz. Yani 

$$
\min_\beta \sum (y_i - x_i^T \beta)^2 \quad \textrm{oyle ki}
$$
$$
\sum |\beta| \le t
$$

3) En Az Mutlak Sapma Regresyonu (Least Absolute Deviations) - bu da
benden. Bu tür regresyon ile kare yerine mutlak deðer operasyonu
kullanýlýyor [1, 14:35].   

$$
\min_\beta \sum |y_i - x_i^T \beta |
$$

BU tür regresyon ile aykýrý (outlier) deðerlere daha az önem verilmiþ
olur. Fakat mutlak deðer hesabý kullanýnca optimizasyon zorlaþýyor çünkü
üstteki formül artýk pürüzsüz deðil. 

4) Sýnýflama - Lojistik Regresyon. LR ile $y_i$ ikisel olur, 0 ya da 1. LR
formülizasyonu normal regresyona benziyor, 

5) Bilgisayar Bilim - Seyahet Eden Satýþ Görevlisi Problemi (TSP),
Planlama, Ayrýksal Optimizasyon. Bu ders bloklarýnýn sonunda Tam Sayý
Programlama (Ýnteðer Programming) konusuna bakacaðýz, bu tür konulara orada
daha çok yaklaþmýþ olacaðýz. 

6) Ýstastistik - Maksimum Olurluk. MO istatistikte pek çok yaptýðýmýz iþin
mihenk taþýdýr. Hatta LR, En Az Kareler, vs aslinda MO'nun özel, spesifik
halleridir. Burada vurgu içbükey olurluk elde etmek, ki bir içbükey
fonksiyonu maksimize etmiþ olalým, bu bir dýþbükey fonksiyonu minimize
etmek ile ayný þey. 

Böyle devam edebilirdik, optimizasyon örnekleri sayfalar
doldurabilirdik. Optimizasyon her yerde. Ama belki de neyin optimizasyon
olmadýðýna da bakmak iyi olur. Mesela istatistikte optimizasyon olmayan
problemler nedir?

Hipotez test etmek, p-deðerleri. Ya da takviyelemek (boosting), önemli bir
konu ama optimizasyon deðil. Rasgele Ormanlar (Random Forests),
deðil. Önyükleyiciler (bootstrap), çapraz-saðlama (cross-validation), yine
deðil [1, 22:09].  

Ve iddiam þu ki optimizasyon olmayan konular hakkýnda olanlara kýyasla daha
fazla teorik bilgimiz var. Üstteki teknikler çoðunlukla prosedürsel. Ama
mesela Lasso diyelim, bu bir dýþbükey optimizasyonun çýktýsý olduðu için
optimalite þartlarý üzerinden onun çözümünün özellikleri hakkýnda konuþmak
kolaylaþýyor. 

Peki biz niye bu dersteki konuyu ogrenmek isteriz, isteyebiliriz? Sonucta
Lasso'yu birisi bulmus onun kodunu cagiririz, is biter. Uc sebep
var. Birincisi farkli algoritmalar duruma gore daha iyi performans
gosterebilir, durum derken veriden bahsediyorum. Bu sebeple her
algoritmanin ozunu anlamak cok onemli. Ikincisi herhangi bir alandaki
problemi cozen optimizasyonun temelini bilmek bize alan hakkinda ek gorus
kazandirabilir. 

Üçüncü sebep optimizasyon hýzlý hareket eden bir alan, eðlenceli! Mesela
optimizasyon alanýndaki NIPS Çalýþtayýna (Workshop) bakarsanýz, her sene
deðiþiyor! Birkaç sene önce dýþbükey olmayan optimizasyon büyük konuydu,
tabii o zaman bu dersi iþlerken utanýr gibi oluyorduk çünkü bizim konu
dýþbükey optimizasyon ve yapay öðrenimdeki en büyük konferansta dýþbükey
olmayan konular iþleniyor.. Fakat o zamanki odaðýn sebebi o zamanlarda bir
sürü yeni dýþbükey olmayan ve yakýnsadýðý ispat edilen metotlarýn bulunmuþ
olmasýydý. Ama bir sonraki sene rasgele (stochastic) optimizasyon geri
dönüþ yapmýþtý, rasgele gradyan iniþi vs. Böyle her sene deðiþim oluyor, bu
güzel bir þey demek ki hala ilerleme için oldukça alan var.

Ornekler

Bu orneklerin cogu tam varyasyon gurultu yoketmek (denoising) etrafinda,
bunun bir diger ismi kaynasmis (fused) lasso. Elimizde iki boyutlu izgara
halinde bir veri var, bir goruntu, $i,j$ kordinatlarinda bir renk degeri
var, 3 ile 7 arasindaki renkler. 

\includegraphics[width=25em]{func_19_intro_01.png}

En soldaki gerçek resim. Ortadaki ise onun gürültülü hali, bizim elimizdeki
veri bu diyelim. Görüntüyü $y$ vektörü olarak temil edeceðiz, bu tek
boyutlu ama düþünün ki görüntüdeki iki boyutu alýp düzleþtirdik, tek vektör
yaptýk, alt alta satýr satýrlarý yanyana koyduk mesela, vs. Bu resim
hakkýnda þunu biliyoruz, görüntü parçasal olarak sabit, yani yanyana
hücreler birbirinden çok farklý deðil. Bazý yerlerde olabilir mesela mavi
arka plandan kýrmýzý objeye geçiþ yapýlan yerlerde, ama diðer yerlerde
benzerlik var. Biz gürültülü resimden gürültüsüz resmi çýkartmak istiyoruz.

Gürültü yoketme alanýnda pek çok yöntem var. Fakat gürültü yoketme
problemine optimizasyon açýsýndan yaklaþabiliriz. Mesela, hedef kriteri þu
haldeki bir optimizasyon problemi,

$$
\min_{\beta \in \mathbb{R}^n} 
\frac{1}{2} \sum _{i=1}^{n} (y_i - \beta_i)^2 + 
\lambda \sum _{(i,j) \in E)}  |\beta_i - \beta_j|
$$








[1] Tibshirani, {\em Convex Optimization, Lecture Video 7}, 
\url{https://www.youtube.com/channel/UCIvaLZcfz3ikJ1cD-zMpIXg}

\end{document}
