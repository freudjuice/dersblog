\documentclass[12pt,fleqn]{article}\usepackage{../../common}
\begin{document}
Dýþbükey Optimizasyonuna (Convex Optimization) Giriþ

Yapay öðrenme (machine learning) ve optimizasyonda sürekli optimizasyonu
görürüz. Diðer disiplenlerde de görülür tabii ama bu ikisi benim ana
konularým o yüzden o konulardan bu derste daha fazla bahsedeceðiz. Derste
belirli bir amaç için gereken optimizasyon problemini çözmekten çok
optimizasyon mekanizmasýnýn detaylarýný inceleyeceðiz. Optimallik
þartlarýna bakmak, varýlan çözümün niteliðine bakmak bu detaylardan
bazýlarý.

Þimdi aklýnýza gelen bazý optimizasyon örneklerini verin bana [öðrenciler
söylüyor]

1) Regresyon - En Az Kareler. Evet. Hata karelerinin toplamý minimize
edilir burada, bir hedef $y$ vardýr, onu bir formül üzerinden katsayýlarý
olan bir denklem vardýr, ve model uyum iyiliðini hata kare toplamý
üzerinden ölçeriz.

$$
\min_\beta \sum (y_i - x_i^T \beta)^2 
$$

Baþka ne tür regresyon þekilleri var?  

2) Regülarize Edilmiþ Regresyon - Lasso. Burada yine hata karelerin toplamý
var, ama üstüne katsayýlarýn L1 norm'unu minimize etmeye çalýþýrýz. Yani 

$$
\min_\beta \sum (y_i - x_i^T \beta)^2 \quad \textrm{oyle ki}
$$
$$
\sum |\beta| \le t
$$

3) En Az Mutlak Sapma Regresyonu (Least Absolute Deviations) - bu da
benden. Bu tür regresyon ile kare yerine mutlak deðer operasyonu
kullanýlýyor [1, 14:35].   

$$
\min_\beta \sum |y_i - x_i^T \beta |
$$

BU tür regresyon ile aykýrý (outlier) deðerlere daha az önem verilmiþ
olur. Fakat mutlak deðer hesabý kullanýnca optimizasyon zorlaþýyor çünkü
üstteki formül artýk pürüzsüz deðil. 

4) Sýnýflama - Lojistik Regresyon. LR ile $y_i$ ikisel olur, 0 ya da 1. LR
formülizasyonu normal regresyona benziyor, 

5) Bilgisayar Bilim - Seyahet Eden Satýþ Görevlisi Problemi (TSP),
Planlama, Ayrýksal Optimizasyon. Bu ders bloklarýnýn sonunda Tam Sayý
Programlama (Ýnteðer Programming) konusuna bakacaðýz, bu tür konulara orada
daha çok yaklaþmýþ olacaðýz. 

6) Ýstastistik - Maksimum Olurluk. MO istatistikte pek çok yaptýðýmýz iþin
mihenk taþýdýr. Hatta LR, En Az Kareler, vs aslinda MO'nun özel, spesifik
halleridir. Burada vurgu içbükey olurluk elde etmek, ki bir içbükey
fonksiyonu maksimize etmiþ olalým, bu bir dýþbükey fonksiyonu minimize
etmek ile ayný þey. 

Böyle devam edebilirdik, optimizasyon örnekleri sayfalar
doldurabilirdik. Optimizasyon her yerde. Ama belki de neyin optimizasyon
olmadýðýna da bakmak iyi olur. Mesela istatistikte optimizasyon olmayan
problemler nedir?

Hipotez test etmek, p-deðerleri. Ya da takviyelemek (boosting), önemli bir
konu ama optimizasyon deðil. Rasgele Ormanlar (Random Forests),
deðil. Önyükleyiciler (bootstrap), çapraz-saðlama (cross-validation), yine
deðil [1, 22:09].  

Ve iddiam þu ki optimizasyon olmayan konular hakkýnda olanlara kýyasla daha
fazla teorik bilgimiz var. Üstteki teknikler çoðunlukla prosedürsel. Ama
mesela Lasso diyelim, bu bir dýþbükey optimizasyonun çýktýsý olduðu için
optimalite þartlarý üzerinden onun çözümünün özellikleri hakkýnda konuþmak
kolaylaþýyor. 

Peki biz niye bu dersteki konuyu ogrenmek isteriz, isteyebiliriz? Sonucta
Lasso'yu birisi bulmus onun kodunu cagiririz, is biter. Uc sebep
var. Birincisi farkli algoritmalar duruma gore daha iyi performans
gosterebilir, durum derken veriden bahsediyorum. Bu sebeple her
algoritmanin ozunu anlamak cok onemli. Ikincisi herhangi bir alandaki
problemi cozen optimizasyonun temelini bilmek bize alan hakkinda ek gorus
kazandirabilir. 

Üçüncü sebep optimizasyon hýzlý hareket eden bir alan, eðlenceli! Mesela
optimizasyon alanýndaki NIPS Çalýþtayýna (Workshop) bakarsanýz, her sene
deðiþiyor! Birkaç sene önce dýþbükey olmayan optimizasyon büyük konuydu,
tabii o zaman bu dersi iþlerken utanýr gibi oluyorduk çünkü bizim konu
dýþbükey optimizasyon ve yapay öðrenimdeki en büyük konferansta dýþbükey
olmayan konular iþleniyor.. Fakat o zamanki odaðýn sebebi o zamanlarda bir
sürü yeni dýþbükey olmayan ve yakýnsadýðý ispat edilen metotlarýn bulunmuþ
olmasýydý. Ama bir sonraki sene rasgele (stochastic) optimizasyon geri
dönüþ yapmýþtý, rasgele gradyan iniþi vs. Böyle her sene deðiþim oluyor, bu
güzel bir þey demek ki hala ilerleme için oldukça alan var.

Ornekler

Bu orneklerin cogu tam varyasyon gurultu yoketmek (denoising) etrafinda,
bunun bir diger ismi kaynasmis (fused) lasso. Elimizde iki boyutlu izgara
halinde bir veri var, bir goruntu, $i,j$ kordinatlarinda bir renk degeri
var, 3 ile 7 arasindaki renkler. 

\includegraphics[width=25em]{func_19_intro_01.png}

En soldaki gerçek resim. Ortadaki ise onun gürültülü hali, bizim elimizdeki
veri bu diyelim. Görüntüyü $y$ vektörü olarak temil edeceðiz, bu tek
boyutlu ama düþünün ki görüntüdeki iki boyutu alýp düzleþtirdik, tek vektör
yaptýk, alt alta satýr satýrlarý yanyana koyduk mesela, vs. Bu resim
hakkýnda þunu biliyoruz, görüntü parçasal olarak sabit, yani yanyana
hücreler birbirinden çok farklý deðil. Bazý yerlerde olabilir mesela mavi
arka plandan kýrmýzý objeye geçiþ yapýlan yerlerde, ama diðer yerlerde
benzerlik var. Biz gürültülü resimden gürültüsüz resmi çýkartmak istiyoruz.

Gürültü yoketme alanýnda pek çok yöntem var. Fakat gürültü yoketme
problemine optimizasyon açýsýndan yaklaþabiliriz. Mesela, hedef kriteri þu
haldeki bir optimizasyon problemi,

$$
\min_{\beta \in \mathbb{R}^n} 
\frac{1}{2} \sum _{i=1}^{n} (y_i - \beta_i)^2 + 
\lambda \sum _{(i,j) \in E)}  |\beta_i - \beta_j|
$$

Ýlk terimde aradýðýmýz ideal resim ile gerçek resim arasýndaki karesel
kayýp hesabý var, yani her hücredeki $\theta_i$'in olabildiði kadar $y_i$
verisine yakýn olmasýný istiyoruz. Ýkinci terimdeki $\lambda$ bizim
dýþarýdan atadýðýmýz bir parametre, iki terim arasýndaki dengeyi
kuruyor. Bu parametrenin çarptýðý ikinci terim bir ceza terimi. Yanyana
olan her $i,j$'ye bakýyor, saðda solda altta üstte olsun, bu hücrelerin
renk farkýný cezalandýrýyor, yani farkýn daha az olmasýný zorluyor çünkü
resimde genel olarak bir süreklilik olmasýný istiyoruz. Oldukça sofistike
bir iþlem aslýnda, ama optimizasyon formülasyonu açýsýndan oldukca
basit. Ýki terim var, o kadar.

Çözüm resimde en saðdaki resimde görülüyor. $\lambda=25$ seçtim onun için,
ve çözdüm. $\lambda$'yi arttýrdýkça resmin daha kaba görüntülü olmaya
baþladýðýný görebilirdiniz, mesela kýrmýzý ile pembe bölgeler birbiri içine
geçmeye baþlayabilirdi. $\lambda=\infty$ için ne olur? Her þey tek bir renk
olur, o renk $y$'nin ortalamasý olurdu. $\lambda=0$ için gürültülü verinin
aynýsýný elde ederiz. 

Çözümü nasýl elde ettim? Üstteki sonucu ADMM ile elde ettim. Bu ders
bloðunun sonunda bu algoritmayi göreceðiz. Bu problemde ADMM'in spesifik
bir versiyonunu kullandým, bu versiyonun bu problemde iyi iþleyeceðini
biliyordum. 300x200 boyutunda bir resimdi, 20 döngü sonrasý sonucu elde
ettim, her döngüde lineer zaman harcadý. Tüm iþleyiþi bir saniyenin ufak
bir parçasýydý. 

Proksimal gradyan iniþi ile 1000 kere döndük, sonuç fena deðil ama bazý
renkler tam birleþmedi. Eðer 10000 kere döndürseydim ADMM sonucuna
yaklaþýrdý. Bu metot ile de her döngüde lineer zaman harcanýyor, ama
algoritmanýn tamamý daha yavaþ yakýnsadý. Yani, amaç için doðru araç
diyemeyiz. 

Sonra kordina iniþ adýnda çok popüler bir diðer metot iþlettim, 10000 kere
döndü, adýmlar lineer zaman, ama yakýnsama olmadý. Hatta sonuç oldukca
kötüydü. Kesinlikle amaç için yanlýþ araç. Yani iyi ile kötü metot arasýnda
boyutsal fark var (order of magnitude), iþlem hýzý bakýmýndan 1, 2, daha
kötü deðil, 10, 100 kat daha kötüden bahsediyoruz, ve kalite iyi deðil.

Bu arada kordinat iniþini öðrenince üstteki kriteri nasýl kullandýðým kafa
karýþtýrabilir, cevap algoritmayi kriterin ikizi üzeride
iþlettim. Dersimizde ilerledikçe bunun anlamýný öðreneceðiz. Bir problemin
ikizini almak ve bu ikize algoritmalarý nasýl uygulanacaðýný
görmek.. bunlarý hep göreceðiz. 

Mesajým ne? ADMM her yerde çok iyi iþler demek mi? Hayýr. ADMM bazý
yerlerde daha kötü iþler. Diðer yerlerde proksimal gradyan daha iyidir. Bu
sebeple tüm seçenek yelpazesinin bilmek, her algoritmanin özelliklerini
anlamak faydalýdýr. 

Bir diger ornek [1, 42:53]. Tam varyasyon gurultu yoketme yapiliyor yine
ama burada iki boyuta bakmak yerine tek boyuta bakiyoruz, yani bazi
acilardan bu problem daha kolay. Veri yine $y_1,..,y_n$ ama duzlestirilmis
goruntu yerine tek bir eksende veri. Ayrica verinin ortalamasi parcasal
sabit, yani tek duz cizgi. 

$$
\min_\theta \frac{1}{2} (y_i-\theta_i)^2 + 
\lambda \sum _{i=1}^{n-1} |\theta_i - \theta_{i+1}|
$$

Burada ceza teriminde yanyana olan iki $\theta$'nin farkini
cezalandiriyoruz, yani yanyana verinin benzer olmasini istiyoruz. 

\includegraphics[width=25em]{func_19_intro_02.png}

Veriye bakarsak iki bolge var, bir bolgede ortalama sabit digerinde de
(baska) bir sabit. Ama algoritma bunu bilmiyor tabii onu kesfetmesi
gerekecek. Eger $\lambda$ buyukse global ortalama ortaya cikiyor, tek
cizgi. Goruntu orneginde soyledigimiz oluyor yani ama tek
boyutta. $\lambda$ kuculdukce farkli ortalama bloklarinin ortaya cikmasini
sagliyoruz. Ortadaki sonuc oldukca iyi. 3. resimde $\lambda$ biraz daha
kucultuldu, burada bakiyoruz algoritma basta ufak bir blok daha yaratmayi
secti. Bloklarin arasindaki noktaya ``degisim noktasi (changepoints)''
denir. 

Bir deðiþim noktasý elde edince, þimdi kendimize bir istatistiki soru
sorabiliriz. Bu deðiþim noktalarýnýn istatistiki önemi (significance)
nedir? Görsel olarak ben bakýnca diyorum ki 3. resimde saðdaki deðiþim
noktasý önemli ama o baþtaki ufak deðiþim deðil. O yapma (spurious) bir
deðiþim herhalde. Tabii $\lambda$'yi daha da ufaltsam daha da fazla uyduruk
deðiþim noktalarý elde ederdim. Optimizasyon probleminin özü böyle, ayar
deðiþkeni $\lambda$ elde edilen sonuçlara, neye ne kadar aðýrlýk
verildiðini kontrol ediyor. Fakat istatistiki öneme dönersek bu tür
sorularý sadece tam varyasyonu iyi anladýðýmýz takdirde
cevaplandýrabiliriz. 

\includegraphics[width=20em]{func_19_intro_03.png}









[devam edecek]

Kaynaklar

[1] Tibshirani, {\em Convex Optimization, Lecture Video 7}, 
\url{https://www.youtube.com/channel/UCIvaLZcfz3ikJ1cD-zMpIXg}

\end{document}
