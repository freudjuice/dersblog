\documentclass[12pt,fleqn]{article}\usepackage{../../common}
\begin{document}
Newton'un Metodu (Newton's Method)

Bu notlar [2] dersinden alýndý.

Newton birazdan bahsedeceðimiz yöntemi tek boyutlu problemler için
kullandý. Rhapson adlý bilimci yöntemi çok boyutlu problemler için
geniþletti. Biz bu yönteme optimizasyon çerçevesinde bakacaðýz.  Konunun
tarihinden biraz bahsetmek istiyorum, bu dersi öðretmeye baþladýðýmda 1986
senesiydi, Newton'un metodunu nasýl gördüðümüz o zamandan beri deðiþime
uðradý. NM o zamanlar son baþvurulan metot diye öðretiliyordu, çünkü metodu
kullanmak için ``büyük'' bir denklem sistemi çözmek gerekiyordu, 500 x 500
bir sistem mesela. Bugüne gelelim artýk NM ilk baþvurulan metot haline
geldi, 50,000 x 50,000 boyutlarýnda bir sistem çözmek ``yetiyor'' ve böyle
bir sistem artýk idare edilebilen bir boyut haline geldi. Yani hesapsal
kapasite NM'in optimizasyon alanýnda oynadýðý rolü tamamen deðiþtirdi.

Diðer bir faktör ileride öðreneceðimiz iç nokta (interior-point)
metotlarýnýn Newton'un metodunu kullanýyor olmalarý. Ýç nokta metotlarý
icbukey optimizayonda çok popüler, onlar için NM gerekiyor, bu da NM'in
popülaritesini arttýrýyor.

NM nedir? Elimde bir kýsýtlanmamýþ (unconstrained) problemim var diyelim,

$$
\min f(x), \quad \textrm{ öyle ki } \quad x \in X = \mathbb{R}^n
$$

Bir Taylor açýlýmý yapabilirim,

$$
f(x) \approx 
f(\bar{x}) + 
\nabla f(\bar{x})^T (x-\bar{x}) + 
\frac{1}{2} (x-\bar{x})^T H (x-\bar{x}) 
$$

ki $H$ Hessian matrisi. Üstteki formüle $h(x)$ diyelim. Böylece bir karesel
model ortaya çýkartmýþ oldum, formülün sað tarafýndaki çarpým onu karesel
yapýyor, ve þimdi onu kesin olarak çözmek istiyorum. Bunu nasýl yaparým?
Formülün gradyanýný sýfýra eþitleyebilirim. Üstteki fonksiyonun $x$'teki
gradyaný nedir? 

Gradyaný $x$'e göre aldýðýmýzý unutmayalým, $h(x)$'in ikinci terimi
$\nabla f(\bar{x})^T$ bir sabit sayý, ikinci gradyan alýnýrken sýfýrlanýr,
ve tüm ikinci terim sýfýrlanýr. Üçüncü terimin gradyanýný almak bir nevi
$\frac{\partial (x^TAx)}{\partial x} $ almak gibi [1], $H$ belli bir
noktadaki ikinci türev matrisi olduðu için $A$ gibi bir sabit matris kabul
edilebilir, $A$ simetrik olunca gradyan $2Ax$ sonucunu veriyordu, $H$
simetrik, o zaman üçüncü terimde $H$ kalýr, 2 ve $1/2$ birbirini iptal
eder, sonuç

$$
\nabla h(\bar{x}) = \nabla f(\bar{x}) + H(\bar{x})(x-\bar{x}) 
$$

Ýki üstteki karesel yaklaþýksal ifadenin gradyaný bu iþte. Onu sýfýra
eþitleriz ve çözeriz. $H$ tersi alinabilir bir matristir, o zaman 

$$
\nabla h(\bar{x}) = \nabla f(\bar{x}) + H(\bar{x})(x-\bar{x}) = 0
$$


$$
(x-\bar{x}) = -H^{-1} \nabla f(\bar{x})
$$
 
Üstteki ifadeye $d$ diyebilirim, ve bu $d$ benim Newton yönüm olarak
görülebilir, yön derken optimizasyon baðlamýnda minimuma giden yön. 

Bu bizi gayet basit 4 adýmlýk bir algoritmaya taþýyor,

0) $x^0$ verildi, bu baþlangýç noktasý, $k = 0$ yap.

1) $d^k = -H(x^k)^{-1} \nabla f(x^k)$. Eðer $d^k=0$ ise dur.

2) $\alpha^k = 1$ adým boyu seç

3) $x^{k+1} = x^k + \alpha^k d^k$, $k = k + 1$ yap, ve 1. adýma geri dön.

Bu metodun önemli bir özelliðinin sadece bir lineer sistemi çözmek olduðunu
görüyoruz. Bir lineer sistemi çözmek kolay midir? Sisteme göre deðiþir, 100
x 100 sistem, problem yok. 10,000 x 10,000 yoðun bir sistem var ise (seyrek
matrisle temsil edilen lineer sisteme nazaran) iþimiz daha zor
olacaktýr. Bu tür sistemlerde Gaussian eliminasyon iþlemeyebilir, bir tür
özyineli metot gerekli. Demek istediðim Newton yönteminin darboðazý bir
lineer denklem sistemini her seferinde sýfýrdan baþlayarak çözmek, ve bunu
her döngüde yapmak. 

Fakat bu çözümün bize pek çok þey kazandýrdýðýný da görmek lazým;
bahsedilen sistemi çözmek bize pek çok bilgi kazandýrýyor çünkü çözülen
problem içinde 1. ve 2. türev bilgisi var. Bu bilgi minimizasyon açýsýndan
daha akýllýca adým atýlabilmesini saðlýyor. 

Metot Hessian'ýn her adýmda tersi alýnabilir olduðunu farzediyor, bu her
zaman doðru olmayabilir. O sebeple bunun doðru olduðu türden problemler ile
uðraþacaðýz, ya da Hessian'ýn tersi alýnabilir olmasýný saðlayan
mekanizmalarý göreceðiz. $H$'nin özünü bozmadan deðiþtirerek tersi
alýnabilir olmasýný saðlayan yöntemler var. 

Ayrýca hedef her adýmda fonksiyonunu oluþturduðumda bu fonksiyonun azalma
garantisi yok. Öyle ya akýllý bir algoritmanin her adýmda hedef
fonksiyonumu daha iyiye götürdüðümü düþünebilirdim, ama þu anda kadar
gördüklerimiz ýþýðýnda, bunun garantisi yok. Bu konuya sonra deðineceðiz. 

Bir diðer nokta 2. adýmýn çizgi arama ile geniþletilebilmesi [atlandý]

NY'nin en çekici tarafý, eðer yakýnsama (convergence) mümkün ise bu
yakýnsamanýn çok hýzlý bir þekilde olmasý, ki bu iyi. Bu konuya gelmeden
metodun bazý ek özelliklerini görelim.

Terminoloji: bir matrise SPD denir eger matris simetrik, pozitif kesin ise
(simetric positive-definite). 

Teklif (Proposition) 1: 

Eger $H(x)$ SPD ise $d \ne 0$, o zaman $d$ $\bar{x}$ noktasinda bir inis
yonune isaret eder. Inis yonu olmasi demek, eger makul ufak bir adim
cercevesinde gidilen noktada $f$'in degerinin o an oldugumuz noktadan daha
az olmasi demektir. 

Ispatlayalim. Onceki dersten hatirlarsak, eger yonum gradyan ile negatif ic
carpima sahip ise, o zaman yonum kesinlikle bir inis yonuydu. 
















[devam edecek]

Kaynaklar 

[1] Bayramlý, Çok Boyutlu Calculus, {\em Vektör Calculus, Kurallar, Matris Türevleri}

[2] Freund, {\em MIT OCW Nonlinear Programming Lecture},
    \url{https://ocw.mit.edu/courses/sloan-school-of-management/15-084j-nonlinear-programming-spring-2004/}

\end{document}





