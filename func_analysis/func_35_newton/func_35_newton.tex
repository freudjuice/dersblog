\documentclass[12pt,fleqn]{article}\usepackage{../../common}
\begin{document}
Newton'un Metodu (Newton's Method)

Bu notlar [2] dersinden alýndý.

Newton birazdan bahsedeceðimiz yöntemi tek boyutlu problemler için
kullandý. Rhapson adlý bilimci yöntemi çok boyutlu problemler için
geniþletti. Biz bu yönteme optimizasyon çerçevesinde bakacaðýz.  Konunun
tarihinden biraz bahsetmek istiyorum, bu dersi öðretmeye baþladýðýmda 1986
senesiydi, Newton'un metodunu nasýl gördüðümüz o zamandan beri deðiþime
uðradý. NM o zamanlar son baþvurulan metot diye öðretiliyordu, çünkü metodu
kullanmak için ``büyük'' bir denklem sistemi çözmek gerekiyordu, 500 x 500
bir sistem mesela. Bugüne gelelim artýk NM ilk baþvurulan metot haline
geldi, 50,000 x 50,000 boyutlarýnda bir sistem çözmek ``yetiyor'' ve böyle
bir sistem artýk idare edilebilen bir boyut haline geldi. Yani hesapsal
kapasite NM'in optimizasyon alanýnda oynadýðý rolü tamamen deðiþtirdi.

Diðer bir faktör ileride öðreneceðimiz iç nokta (interior-point)
metotlarýnýn Newton'un metodunu kullanýyor olmalarý. Ýç nokta metotlarý
icbukey optimizayonda çok popüler, onlar için NM gerekiyor, bu da NM'in
popülaritesini arttýrýyor.

NM nedir? Elimde bir kýsýtlanmamýþ (unconstrained) problemim var diyelim,

$$
\min f(x), \quad \textrm{ öyle ki } \quad x \in X = \mathbb{R}^n
$$

Bir Taylor açýlýmý yapabiliriz,

$$
f(x) \approx 
f(\bar{x}) + 
\nabla f(\bar{x})^T (x-\bar{x}) + 
\frac{1}{2} (x-\bar{x})^T H (x-\bar{x}) 
$$

ki $H$ Hessian matrisi. Üstteki formüle $h(x)$ diyelim. Böylece bir karesel
model ortaya çýkartmýþ oldum, formülün sað tarafýndaki çarpým onu karesel
yapýyor, ve þimdi onu kesin olarak çözmek istiyorum. Bunu nasýl yaparým?
Formülün gradyanýný sýfýra eþitleyebilirim. Üstteki fonksiyonun $x$'teki
gradyaný nedir? 

Gradyaný $x$'e göre aldýðýmýzý unutmayalým, $h(x)$'in ikinci terimi
$\nabla f(\bar{x})^T$ bir sabit sayý, ikinci gradyan alýnýrken sýfýrlanýr,
ve tüm ikinci terim sýfýrlanýr. Üçüncü terimin gradyanýný almak bir nevi
$\frac{\partial (x^TAx)}{\partial x} $ almak gibi [1], $H$ belli bir
noktadaki ikinci türev matrisi olduðu için $A$ gibi bir sabit matris kabul
edilebilir, $A$ simetrik olunca gradyan $2Ax$ sonucunu veriyordu, $H$
simetrik, o zaman üçüncü terimde $H$ kalýr, 2 ve $1/2$ birbirini iptal
eder, sonuç

$$
\nabla h(\bar{x}) = \nabla f(\bar{x}) + H(\bar{x})(x-\bar{x}) 
$$

Ýki üstteki karesel yaklaþýksal ifadenin gradyaný bu iþte. Onu sýfýra
eþitleriz ve çözeriz. $H$ tersi alinabilir bir matristir, o zaman 

$$
\nabla h(\bar{x}) = \nabla f(\bar{x}) + H(\bar{x})(x-\bar{x}) = 0
$$


$$
(x-\bar{x}) = -H^{-1} \nabla f(\bar{x})
$$
 
Üstteki ifadeye $d$ diyebilirim, ve bu $d$ benim Newton yönüm olarak
görülebilir, yön derken optimizasyon baðlamýnda minimuma giden yön. 

Bu bizi gayet basit 4 adýmlýk bir algoritmaya taþýyor,

0) $x^0$ verildi, bu baþlangýç noktasý, $k = 0$ yap.

1) $d^k = -H(x^k)^{-1} \nabla f(x^k)$. Eðer $d^k=0$ ise dur.

2) $\alpha^k = 1$ adým boyu seç

3) $x^{k+1} = x^k + \alpha^k d^k$, $k = k + 1$ yap, ve 1. adýma geri dön.

Bu metodun önemli bir özelliðinin her adýmda sadece bir lineer sistemi
çözmek olduðunu görüyoruz (tersini alma iþlemi). Bir lineer sistemi çözmek
kolay mýdýr? Sisteme göre deðiþir, 100 x 100 sistem, problem yok. 10,000 x
10,000 yoðun bir sistem var ise (seyrek matrisle temsil edilen lineer
sisteme nazaran) iþimiz daha zor olacaktýr. Bu tür sistemlerde Gaussian
eliminasyon iþlemeyebilir, bir tür özyineli metot gerekli. Demek istediðim
Newton yönteminin darboðazý bir lineer denklem sistemini her seferinde
sýfýrdan baþlayarak çözmek, ve bunu her döngüde yapmak.

Fakat bu çözümün bize pek çok þey kazandýrdýðýný da görmek lazým;
bahsedilen sistemi çözmek bize pek çok bilgi kazandýrýyor çünkü çözülen
problem içinde 1. ve 2. türev bilgisi var. Bu bilgi minimizasyon açýsýndan
daha akýllýca adým atýlabilmesini saðlýyor. 

Metot Hessian'ýn her adýmda tersi alýnabilir olduðunu farzediyor, bu her
zaman doðru olmayabilir. O sebeple bunun doðru olduðu türden problemler ile
uðraþacaðýz, ya da Hessian'ýn tersi alýnabilir olmasýný saðlayan
mekanizmalarý göreceðiz. $H$'nin özünü bozmadan deðiþtirerek tersi
alýnabilir olmasýný saðlayan yöntemler var. 

Ayrýca hedef her adýmda fonksiyonunu oluþturduðumda bu fonksiyonun azalma
garantisi yok. Öyle ya akýllý bir algoritmanin her adýmda hedef
fonksiyonumu daha iyiye götürdüðümü düþünebilirdim, ama þu anda kadar
gördüklerimiz ýþýðýnda, bunun garantisi yok. Bu konuya sonra deðineceðiz. 

Bir diðer nokta 2. adýmýn çizgi arama ile geniþletilebilmesi [atlandý]

NY'nin en çekici tarafý, eðer yakýnsama (convergence) mümkün ise bu
yakýnsamanýn çok hýzlý bir þekilde olmasý, ki bu iyi. Bu konuya gelmeden
metodun bazý ek özelliklerini görelim.

Terminoloji: bir matrise SPD denir eger matris simetrik, pozitif kesin ise
(simetric positive-definite). 

Teklif (Proposition) 1: 

Eðer $H(x)$ SPD ise $d \ne 0$, o zaman $d$ $\bar{x}$ noktasýnda bir iniþ
yönüne iþaret eder. Ýniþ yönü olmasý demek, eðer makul ufak bir adým
çerçevesinde gidilen noktada $f$'in deðerinin o an olduðumuz noktadan daha
az olmasý demektir.

Nasýl ispatlarým? Önceki dersten hatýrlarsak, eðer yönüm gradyan ile
negatif iç çarpýma sahip ise, o zaman yönüm kesinlikle bir iniþ yönüydü.

Teori 

Diyelim ki $f(x)$ fonksiyonu $\bar{x}$ noktasýnda türevi alýnabilir halde
[2, sf. 9]. Eðer elimizde $\nabla f(\bar{x})^T d < 0$ sonucunu veren bir
$d$ vektörü var ise, öyle ki her yeterince küçük $\lambda > 0$ için
$f(\bar{x}+\lambda d) < f(\bar{x})$ olacak þekilde, o zaman $d$ bir iniþ
yönüdür.

Ýspat 

Taylor açýlýmý ile yönsel türev tanýmýna bakarsak,

$$
f(\bar{x} + \lambda d ) = 
f(\bar{x}) + \lambda \nabla f(\bar{x})^T d + 
\lambda ||d|| \alpha(\bar{x},\lambda d)
$$

öyle ki $\alpha(\bar{x},\lambda d) \to 0$, $\lambda \to 0$ olurken. Not:
Norm içeren üçüncü terimdeki $\lambda ||d|| \alpha(\bar{x},\lambda d)$
ifadesi Taylor serisinin artýklý tanýmýndan geliyor. Detaylar için [3,
sf. 360]'a bakýlabilir.

Üstteki ifadeyi tekrar düzenlersek,

$$
\frac{f(\bar{x} + \lambda d) - f(\bar{x})}{\lambda} = 
\nabla f(\bar{x})^T + ||d||\alpha(\bar{x},\lambda d)
$$

$\nabla f(\bar{x})^T d < 0$ olduðuna göre (aradýðýmýz þart bu) o zaman, ve
$\alpha(\bar{x},\lambda d) \to 0$, $\lambda \to 0$ iken, her yeterince
küçük $\lambda > 0$ için $f(\bar{x} + \lambda d)-f(\bar{x}) < 0$ olmalýdýr,
yani her hangi bir yönde atýlan adým bir önceki $f$ deðerinden bizi daha
ufak bir $f$ deðerine götürmelidir. 

Ana Teklif'e dönelim. Newton adýmýnýdaki SPD $H$ için $0 < d^T \nabla f$
olduðunu göstermemiz lazým (ki böylece iniþ yönü olduðunü ispatlayabilelim,
bir önceki teori),

$$
d = -H^{-1} \nabla f(\bar{x})
$$

demiþtik, her iki tarafý $\nabla f(x)$ ile çarpalým,

$$
d \nabla f(x) = -\nabla f(x) H^{-1} \nabla f(x) 
$$

Eþitliðin sað tarafýndaki ifade hangi þartlarda eksi olur? Eðer $H$ matrisi
pozitif kesin ise deðil mi? Genel matrislerden hatýrlarsak, matris $A$ ve
bir vektör için $v$ eðer $A$ pozitif kesin ise $v^TAv > 0$. Daha önce
$H$'nin pozitif kesin olduðunu söylemiþtik, o zaman bir þekilde eðer $H$
pozitif kesin olmasýnýn sadece ve sadece $H$'nin tersinin pozitif kesin
olmasýna baðlý olduðuna gösterebilirsem amacýma ulaþabilirim.

Bunu yapmak aslýnda pek zor deðil. Biliyorum ki $H(x)$ SPD. Simdi herhangi
bir vektor $v$ icin 

$$
0 < v^T H(x)^{-1}v
$$

ifadeyi þöyle geniþletelim, $H(x)H(x)^{-1}$ eklemek hiçbir þeyi deðiþtirmez
çünkü bu çarpým birim matristir, 

$$
v^T H(x)^{-1}v = v^T H(x)^{-1} H(x)H(x)^{-1} v > 0
$$

Geniþlemiþ ifadenin harfiyen pozitif olduðunu biliyorum, iki üstteki
tanýmdan. Ama þimdi üstteki ifadeye farklý bir þekilde bakarsak,

$$
v^T H(x)^{-1}v = \underbrace{v^TH(x)^{-1}} H(x) \underbrace{H(x)^{-1} v} > 0
$$

Ýþaretlenen bölümlerin birer vektör olduðunu görebiliriz, bu durumda
$v^TAv > 0$ pozitif kesinlik formülü farklý bir $v$ için hala geçerlidir, o
zaman ortadaki $A$, bu durumda $H(x)$ pozitif kesin olmalýdýr. 

Örnek 1

$f(x) = 7x - \ln(x)$ olsun. O zaman $\nabla f(x) = 7 - \frac{1}{x}$ ve
$H(x) = f''(x) = \frac{1}{x^2}$. Bu fonksiyonun özgün global minimumunun
$x^* = 1/7 = 1.428..$ olduðunu kontrol etmek zor deðil. $x$ noktasýndaki
Newton yönü

$$
d = -H(x)^{-1} \nabla f(x) = -\frac{f'(x)}{f''(x)} = 
-x^2 \left( 7 - \frac{1}{x}  \right)=
x - 7x^2
$$

Newton yöntemi $\{ x^k \}$ serisini üretecek, öyle ki

$$
x^{k+1} = x^k + ( x^k - 7(x^k)^2  ) = 2x^k - 7(x^k)^2
$$

Altta farklý baþlangýç noktalarýna göre üretilen serileri
görüyoruz. Yakýnsamanýn hangi deðere doðru olduðu bariz, ve global minimum
da o deðer zaten. 

\begin{minted}[fontsize=\footnotesize]{python}
import pandas as pd
pd.set_option('display.notebook_repr_html', False)
pd.set_option('display.max_columns', 20)
pd.set_option('display.max_rows', 30) 
pd.set_option('display.width', 82) 
pd.set_option('precision', 6)
\end{minted}

\begin{minted}[fontsize=\footnotesize]{python}
df = pd.DataFrame(index=np.arange(11))

def calculate_newton_ex1(x):
    arr = []
    for i in range(11):
        arr.append(x)
        x = 2*x - 7*x**2
        if (x > 1e100):  x = np.inf
        if (x < -1e100):  x = -np.inf
    return arr

df['1'] = calculate_newton_ex1(1.0)
df['2'] = calculate_newton_ex1(0.0)
df['3'] = calculate_newton_ex1(0.1)
df['4'] = calculate_newton_ex1(0.01)

print (df)    
\end{minted}

\begin{verbatim}
               1    2         3         4
0   1.000000e+00  0.0  0.100000  0.010000
1  -5.000000e+00  0.0  0.130000  0.019300
2  -1.850000e+02  0.0  0.141700  0.035993
3  -2.399450e+05  0.0  0.142848  0.062917
4  -4.030157e+11  0.0  0.142857  0.098124
5  -1.136952e+24  0.0  0.142857  0.128850
6  -9.048612e+48  0.0  0.142857  0.141484
7  -5.731417e+98  0.0  0.142857  0.142844
8           -inf  0.0  0.142857  0.142857
9           -inf  0.0  0.142857  0.142857
10          -inf  0.0  0.142857  0.142857
\end{verbatim}

Örnek 2

Bu örnekte iki deðiþkenli bir fonksiyon görelim. Global minimum
$(1/3,1/3)$. Bakalým bu deðeri bulabilecek miyiz?

$f(x) = -\ln( 1 - x_1 - x_2) - \ln x_1 - \ln x_2$

$$
\nabla f(x) = 
\left[\begin{array}{r}
\frac{1}{1-x_1-x_2} - \frac{1}{x_1} \\
\frac{1}{1-x_1-x_2} - \frac{1}{x_2} 
\end{array}\right]
$$

$$
H(x) = 
\left[\begin{array}{rr}
(\frac{1}{1-x_1-x_2})^2 - (\frac{1}{x_1})^2 & (\frac{1}{1-x_1-x_2} )^2 \\
(\frac{1}{1-x_1-x_2} )^2 & (\frac{1}{1-x_1-x_2})^2 - (\frac{1}{x_2})^2
\end{array}\right]
$$

\begin{minted}[fontsize=\footnotesize]{python}
import numpy.linalg as lin
df = pd.DataFrame(index=np.arange(11))

def calculate_newton_ex2(x):    
    arr = []
    for i in range(8):
        arr.append(x)
        x1,x2 = x[0],x[1]    
        H = [[(1.0/(1.0-x1-x2))**2 + (1.0/x1)**2.0, (1.0/(1.0-x1-x2))**2.0],
             [(1.0/(1.0-x1-x2))**2, (1.0/(1.0-x1-x2))**2.0 + (1.0/x2)**2.0]]
        H = np.array(H)

        Df = [[1.0/(1.0-x1-x2) - (1.0/x1)], [1.0/(1.0-x1-x2)-(1.0/x2)]]
        Df = np.array(Df)

        d = np.dot(-lin.inv(H),Df)
        x = x + d.flatten()

    return np.array(arr)

res = calculate_newton_ex2([0.85,0.05])
print (res)
\end{minted}

\begin{verbatim}
[[0.85  0.05 ]
 [0.717 0.097]
 [0.513 0.176]
 [0.352 0.273]
 [0.338 0.326]
 [0.333 0.333]
 [0.333 0.333]
 [0.333 0.333]]
\end{verbatim}

[diðer yakýnsama konusu atlandý]

Dikkat edilirse þimdiye kadar diþbükeylik (convexity) farzýný yapmadýk,
sadece Hessian matrinin tersi alýnabilir olduðunu farzettik. 

Devam edersek, özyineli þekilde güncelememizi yaparken Hessian'ýn eþsiz
olduðu bazý noktalara gelmiþ olabiliriz. Bu olduðunda çoðu yazýlým bu
durumu yakalayacak þekilde yazýlmýþtýr, ``yeterince eþsiz'' Hessian
matrislere önceden tanýmlý ufak bir $\epsilon$ çarpý birim matrisi kadar
bir ekleme yaparlar, böylece tersin alýnamama durumundan kurtulunmuþ olur. 
Bu metotlara Newton-ýmsý (quasi-Newton) ismi de veriliyor. 

Eskiden Newton-ýmsý metotlar koca bir araþtýrma sahasýydý. Benim bildiðim
kadarýyla tarihte 15 sene kadar geriye gidersek, üstteki görüldüðü gibi her
adýmda büyük bir denklem sistemi çözmek istemiyoruz, $x$ noktasýndayým,
Hessian iþliyorum, Newton yönümü buluyorum, adým atýyorum, yeni bir
noktadayým. Newton-ýmsý metotlarda bu yeni noktada sil baþtan bir Hessian
iþlemek yerine bir önceki adýmdaki iþlenen Hessian sonuçlarýný, bir
þekilde, az ek iþlem yaparak sonraki adýmda kullanmaya uðraþýyorlar.
Aslýnda pek çok farklý Newton-ýmsý metot var, hepsi farklý þekilde Newton
metotundan farklý (!)

Bu alaný çok iyi takip ettiðim söylenemez, Newton-ýmsý metotlar eskiden
olduðu kadar on planda deðil çünkü bilgisayar hesaplama kapasitesi arttýkça
onlara olan ihtiyaç azaldý.

Not: Hocanýn söylediklerine bu noktada bir ek yapmak lazým, [4, pg. 522]'de
belirtildiði gibi Hessian'ýn silbaþtan yaratýlmasý yerine güncellenmesi
aslýnda paradoksal bir þekilde daha iyi optimizasyon yapýlmasýný
saðlayabiliyor. Newton yönünün iniþ yönü olmasý için $H$ pozitif kesin
olmalýdýr, fakat minimum noktasýndan uzaktaki yerlerde bunun garantisi
yoktur. Yani gerçek Hessian ile gerçek Newton adýmýný atmak bizi
fonksiyonun arttýðý yerlere götürebilir. Newton-ýmsý metotlarýn esprisi
pozitif kesin bir þekilde baþlamak ve ýnise giderken azar azar $H$'yi
yaklaþýksal þekilde öyle günellemek ki $H$ hep pozitif kesin kalsýn.

[Teori 1.1 Ýspatý Atlandý]

Kaynaklar 

[1] Bayramlý, Çok Boyutlu Calculus, {\em Vektör Calculus, Kurallar, Matris Türevleri}

[2] Freund, {\em MIT OCW Nonlinear Programming Lecture},
    \url{https://ocw.mit.edu/courses/sloan-school-of-management/15-084j-nonlinear-programming-spring-2004/}

[3] Miller, {\em Numerical Analysis for Scientists and Engineers}

[4] Flannery, {\em Numerical Recipes, 3rd Edition}

\end{document}
