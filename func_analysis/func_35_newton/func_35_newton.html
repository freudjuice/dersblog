<h1>Newton'un Metodu (Newton's Method)</h1>
<!DOCTYPE html>
<html>
  <head>
    <title>Newton'un Metodu (Newton's Method)
</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {inlineMath: [["$","$"]]}
      });
    MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
      MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
        cancel: ["Extension","cancel"],
        bcancel: ["Extension","cancel"],
        xcancel: ["Extension","cancel"],
        cancelto: ["Extension","cancel"]
      });
    });
    </script>
<script type="text/javascript"
   src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML-full">
</script>
</head>

<p>Newton birazdan bahsedeceğimiz yöntemi tek boyutlu problemler için
kullandı. Rhapson adlı bilimci yöntemi çok boyutlu problemler için
genişletti. Biz bu yönteme optimizasyon çerçevesinde bakacağız. 
Konunun tarihinden biraz bahsetmek istiyorum, bu dersi öğretmeye
başladığımda 1986 senesiydi, Newton'un metodunu nasıl gördüğümüz o zamandan
beri değişime uğradı. NM o zamanlar son başvurulan metot diye
öğretiliyordu, çünkü metodu kullanmak için "büyük" bir denklem sistemi
çözmek gerekiyordu, 500 x 500 bir sistem mesela. Bugüne gelelim artık NM
ilk başvurulan metot haline geldi, 50,000 x 50,000 boyutlarında bir sistem
çözmek "yetiyor" ve böyle bir sistem artık idare edilebilen bir boyut
haline geldi. Yani hesapsal kapasite NM'in optimizasyon alanında oynadığı
rolü tamamen değiştirdi. </p>
<p>Diğer bir faktör ileride öğreneceğimiz iç nokta (interior-point)
metotlarının Newton'un metodunu kullanıyor olmaları. İç nokta metotları
icbukey optimizayonda çok popüler, onlar için NM gerekiyor, bu da NM'in
popülaritesini arttırıyor.</p>
<p>NM nedir? Elimde bir kısıtlanmamış (unconstrained) problemim var diyelim,</p>
<p>$$
\min f(x), \quad \textrm{ öyle ki } \quad x \in X = \mathbb{R}^n
$$</p>
<p>Bir Taylor açılımı yapabilirim,</p>
<p>$$
f(x) \approx 
f(\bar{x}) + 
\nabla f(\bar{x})^T (x-\bar{x}) + 
\frac{1}{2} (x-\bar{x})^T H (x-\bar{x}) 
$$</p>
<p>ki $H$ Hessian matrisi. Üstteki formüle $h(x)$ diyelim. Böylece bir karesel
model ortaya çıkartmış oldum, formülün sağ tarafındaki çarpım onu karesel
yapıyor, ve şimdi onu kesin olarak çözmek istiyorum. Bunu nasıl yaparım?
Formülün gradyanını sıfıra eşitleyebilirim. Üstteki fonksiyonun $x$'tei
gradyanı nedir? </p>
<p>Gradyanı $x$'e göre aldığımızı unutmayalım, $h(x)$'in ikinci terimi
$\nabla f(\bar{x})^T$ bir sabit sayı, ikinci gradyan alınırken sıfırlanır,
ve tüm ikinci terim sıfırlanır. Üçüncü terimin gradyanını almak bir nevi
$\frac{\partial (x^TAx)}{\partial x} $ almak gibi [1], $A$ simetrik olunca
gradyan $2Ax$ sonucunu veriyordu, o zaman üçüncü terimde $H$ kalır, 2 ve
$1/2$ birbirini iptal eder, sonuç</p>
<p>$$
\nabla h() = \nabla f(\bar{x}) + H(\bar{x})(x-\bar{x})
$$</p>
<p>İki üstteki karesel yaklaşıksal ifadenin gradyanı bu işte. Onu sıfıra
eşitleriz ve çözeriz, </p>
<p>[devam edecek]</p>
<p>Kaynaklar </p>
<p>[1] Bayramli, Cok Boyutlu Calculus, <em>Vektör Calculus, Kurallar, Matris Türevleri</em></p>