\documentclass[12pt,fleqn]{article}\usepackage{../../common}
\begin{document}
Altgradyanlar (Subgradients) - 2

Bir dýþbükey kümesi üzerinden tanýmlý ama pürüzsüz olmayabilecek bir
dýþbükey fonksiyonu minimize etmek için yansýtýlan altgradyan (projected
subgradient) metotu adlý bir metot ta kullanýlabilir [1, 22:04].  

Dýþbükey $f$'yi dýþbükey küme $C$ üzerinden optimize etmek için 

$$
\min_x f(x) \quad \textrm{öyle ki} \quad x \in C
$$

$$
x^{(k)} = P_C \left( x^{(k-1)} - t_k g_k^{(k-1)} \right)
$$

Bu metot normal altgradyan metotu gibi, tek fark parantez içinde görülen
altgradyan adýmý atýldýktan sonra elde edilen sonucun $C$ kümesine geri
yansýtýlmasý (projection), çünkü atýlan adým sonucunda olurlu bir sonuç
elde etmemiþ olabiliriz. 

Yakýnsama analizi normal altgraydan metotuna benziyor, bu metotla da benzer
yakýnsama garantisi elde edilebiliyor, yakýnsama oraný da buna dahil.

Yansýtma adýmý bazen zor olabilir, hangi durumlarda kolay olduðunun listesi
aþaðýda [1, 23:53]. Hangi kümelere yansýtmak kolaydýr?

1) Doðrusal görüntüler: $\{  Ax + b: x \in \mathbb{R}^n  \}$

2) $\{ x: Ax = b \}$ sisteminin çözüm kümesi 

3) Negatif olmayan bölge: $\mathbb{R}_{+}^n = \{x: x \ge 0 \}$. Verilen
vektörün negatif deðerlerinin sýfýr yapmak, pozitifleri tutmak bize o
vektörün negatif olmayan bölge karþýlýðýný veriyor. 

4) Bazý norm toplarý $\{ x: ||x||_p \le 1 \}$, $p=1,2,..,\infty$ için. Daha
önce 2-norm topuna yansýtmayý gördük, vektörü alýp normalize edersek bu
kümeye yansýtma yapmýþ oluyoruz aslýnda. Bu yansýtma bizi o vektörün 2-norm
topundaki en yakýn diðer vektöre götürüyor. Sonsuz norm kolay, bir kutuya
yansýtma yapmýþ oluyoruz, bunun ne demek olduðunu düþünmeyi size
býrakýyorum, 1-norm en zoru. 

5) Bazý çokyüzlüler (polyhedra) ve basit koniler.

Bir uyarýda bulunalým, tanýmý basit duran kümeler ortaya çýkartmak
kolaydýr, fakat bu kümelere yansýtma yapan operatörler çok zor
olabilir. Mesela geliþigüzel bir çokyüzlü $C = \{ x: Ax \le b \}$ kümesine
yansýtma yapmak zordur. Bu problemin kendisi apayrý bir optimizasyon
problemi aslýnda, bir QP. 

[stochastic subgradient method atlandý]

Altgradyanlarýn iyi tarafý genel uygulanabilirlik. Altgradyan metotlarý
dýþbükey, Lipschitz fonksiyonlarý üzerinde bir anlamda optimaldir. Ünlü
bilimci Neþterov'un bu baðlamda bir teorisi vardýr [1, 45:12], pürüzsüz
durumlarda ve 1. derece yöntemlerde altgradyanlarýn lineer kombinasyonlarý
üzerinden güncellemenin sonucu olan adýmlarýn baþarýsýna bir alt sýnýr
tanýmlar. 

Proksimal / Yakýnsal Gradyan Metotu (Proximal Gradient Method) 

Bu metot, herhangi bir pürüzsüz olmayan fonksiyonu optimize etmeye uðraþmak
yerine belli bir yapýya uyan çetrefil fonksiyonlarý optimize etmeye
uðraþýr. Bu yapý

$$
f(x) = g(x) + h(x) \mlabel{1}
$$

formundadýr [1, 45:59]. $g,h$'in ikisi de dýþbükey. $g$'nin pürüzsüz,
türevi alýnabilir olduðu farz edilir (çoðunlukla oldukca çetrefil olabilir)
ve $\dom(g) = \mathbb{R}^n$, $h$ ise pürüzsüz olmayabilir. Tabii pürüzsüz
artý pürüzsüz olmayan iki fonksýsyon toplamý kriterin tamamýný pürüzsüz
olmayan hale çevirir. Ama bu toplam daha basittir denebilir, onun üzerinde
proksimal operatörü uygulanabilir.

Hatýrlarsak eðer $f$ türevi alýnabilir olsaydý gradyan iniþ güncellemesi 

$$
x^{+} = x - t \cdot \nabla f(x)
$$

Bu formüle eriþmenin bir yöntemi karesel yaklaþýklama üzerinden idi,
$f$'nin $x$ etrafýndaki yaklaþýklamasýnda $\nabla^2f(x)$ yerine
$\frac{1}{t} I$ koyunca,

$$
x^{+} = \arg\min_x f(x) + 
\underbrace{
  \nabla f(x)^T (z-x) + \frac{1}{2t} ||z-x||_2^2 
}_{f_t(z)}
$$

elde ediliyordu. Yani gradyan iniþi sanki ardý ardýna geldiði her noktada
bir karesel yaklaþýklama yapýyor, ve onu adým atarak minimize etmeye
uðraþýyor.

Ama (1)'deki $f$ pürüzsüz deðil, o sebeple üstteki mantýk ise
yaramayacak. Fakat, belki de karesel yaklaþýklamanýn bir kýsmýný hala
kullanabiliriz, ve minimizasyonu sadece $g$'ye uygularýz çünkü pürüzsüz
olan kýsým o. Yani niye $g$'nin yerine karesel yaklaþýklama koymayalým?
Þimdi bunu yapacaðýz [1, 50:10]. 







[devam edecek]

Proksimal [2]

\begin{minted}[fontsize=\footnotesize]{python}
N = 100
dim = 30
lamda = 1/np.sqrt(N);
np.random.seed(50)
w = np.matrix(np.random.multivariate_normal([0.0]*dim, np.eye(dim))).T
X = np.matrix(np.random.multivariate_normal([0.0]*dim, np.eye(dim), size = N))
y = X*w

L = (np.linalg.svd(X)[1][0])**2
print(L)
max_iter = 100

def obj(w):
    r = X*w-y;
    return np.sum(np.multiply(r,r))/2 +  lamda * np.sum(np.abs(w))

def f_grad(w):
    return  X.T*(X*w-y) 

def soft_threshod(w,mu):
    return np.multiply(np.sign(w), np.maximum(np.abs(w)- mu,0))  

w = np.matrix([0.0]*dim).T
obj_PG = []
for t in range(0, max_iter):
    obj_val = obj(w)
    w = w - (1/L)* f_grad(w)
    w= soft_threshod(w,lamda/L)
    
    obj_PG.append(obj_val.item())
    if (t % 5==0):
        print('iter= {},\tobjective= {:3f}'.format(t, obj_val.item()))
\end{minted}

\begin{verbatim}
228.3801231037125
iter= 0,	objective= 1761.785102
iter= 5,	objective= 28.002804
iter= 10,	objective= 4.813366
iter= 15,	objective= 2.892747
iter= 20,	objective= 2.559209
iter= 25,	objective= 2.465096
iter= 30,	objective= 2.433524
iter= 35,	objective= 2.422408
iter= 40,	objective= 2.418440
iter= 45,	objective= 2.417017
iter= 50,	objective= 2.416505
iter= 55,	objective= 2.416321
iter= 60,	objective= 2.416255
iter= 65,	objective= 2.416231
iter= 70,	objective= 2.416222
iter= 75,	objective= 2.416219
iter= 80,	objective= 2.416218
iter= 85,	objective= 2.416218
iter= 90,	objective= 2.416218
iter= 95,	objective= 2.416218
\end{verbatim}

Kaynaklar

[1] Tibshirani, {\em Convex Optimization, Lecture Video 8}, 
\url{https://www.youtube.com/channel/UCIvaLZcfz3ikJ1cD-zMpIXg}

[2] He, {\em IE 598 - BIG DATA OPTIMIZATION},  
    \url{http://niaohe.ise.illinois.edu/IE598_2016/}


\end{document}

