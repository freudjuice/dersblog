<h1>Altgradyanlar (Subgradients) - 2</h1>
<!DOCTYPE html>
<html>
  <head>
    <title>Altgradyanlar (Subgradients) - 2
</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {inlineMath: [["$","$"]]}
      });
    MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
      MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
        cancel: ["Extension","cancel"],
        bcancel: ["Extension","cancel"],
        xcancel: ["Extension","cancel"],
        cancelto: ["Extension","cancel"]
      });
    });
    </script>
<script type="text/javascript"
   src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML-full">
</script>
</head>

<p>Bir dışbükey kümesi üzerinden tanımlı ama pürüzsüz olmayabilecek bir
dışbükey fonksiyonu minimize etmek için yansıtılan altgradyan (projected
subgradient) metotu adlı bir metot ta kullanılabilir [1, 22:04].  </p>
<p>Dışbükey $f$'yi dışbükey küme $C$ üzerinden optimize etmek için </p>
<p>$$
\min_x f(x) \quad \textrm{öyle ki} \quad x \in C
$$</p>
<p>$$
x^{(k)} = P_C \left( x^{(k-1)} - t_k g_k^{(k-1)} \right)
$$</p>
<p>Bu metot normal altgradyan metotu gibi, tek fark parantez içinde görülen
altgradyan adımı atıldıktan sonra elde edilen sonucun $C$ kümesine geri
yansıtılması (projection), çünkü atılan adım sonucunda olurlu bir sonuç
elde etmemiş olabiliriz. </p>
<p>Yakınsama analizi normal altgraydan metotuna benziyor, bu metotla da benzer
yakınsama garantisi elde edilebiliyor, yakınsama oranı da buna dahil.</p>
<p>Yansıtma adımı bazen zor olabilir, hangi durumlarda kolay olduğunun listesi
aşağıda [1, 23:53]. Hangi kümelere yansıtmak kolaydır?</p>
<p>1) Doğrusal görüntüler: ${  Ax + b: x \in \mathbb{R}^n  }$</p>
<p>2) ${ x: Ax = b }$ sisteminin çözüm kümesi </p>
<p>3) Negatif olmayan bölge: $\mathbb{R}_{+}^n = {x: x \ge 0 }$. Verilen
vektörün negatif değerlerinin sıfır yapmak, pozitifleri tutmak bize o
vektörün negatif olmayan bölge karşılığını veriyor. </p>
<p>4) Bazı norm topları ${ x: ||x||_p \le 1 }$, $p=1,2,..,\infty$ için. Daha
önce 2-norm topuna yansıtmayı gördük, vektörü alıp normalize edersek bu
kümeye yansıtma yapmış oluyoruz aslında. Bu yansıtma bizi o vektörün 2-norm
topundaki en yakın diğer vektöre götürüyor. Sonsuz norm kolay, bir kutuya
yansıtma yapmış oluyoruz, bunun ne demek olduğunu düşünmeyi size
bırakıyorum, 1-norm en zoru. </p>
<p>5) Bazı çokyüzlüler (polyhedra) ve basit koniler.</p>
<p>Bir uyarıda bulunalım, tanımı basit duran kümeler ortaya çıkartmak
kolaydır, fakat bu kümelere yansıtma yapan operatörler çok zor
olabilir. Mesela gelişigüzel bir çokyüzlü $C = { x: Ax \le b }$ kümesine
yansıtma yapmak zordur. Bu problemin kendisi apayrı bir optimizasyon
problemi aslında, bir QP. </p>
<p>[stochastic subgradient method atlandı]</p>
<p>Altgradyanların iyi tarafı genel uygulanabilirlik. Altgradyan metotları
dışbükey, Lipschitz fonksiyonları üzerinde bir anlamda optimaldir. Ünlü
bilimci Neşterov'un bu bağlamda bir teorisi vardır [1, 45:12], pürüzsüz
durumlarda ve 1. derece yöntemlerde altgradyanların lineer kombinasyonları
üzerinden güncellemenin sonucu olan adımların başarısına bir alt sınır
tanımlar. </p>
<p>Proksimal / Yakınsal Gradyan Metotu (Proximal Gradient Method) </p>
<p>Bu metot, herhangi bir pürüzsüz olmayan fonksiyonu optimize etmeye uğraşmak
yerine belli bir yapıya uyan çetrefil fonksiyonları optimize etmeye
uğraşır. Bu yapı</p>
<p>$$
f(x) = g(x) + h(x) 
\qquad (1)
$$</p>
<p>formundadır [1, 45:59]. $g,h$'in ikisi de dışbükey. $g$'nin pürüzsüz,
türevi alınabilir olduğu farz edilir (çoğunlukla oldukca çetrefil olabilir)
ve $\mathrm{dom}(g) = \mathbb{R}^n$, $h$ ise pürüzsüz olmayabilir. Tabii pürüzsüz
artı pürüzsüz olmayan iki fonksısyon toplamı kriterin tamamını pürüzsüz
olmayan hale çevirir. Ama bu toplam daha basittir denebilir, onun üzerinde
proksimal operatörü uygulanabilir.</p>
<p>Hatırlarsak eğer $f$ türevi alınabilir olsaydı gradyan iniş güncellemesi </p>
<p>$$
x^{+} = x - t \cdot \nabla f(x)
$$</p>
<p>Bu formüle erişmenin bir yöntemi karesel yaklaşıklama üzerinden idi,
$f$'nin $x$ etrafındaki yaklaşıklamasında $\nabla^2f(x)$ yerine
$\frac{1}{t} I$ koyunca,</p>
<p>$$
x^{+} = \arg\min_x f(x) + 
\underbrace{
  \nabla f(x)^T (z-x) + \frac{1}{2t} ||z-x||_2^2 
}_{f_t(z)}
$$</p>
<p>elde ediliyordu. Yani gradyan inişi sanki ardı ardına geldiği her noktada
bir karesel yaklaşıklama yapıyor, ve onu adım atarak minimize etmeye
uğraşıyor.</p>
<p>Ama (1)'deki $f$ pürüzsüz değil, o sebeple üstteki mantık ise
yaramayacak. Fakat, belki de karesel yaklaşıklamanın bir kısmını hala
kullanabiliriz, ve minimizasyonu sadece $g$'ye uygularız çünkü pürüzsüz
olan kısım o. Yani niye $g$'nin yerine karesel yaklaşıklama koymayalım?
Şimdi bunu yapacağız [1, 50:10]. </p>
<p>$$
x^{+} = \arg\min_z \tilde{g}_t(z) + h(z)
$$</p>
<p>ki $\tilde{g}_t(z)$ $g$'nin $x$ noktası etrafındaki karesel yaklaşıklaması
oluyor.  Eğer elimizde $h$ olmasaydı üstteki sadece bir gradyan
güncellemesine indirgenebilirdi. Neyse açılımı yaparsak</p>
<p>$$
= \arg\min_z g(x) + \nabla g(x)^T (z-x) + \frac{1}{2t} ||z-x||_2^2 + h(z)
$$</p>
<p>Daha uygun bir formda yazabiliriz, $z$'nin $g$ üzerindeki değişikliğe
karesel uzaklığı olarak,</p>
<p>$$
= \arg\min_z \frac{1}{2t} 
\big|\big| z - \big( x-t\nabla g(x) \big) \big|\big|_2^2 + 
h(z)
$$</p>
<p>Yani söylenmek istenen, hem sadece $g$ olsaydı atacağımız gradyan adımına
yakın durmaya çalışmak, hem de $h$'nin kendisini ufak tutmak. Dışarıdan
ayarlanan $t$ parametresi $g$ gradyan adımı ile $h$ arasında bir denge
kurmak gibi görülebilir, eğer $t$ ufak ise o zaman gradyan adımına yakın
durmaya daha fazla önem atfetmiş olacağız, $h$'nin ufak tutulmasına daha
az. $t$ çok büyük ise tam tersi olacak. </p>
<p>Bu aslında kabaca Proksimal Gradyan İnişi yöntemini tarif etmiş
oluyor. Şimdi proksimal eşlemesi operatörünü göstereceğiz, ki bu
algoritmaları daha temiz olarak yazmamıza yardımcı olacak. </p>
<p>Bir $h$ fonksiyonu için bir $\mathrm{prox}$ operatörü tanımlıyoruz, </p>
<p>$$
\mathrm{prox}_{t} (x) = \arg\min_z \frac{1}{2t} ||x-z||_2^2 + h(z)
\qquad (3)
$$</p>
<p>Üstteki $x$'in bir fonksiyonu olarak $\arg\min$'de gösterilen kriteri
minimize eden $z$'yi buluyor. Operatör $t$'ye bağlı, dışarıdan verilen
parametre. Tabii ki $h$'ye de bağlı, ki bazen üstteki operatörü
$\mathrm{prox}_{h,t}$ olarak gösteren de oluyor. </p>
<p>Üstteki ifadenin eşleme olduğunu kontrol edelim. Ciddi tanımlı bir
fonksiyon üstteki değil mi? Ona bir $x$ veriyorsunuz, o da size tek bir
sonuç döndürüyor. Ayrıca bu eşleme / fonksiyonun kendisi bir
minimizasyon. Peki bu minimizasyon problemi dışbükey mi? Evet. Peki bu
problemin özgün bir sonucu var mıdır? Vardır çünkü üstteki problem harfiyen
dışbükey. Değil mi? Eğer $h$ harfiyen dışbükey olmasa bile ifadenin tümü
harfiyen dışbükey olurdu çünkü $\frac{1}{2t} ||x-z||_2^2 $ harfiyen
dışbükey [1, 55:09], $z-x$'in karesi var [ayrıca $x$ değişkeni $h$'ye
geçilmiyor].</p>
<p>Yani harfiyen dışbükey, o zaman özgün sonuç var. Biz de bu özgün sonucu
alarak bir iniş algoritması yazıyoruz [1, 56:28], </p>
<p>$$
x^{(k)} = \mathrm{prox}_{t_k} \big( 
x^{(k-1)} - t_k \nabla g(x^{(k-1)})
\big), \quad k=1,2,...
\qquad (2)
$$</p>
<p>Güncelleme adımını tanıdık şekilde yazmak için </p>
<p>$$
x^{(k)} = x^{(k-1)} - t_k \cdot G_{t_k} (x^{(k-1)})
$$</p>
<p>ki $G_t$'ye $f$'nin genelleştirilmiş gradyanı denebilir,</p>
<p>$$
G_t(x) = \frac{x - \mathrm{prox}_{t}(x - t \nabla g(x))}{t}
$$</p>
<p>$G$'nin dışbükey fonksiyonların alışılageldik gradyanlarına benzer pek çok
özelliği vardır, ki bu özellikler proksimal metotların yakınsadığıyla
alakalı ispatlarda kullanılabilir.</p>
<p>Şimdiye kadar anlattıklarımıza bakanlara bu komik bir hikaye gibi
gelebilir. Bir $g + h$ toplamını minimize etmek istiyordum, bunu yapabilmek
için (2) formunda adımlar atacağım, bir $\mathrm{prox}$ operatörüm var, ama bu
operatör bir sonuç döndürüyor aslında, ve bu sonuç bir başka
minimizasyondan geliyor. Yani $g + h$ türü bir toplam minimizasyonu yerine
her adımda, bir sürü minimizasyonları koymuş oldum. Bu nasıl daha iyi bir
sonuç verecek ki?</p>
<p>Şunu belirtmek lazım, sadece eğer proksimal gradyanları analitik, ya da
hızlı bir şekilde hesaplayabiliyorsak onları çözüm için düşünürüz. Yani her
ne kadar her adımda (3) turu optimizasiyonlar yapıyorsak ta, bunu pürüzsüz
olan kısım $h$ yeterince basit olduğu zaman yapıyoruz ki tüm (3) için
analitik  ya da hızlı hesapsal çözüm olsun [1, 58:54]. </p>
<p>Diğer noktalar, dikkat edersek proksimal operatör $g$'ye bağlı değil,
tamamen $h$ bazlı. Eğer $h$ basit ise ama $g$ müthiş çetrefil ise bu
proksimal hesaplarını çok zorlaştırmıyor. Eğer o çok çetrefil (ve pürüzsüz)
$g$ için gradyan hesaplanabiliyorsa, durumu kurtardık demektir. </p>
<p>Tekrar bir Lasso problemi göreceğiz. Bu Lasso için gördüğümüz ikinci
algoritma, ve belirtmek gerekir ki Proksimal metot altgradyan metotuna göre
çok daha verimlidir, hızlıdır.</p>
<p>Verili bir $y \in \mathbb{R}^n$, $X \in \mathbb{R}^{n \times p}$ için Lasso
kriterini hatırlarsak, </p>
<p>$$
f(\beta) = \frac{1}{2} || y - X \beta ||_2^2 + \lambda ||\beta||_1 
$$</p>
<p>Kriterdeki ilk terim en az kareler kayıp fonksiyonu, ikinci terim bir ayar
parametresi üzerinden katsayıların 1. normu. Bu kriteri pürüzsüz, ve
pürüzsüz olmayan ama basitçe olan iki kısma ayıracağız, yani zaten oldukca
bariz, pürüzsüz kısım 1. terim, $g(\beta)$ diyelim, olmayan 2. terim,
$h(\beta)$ diyelim. Proksimal gradyan inişi için bize iki şey gerekiyor,
birincisi $g$'nin gradyanı, ikincisi $h$ için $\mathrm{prox}$ operatörünü
hesaplayabilmek.</p>
<p>$g$'nin gradyanı oldukca basit, onu bu noktada uykumuzda bile bulabiliyor
olmamız lazım. $h$'nin $\mathrm{prox}$ operatörü,</p>
<p>$$
\mathrm{prox}_t(\beta) = \arg\min_z \frac{1}{2t} ||\beta-z||_2^2 + \lambda ||z||_1
$$</p>
<p>Her şeyi $t$ ile çarparsam,</p>
<p>$$
\mathrm{prox}_t(\beta) = \arg\min_z \frac{1}{2} ||\beta-z||_2^2 + \lambda t ||z||_1
$$</p>
<p>Üstteki minimizasyonun çözümünü daha önce altgradyanlar üzerinden
görmüştük, </p>
<p>$$
= S_{\lambda t}(\beta)
$$</p>
<p>Yine yumuşak eşikleme (soft-threshold) operatörüne gelmiş olduk, </p>
<p>$$
[ S_{\lambda} (\beta) ]_i = 
\left\{ \begin{array}{ll}
\beta_i - \lambda &amp; \textrm{eğer } \beta_i &gt; \lambda_i \\
0 &amp; \textrm{eğer } -\lambda \ge \beta_i \ge \lambda, \quad i=1,..,n \\
\beta_i + \lambda &amp; \textrm{eğer } \beta_i &lt; -\lambda_i 
\end{array} \right.
$$</p>
<p>Tüm algoritma neye benziyor? Önce $g$'ye göre bir graydan güncellemesi
yaparım, gradyan</p>
<p>$$
\nabla g(\beta) = -X^T (y-X\beta)
$$</p>
<p>O zaman güncelleme </p>
<p>$$\beta + t X^T (y-X\beta)$$ </p>
<p>olur. Buna $\mathrm{prox}$ uygularsak,</p>
<p>$$
\beta^+ = S_{\lambda t}(\beta + t X^T (y-X\beta))
$$</p>
<p>Yumuşak eşikleme ne yapar? Her ögeye teker teker bakar, eğer mutlak değeri
çok ufaksa onu ya sıfıra eşitler, ya da onu $\lambda \cdot t$ kadar sıfıra
yaklaştırır. </p>
<p>Üstteki Lasso algoritmasina İSTA adı da verilir. </p>
<p>Geriye Çizgisel İz Sürme  (Backtracking line search)</p>
<p>Graydan inişinde görmüştük ki adım büyüklüklerini dinamik olarak
seçebiliyorduk, her adımdaki duruma adapte olabiliyorduk, tipik olarak
Lipschitz sabitini bilmiyoruz çünkü. O sebeple geriye iz sürme pratikte
iyi işlemesiyle beraber, teorik olarak yakınsamayı da garantiliyordu. </p>
<p>Proksimal gradyanları için benzer bir kavram geçerli. Ayrıca proksimal
durumda geriye doğru iz sürmenin birden fazla yolu var. </p>
<p>[atlandı]</p>
<p>Örnek kod, Lasso problem çözümü [2]</p>
<pre><code class="python">import pandas as pd
diabetes = pd.read_csv(&quot;../../stat/stat_120_regular/diabetes.csv&quot;,sep=';')
y = np.array(diabetes['response'].astype(float)).reshape(442,1)
X = np.array(diabetes.drop(&quot;response&quot;,axis=1))
N,dim = X.shape
print (N,dim)

lam = 1/np.sqrt(N);
w = np.matrix(np.random.multivariate_normal([0.0]*dim, np.eye(dim))).T

L = (np.linalg.svd(X)[1][0])**2
print(L)
max_iter = 500

def obj(w):
    r = X*w-y;
    return np.sum(np.multiply(r,r))/2 +  lam * np.sum(np.abs(w))

def f_grad(w):
    return  X.T*(X*w-y) 

def soft_threshod(w,mu):
    return np.multiply(np.sign(w), np.maximum(np.abs(w)-mu,0))  

w = np.matrix([0.0]*dim).T
for t in range(0, max_iter):
    obj_val = obj(w)
    w = w - (1/L)* f_grad(w)
    w= soft_threshod(w,lam/L)    
    if (t % 50==0):
        print('iter= {},\tobjective= {:3f}'.format(t, obj_val.item()))

print (w)
</code></pre>

<pre><code>442 10
4.0242141761466925
iter= 0,    objective= 6425460.500000
iter= 50,   objective= 5751070.568959
iter= 100,  objective= 5750285.357193
iter= 150,  objective= 5749670.506866
iter= 200,  objective= 5749177.635558
iter= 250,  objective= 5748779.527464
iter= 300,  objective= 5748457.810485
iter= 350,  objective= 5748197.804952
iter= 400,  objective= 5747987.670443
iter= 450,  objective= 5747817.840900
[[  -8.71913404]
 [-238.35531517]
 [ 522.93302022]
 [ 323.11825944]
 [-526.09642955]
 [ 265.58097894]
 [ -17.84381222]
 [ 143.15165377]
 [ 652.14114865]
 [  68.55685031]]
</code></pre>

<p>Kaynaklar</p>
<p>[1] Tibshirani, <em>Convex Optimization, Lecture Video 8</em>, 
<a href="https://www.youtube.com/channel/UCIvaLZcfz3ikJ1cD-zMpIXg">https://www.youtube.com/channel/UCIvaLZcfz3ikJ1cD-zMpIXg</a></p>
<p>[2] He, <em>IE 598 - Big Data Optimization</em>,<br />
    <a href="http://niaohe.ise.illinois.edu/IE598_2016/">http://niaohe.ise.illinois.edu/IE598_2016/</a></p>