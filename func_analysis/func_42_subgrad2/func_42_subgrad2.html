<h1>Altgradyanlar (Subgradients) - 2</h1>
<!DOCTYPE html>
<html>
  <head>
    <title>Altgradyanlar (Subgradients) - 2
</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {inlineMath: [["$","$"]]}
      });
    MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
      MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
        cancel: ["Extension","cancel"],
        bcancel: ["Extension","cancel"],
        xcancel: ["Extension","cancel"],
        cancelto: ["Extension","cancel"]
      });
    });
    </script>
<script type="text/javascript"
   src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML-full">
</script>
</head>

<p>Bir dışbükey kümesi üzerinden tanımlı ama pürüzsüz olmayabilecek bir
dışbükey fonksiyonu minimize etmek için yansıtılan altgradyan (projected
subgradient) metotu adlı bir metot ta kullanılabilir [1, 22:04].  </p>
<p>Dışbükey $f$'yi dışbükey küme $C$ üzerinden optimize etmek için </p>
<p>$$
\min_x f(x) \quad \textrm{öyle ki} \quad x \in C
$$</p>
<p>$$
x^{(k)} = P_C \left( x^{(k-1)} - t_k g_k^{(k-1)} \right)
$$</p>
<p>Bu metot normal altgradyan metotu gibi, tek fark parantez içinde görülen
altgradyan adımı atıldıktan sonra elde edilen sonucun $C$ kümesine geri
yansıtılması (projection), çünkü atılan adım sonucunda olurlu bir sonuç
elde etmemiş olabiliriz. </p>
<p>Yakınsama analizi normal altgraydan metotuna benziyor, bu metotla da benzer
yakınsama garantisi elde edilebiliyor, yakınsama oranı da buna dahil.</p>
<p>Yansıtma adımı bazen zor olabilir, hangi durumlarda kolay olduğunun listesi
aşağıda [1, 23:53]. Hangi kümelere yansıtmak kolaydır?</p>
<p>1) Doğrusal görüntüler: ${  Ax + b: x \in \mathbb{R}^n  }$</p>
<p>2) ${ x: Ax = b }$ sisteminin çözüm kümesi </p>
<p>3) Negatif olmayan bölge: $\mathbb{R}_{+}^n = {x: x \ge 0 }$. Verilen
vektörün negatif değerlerinin sıfır yapmak, pozitifleri tutmak bize o
vektörün negatif olmayan bölge karşılığını veriyor. </p>
<p>4) Bazı norm topları ${ x: ||x||_p \le 1 }$, $p=1,2,..,\infty$ için. Daha
önce 2-norm topuna yansıtmayı gördük, vektörü alıp normalize edersek bu
kümeye yansıtma yapmış oluyoruz aslında. Bu yansıtma bizi o vektörün 2-norm
topundaki en yakın diğer vektöre götürüyor. Sonsuz norm kolay, bir kutuya
yansıtma yapmış oluyoruz, bunun ne demek olduğunu düşünmeyi size
bırakıyorum, 1-norm en zoru. </p>
<p>5) Bazı çokyüzlüler (polyhedra) ve basit koniler.</p>
<p>Bir uyarıda bulunalım, tanımı basit duran kümeler ortaya çıkartmak
kolaydır, fakat bu kümelere yansıtma yapan operatörler çok zor
olabilir. Mesela gelişigüzel bir çokyüzlü $C = { x: Ax \le b }$ kümesine
yansıtma yapmak zordur. Bu problemin kendisi apayrı bir optimizasyon
problemi aslında, bir QP. </p>
<p>[stochastic subgradient method atlandı]</p>
<p>Altgradyanların iyi tarafı genel uygulanabilirlik. Altgradyan metotları
dışbükey, Lipschitz fonksiyonları üzerinde bir anlamda optimaldir. Ünlü
bilimci Neşterov'un bu bağlamda bir teorisi vardır [1, 45:12], pürüzsüz
durumlarda ve 1. derece yöntemlerde altgradyanların lineer kombinasyonları
üzerinden güncellemenin sonucu olan adımların başarısına bir alt sınır
tanımlar. </p>
<p>Proksimal / Yakınsal Gradyan Metotu (Proximal Gradient Method) </p>
<p>Bu metot, herhangi bir pürüzsüz olmayan fonksiyonu optimize etmeye uğraşmak
yerine belli bir yapıya uyan çetrefil fonksiyonları optimize etmeye
uğraşır. Bu yapı</p>
<p>$$
f(x) = g(x) + h(x) \qquad (1)
$$</p>
<p>formundadır [1, 45:59]. $g,h$'in ikisi de dışbükey. $g$'nin pürüzsüz,
türevi alınabilir olduğu farz edilir (çoğunlukla oldukca çetrefil olabilir)
ve $\mathrm{dom}(g) = \mathbb{R}^n$, $h$ ise pürüzsüz olmayabilir. Tabii pürüzsüz
artı pürüzsüz olmayan iki fonksısyon toplamı kriterin tamamını pürüzsüz
olmayan hale çevirir. Ama bu toplam daha basittir denebilir, onun üzerinde
proksimal operatörü uygulanabilir.</p>
<p>Hatırlarsak eğer $f$ türevi alınabilir olsaydı gradyan iniş güncellemesi </p>
<p>$$
x^{+} = x - t \cdot \nabla f(x)
$$</p>
<p>Bu formüle erişmenin bir yöntemi karesel yaklaşıklama üzerinden idi,
$f$'nin $x$ etrafındaki yaklaşıklamasında $\nabla^2f(x)$ yerine
$\frac{1}{t} I$ koyunca,</p>
<p>$$
x^{+} = \arg\min_x f(x) + 
\underbrace{
  \nabla f(x)^T (z-x) + \frac{1}{2t} ||z-x||_2^2 
}_{f_t(z)}
$$</p>
<p>elde ediliyordu. Yani gradyan inişi sanki ardı ardına geldiği her noktada
bir karesel yaklaşıklama yapıyor, ve onu adım atarak minimize etmeye
uğraşıyor.</p>
<p>Ama (1)'deki $f$ pürüzsüz değil, o sebeple üstteki mantık ise
yaramayacak. Fakat, belki de karesel yaklaşıklamanın bir kısmını hala
kullanabiliriz, ve minimizasyonu sadece $g$'ye uygularız çünkü pürüzsüz
olan kısım o. Yani niye $g$'nin yerine karesel yaklaşıklama koymayalım?
Şimdi bunu yapacağız [1, 50:10]. </p>
<p>$$
x^{+} = \arg\min_z \tilde{g}_t(z) + h(z)
$$</p>
<p>ki $\tilde{g}_t(z)$ $g$'nin $x$ noktası etrafındaki karesel yaklaşıklaması
oluyor.  Eğer elimizde $h$ olmasaydı üstteki sadece bir gradyan
güncellemesine indirgenebilirdi. Neyse açılımı yaparsak</p>
<p>$$
= \arg\min_z g(x) + \nabla g(x)^T (z-x) + \frac{1}{2t} ||z-x||_2^2 + h(z)
$$</p>
<p>Daha uygun bir formda yazabiliriz, $z$'nin $g$ üzerindeki değişikliğe
karesel uzaklığı olarak,</p>
<p>$$
= \arg\min_z \frac{1}{2t} 
\big|\big| z - \big( x-t\nabla g(x) \big) \big|\big|_2^2 + 
h(z)
$$</p>
<p>Yani söylenmek istenen, hem sadece $g$ olsaydı atacağımız gradyan adımına
yakın durmaya çalışmak, hem de $h$'nin kendisini ufak tutmak. Dışarıdan
ayarlanan $t$ parametresi $g$ gradyan adımı ile $h$ arasında bir denge
kurmak gibi görülebilir, eğer $t$ ufak ise o zaman gradyan adımına yakın
durmaya daha fazla önem atfetmiş olacağız, $h$'nin ufak tutulmasına daha
az. $t$ çok büyük ise tam tersi olacak. </p>
<p>Bu aslında kabaca Proksimal Gradyan İnişi yöntemini tarif etmiş
oluyor. Şimdi proksimal eşlemesi operatörünü göstereceğiz, ki bu
algoritmaları daha temiz olarak yazmamıza yardımcı olacak. </p>
<p>Bir $h$ fonksiyonu için bir $\mathrm{prox}$ operatörü tanımlıyoruz, </p>
<p>$$
\mathrm{prox}_{t} (x) = \arg\min_z \frac{1}{2t} ||x-z||_2^2 + h(z)
$$</p>
<p>Ustteki $x$'in bir fonksiyonu olarak $\arg\min$'de gosterilen kriteri
minimize eden $z$'yi buluyor. Operator $t$'ye bagli, disaridan verilen
parametre. Tabii ki $h$'ye de bagli, ki bazen ustteki operatoru
$\mathrm{prox}_{h,t}$ olarak ta gorebiliyoruz. </p>
<p>Üstteki ifadenin eşleme olduğunu kontrol edelim. Ciddi tanımlı bir
fonksiyon üstteki değil mi? Ona bir $x$ veriyorsunuz, o da size tek bir
sonuç donduruyor. Ayrıca bu eşleme / fonksiyonun kendisi bir
minimizasyon. Peki bu minimizasyon problemi dışbükey mi? Evet. Peki bu
problemin özgün bir sonucu var mıdır? Vardır çünkü üstteki problem harfiyen
dışbükey. Değil mi? Eğer $h$ harfiyen dışbükey olmasa bile ifadenin tümü
harfiyen dışbükey olurdu çünkü $\frac{1}{2t} ||x-z||_2^2 $ harfiyen
dışbükey, $z-x$'in karesi var [ayrıca $x$ değişkeni $h$'ye geçilmiyor]. </p>
<p>[devam edecek]</p>
<p>Proksimal [2]</p>
<pre><code class="python">N = 100
dim = 30
lamda = 1/np.sqrt(N);
np.random.seed(50)
w = np.matrix(np.random.multivariate_normal([0.0]*dim, np.eye(dim))).T
X = np.matrix(np.random.multivariate_normal([0.0]*dim, np.eye(dim), size = N))
y = X*w

L = (np.linalg.svd(X)[1][0])**2
print(L)
max_iter = 100

def obj(w):
    r = X*w-y;
    return np.sum(np.multiply(r,r))/2 +  lamda * np.sum(np.abs(w))

def f_grad(w):
    return  X.T*(X*w-y) 

def soft_threshod(w,mu):
    return np.multiply(np.sign(w), np.maximum(np.abs(w)- mu,0))  

w = np.matrix([0.0]*dim).T
obj_PG = []
for t in range(0, max_iter):
    obj_val = obj(w)
    w = w - (1/L)* f_grad(w)
    w= soft_threshod(w,lamda/L)

    obj_PG.append(obj_val.item())
    if (t % 5==0):
        print('iter= {},\tobjective= {:3f}'.format(t, obj_val.item()))
</code></pre>

<pre><code>228.3801231037125
iter= 0,    objective= 1761.785102
iter= 5,    objective= 28.002804
iter= 10,   objective= 4.813366
iter= 15,   objective= 2.892747
iter= 20,   objective= 2.559209
iter= 25,   objective= 2.465096
iter= 30,   objective= 2.433524
iter= 35,   objective= 2.422408
iter= 40,   objective= 2.418440
iter= 45,   objective= 2.417017
iter= 50,   objective= 2.416505
iter= 55,   objective= 2.416321
iter= 60,   objective= 2.416255
iter= 65,   objective= 2.416231
iter= 70,   objective= 2.416222
iter= 75,   objective= 2.416219
iter= 80,   objective= 2.416218
iter= 85,   objective= 2.416218
iter= 90,   objective= 2.416218
iter= 95,   objective= 2.416218
</code></pre>

<p>Kaynaklar</p>
<p>[1] Tibshirani, <em>Convex Optimization, Lecture Video 8</em>, 
<a href="https://www.youtube.com/channel/UCIvaLZcfz3ikJ1cD-zMpIXg">https://www.youtube.com/channel/UCIvaLZcfz3ikJ1cD-zMpIXg</a></p>
<p>[2] He, <em>IE 598 - BIG DATA OPTIMIZATION</em>,<br />
    <a href="http://niaohe.ise.illinois.edu/IE598_2016/">http://niaohe.ise.illinois.edu/IE598_2016/</a></p>