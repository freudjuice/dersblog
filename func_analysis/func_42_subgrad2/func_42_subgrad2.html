<h1>Altgradyanlar (Subgradients) - 2</h1>
<!DOCTYPE html>
<html>
  <head>
    <title>Altgradyanlar (Subgradients) - 2
</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {inlineMath: [["$","$"]]}
      });
    MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
      MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
        cancel: ["Extension","cancel"],
        bcancel: ["Extension","cancel"],
        xcancel: ["Extension","cancel"],
        cancelto: ["Extension","cancel"]
      });
    });
    </script>
<script type="text/javascript"
   src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML-full">
</script>
</head>

<p>Bir dışbükey kümesi üzerinden tanımlı ama pürüzsüz olmayabilecek bir
dışbükey fonksiyonu minimize etmek için yansıtılan altgradyan (projected
subgradient) metotu adlı bir metot ta kullanılabilir [1, 22:04].  </p>
<p>Dışbükey $f$'yi dışbükey küme $C$ üzerinden optimize etmek için </p>
<p>$$
\min_x f(x) \quad \textrm{öyle ki} \quad x \in C
$$</p>
<p>$$
x^{(k)} = P_C \left( x^{(k-1)} - t_k g_k^{(k-1)} \right)
$$</p>
<p>Bu metot normal altgradyan metotu gibi, tek fark parantez içinde görülen
altgradyan adımı atıldıktan sonra elde edilen sonucun $C$ kümesine geri
yansıtılması (projection), çünkü atılan adım sonucunda olurlu bir sonuç
elde etmemiş olabiliriz. </p>
<p>Yakınsama analizi normal altgraydan metotuna benziyor, bu metotla da benzer
yakınsama garantisi elde edilebiliyor, yakınsama oranı da buna dahil.</p>
<p>Yansıtma adımı bazen zor olabilir, hangi durumlarda kolay olduğunun listesi
aşağıda [1, 23:53]. Hangi kümelere yansıtmak kolaydır?</p>
<p>1) Doğrusal görüntüler: ${  Ax + b: x \in \mathbb{R}^n  }$</p>
<p>2) ${ x: Ax = b }$ sisteminin çözüm kümesi </p>
<p>3) Negatif olmayan bölge: $\mathbb{R}_{+}^n = {x: x \ge 0 }$. Verilen
vektörün negatif değerlerinin sıfır yapmak, pozitifleri tutmak bize o
vektörün negatif olmayan bölge karşılığını veriyor. </p>
<p>4) Bazı norm topları ${ x: ||x||_p \le 1 }$, $p=1,2,..,\infty$ için. Daha
önce 2-norm topuna yansıtmayı gördük, vektörü alıp normalize edersek bu
kümeye yansıtma yapmış oluyoruz aslında. Bu yansıtma bizi o vektörün 2-norm
topundaki en yakın diğer vektöre götürüyor. Sonsuz norm kolay, bir kutuya
yansıtma yapmış oluyoruz, bunun ne demek olduğunu düşünmeyi size
bırakıyorum, 1-norm en zoru. </p>
<p>5) Bazı çokyüzlüler (polyhedra) ve basit koniler.</p>
<p>Bir uyarıda bulunalım, tanımı basit duran kümeler ortaya çıkartmak
kolaydır, fakat bu kümelere yansıtma yapan operatörler çok zor
olabilir. Mesela gelişigüzel bir çokyüzlü $C = { x: Ax \le b }$ kümesine
yansıtma yapmak zordur. Bu problemin kendisi apayrı bir optimizasyon
problemi aslında, bir QP. </p>
<p>[stochastic subgradient method atlandı]</p>
<p>Altgradyanların iyi tarafı genel uygulanabilirlik. Altgradyan metotları
dışbükey, Lipschitz fonksiyonları üzerinde bir anlamda optimaldir. Ünlü
bilimci Neşterov'un bu bağlamda bir teorisi vardır [1, 45:12], pürüzsüz
durumlarda ve 1. derece yöntemlerde altgradyanların lineer kombinasyonları
üzerinden güncellemenin sonucu olan adımların başarısına bir alt sınır
tanımlar. </p>
<p>Proksimal / Yakınsal Gradyan Metotu (Proximal Gradient Method) </p>
<p>Bu metot, herhangi bir pürüzsüz olmayan fonksiyonu optimize etmeye uğraşmak
yerine belli bir yapıya uyan çetrefil fonksiyonları optimize etmeye
uğraşır. Bu yapı</p>
<p>$$
f(x) = g(x) + h(x)
$$</p>
<p>formundadir. </p>
<p>Kaynaklar</p>
<p>[1] Tibshirani, <em>Convex Optimization, Lecture Video 8</em>, 
<a href="https://www.youtube.com/channel/UCIvaLZcfz3ikJ1cD-zMpIXg">https://www.youtube.com/channel/UCIvaLZcfz3ikJ1cD-zMpIXg</a></p>