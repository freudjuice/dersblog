<h1>BFGS</h1>
<!DOCTYPE html>
<html>
  <head>
    <title>BFGS 
</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {inlineMath: [["$","$"]]}
      });
    MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
      MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
        cancel: ["Extension","cancel"],
        bcancel: ["Extension","cancel"],
        xcancel: ["Extension","cancel"],
        cancelto: ["Extension","cancel"]
      });
    });
    </script>
<script type="text/javascript"
   src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML-full">
</script>
</head>

<p>Newton yöntemine göre özyineli döngü şu şekilde belirtilir, </p>
<p>$$
x_{i+1} = x_i - H_i^{-1} \nabla f (x_i)<br />
$$</p>
<p>ki $f$ fonksiyonunun $H$ Hessian matrisidir. Newton-imsi (quasi-Newton)
metotlarda ya $H_i$ matrisi bir diğer $A_i$ ile yaklaşık olarak temsil
edilir (ya da $H_i ^{-1}$ matrisi $B_i$ ile). O zaman ustteki denklem yerine</p>
<p>$$
x_{i+1} = x_i - \lambda_i B_i \nabla f(x_i)
$$</p>
<p>ki $\lambda_i$ $s_i = -\lambda_i B_i \nabla f(x_i)$ yönünde bir optimal
adım uzunluğudur.</p>
<p>[devam edecek]</p>
<pre><code class="python">import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import numpy.linalg as lin

def rosen(x):
    y = 100*(x[1]-x[0]**2)**2+(1-x[0])**2;

    gy =[-400*(x[1]-x[0]**2)*x[0]-2*(1-x[0]),
         200*(x[1]-x[0]**2)]

    return y,gy

def linesearch_secant(f, d, x):
    epsilon=10**(-5)
    max = 500
    alpha_curr=0
    alpha=10**-5
    y,grad=f(x)
    dphi_zero=np.dot(np.array(grad).T,d)

    dphi_curr=dphi_zero
    i=0;
    while np.abs(dphi_curr)&gt;epsilon*np.abs(dphi_zero):
        alpha_old=alpha_curr
        alpha_curr=alpha
        dphi_old=dphi_curr
        y,grad=f(x+alpha_curr*d)
        dphi_curr=np.dot(np.array(grad).T,d)
        alpha=(dphi_curr*alpha_old-dphi_old*alpha_curr)/(dphi_curr-dphi_old);
        i += 1
        if (i &gt;= max) and (np.abs(dphi_curr)&gt;epsilon*np.abs(dphi_zero)):
            print('Line search terminating with number of iterations:')
            print(i)
            print(alpha)
            break

    return alpha

x=np.array([-1.0,0])

H = np.eye(2)

tol = 1e-20

y,grad = rosen(x)

dist=2*tol
epsilon = tol

iter=0;

while lin.norm(grad)&gt;1e-6:
    value,grad=rosen(x)
    p=np.dot(-H,grad)
    lam = linesearch_secant(rosen,p,x)
    iter += 1
    xt = x
    x = x + lam*p
    s = lam*p
    dist=lin.norm(s)
    newvalue,newgrad=rosen(x)
    y = np.array(newgrad)-grad
    rho=1/np.dot(y.T,s)
    s = s.reshape(2,1)
    y = y.reshape(2,1)
    tmp1 = np.eye(2)-rho*np.dot(s,y.T)
    tmp2 = np.eye(2)-rho*np.dot(y,s.T)
    tmp3 = rho*np.dot(s,s.T)
    H= np.dot(np.dot(tmp1,H),tmp2) + tmp3
    print ('lambda:',lam)

print (xt)
print ('iter',iter)

</code></pre>

<p>Kaynaklar </p>
<p>Dutta, <em>Optimization in Chemical Engineering</em></p>
<p>Zak, <em>An Introduction to Optimization, 4th Edition</em></p>