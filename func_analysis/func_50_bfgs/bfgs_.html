<h1>BFGS</h1>
<!DOCTYPE html>
<html>
  <head>
    <title>BFGS 
</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {inlineMath: [["$","$"]]}
      });
    MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
      MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
        cancel: ["Extension","cancel"],
        bcancel: ["Extension","cancel"],
        xcancel: ["Extension","cancel"],
        cancelto: ["Extension","cancel"]
      });
    });
    </script>
<script type="text/javascript"
   src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML-full">
</script>
</head>

<p>Newton yöntemine göre özyineli döngü şu şekilde belirtilir, </p>
<p>$$
x_{i+1} = x_i - H_i^{-1} \nabla f (x_i)<br />
$$</p>
<p>ki $f$ fonksiyonunun $H$ Hessian matrisidir. Newton-imsi (quasi-Newton)
metotlarda ya $H_i$ matrisi bir diğer $A_i$ ile yaklaşık olarak temsil
edilir (ya da $H_i ^{-1}$ matrisi $B_i$ ile). O zaman ustteki denklem yerine</p>
<p>$$
x_{i+1} = x_i - \lambda_i B_i \nabla f(x_i)
$$</p>
<p>ki $\lambda_i$ $s_i = -\lambda_i B_i \nabla f(x_i)$ yönünde bir optimal
adım uzunluğudur.</p>
<p>[devam edecek]</p>
<pre><code class="python">from numpy import *
from math import *
from numpy.linalg import *

rosen = lambda x: (1-x[0])**2 + 100*(x[1]-x[0]**2)**2

def rosen_d(x):
    return array([2*100*(x[1] - x[0]**2)*(-2*x[0]) - 2*(1.-x[0]), 2*100*(x[1]-x[0]**2)])

def BFGS(F,Fprime, Start, epsi = 10e-8, tol = 10e-6, sigma= 10**-1, beta = 10 ): 
    def LineSearch(g,x,s,sigma= 10**-1, beta = 10, convergval = 0.00001):
        def QFind(alpha):
            if abs(alpha) &lt; convergval: return 1
            return (F(x + alpha*s) - F(x))/(alpha * dot(g,s))

        alpha = 1.

        while QFind(alpha) &gt;= sigma:
            alpha=alpha * 2

        while QFind(alpha)&lt; sigma:
            alphap=alpha / ( 2.0* ( 1- QFind(alpha)))
            alpha=max(1.0/beta * alpha, alphap)

        return alpha

    x = Start
    xold = inf
    N = shape(x)[0]
    H = 1.0 * eye(N)
    counter = 1
    alpha = 1
    g = Fprime(x)
    while norm(g) &gt; epsi and norm(xold - x) &gt; tol:
        s = -dot(H,g)
        alpha = LineSearch(g,x,s)
        x = x+alpha*s
        gold = g
        g = Fprime(x)
        y = (g - gold)/alpha
        dotsy = dot(s,y)
        if dotsy&gt;0:
            z = dot(H,y)
            H += outer(s,s)*(dot(s,y) + dot(y, z))/dotsy**2 - \
                 (outer(z,s)+ outer(s, z)) / dotsy
        counter+=1
    return (x , counter)

(result, counter) =  BFGS(rosen, rosen_d, array([-1,0]))
print (&quot;Custom BFGS:&quot;)
print (&quot;Final Result: &quot; + str(result))
print (&quot;Iteration Count: &quot; + str(counter))

</code></pre>

<p>Kaynaklar </p>
<p>Dutta, <em>Optimization in Chemical Engineering</em></p>