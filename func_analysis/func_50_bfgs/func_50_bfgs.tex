\documentclass[12pt,fleqn]{article}\usepackage{../../common}
\begin{document}
Newton-umsu Matotlar (Quasi-Newton Methods), DFP, BFGS 

Bir $f$ hedef fonksiyonunun minimizasyonu için Newton'un özyineli
algoritmasi

$$
x^{k+1} = x^k - F(x^k)^{-1} g^k
$$

ki $g$ gradyan, $F$ ise Hessian. 

Ya da

$$
x^{k+1} = x^k - (\nabla^2 f(x^k))^{-1} \nabla f(x^k)
$$

Newton'umsu metotlarýn ana fikri Hessian matrisi yerine sadece gradyan
bilgisini kullanarak yaklaþýk bir $F_k$ kullanmak. Bunu nasýl yapacaðýz? 

$f(\cdot)$'un karesel olarak temsilini yazalým, özyineli gidiþat sýrasýnda,
bir herhangi bir $x^{k+1}$ etrafýnda Taylor açýlýmý

$$
m_k(x) \equiv f(x^{k+1}) + \nabla f(x^{k+1})^T (x-x^{k+1}) + 
\frac{1}{2} (x-x^{k+1}) ^T F_{t+1}^T (x-x^{k+1}) 
$$

Eger gradyani alirsak 

$$
\nabla m_k(x) = \nabla f(x^{k+1}) + F_{k+1}^{-1} (x-x^{k+1})
$$

Þimdi $k$ ve $k+1$ noktalarý, gradyanlarý üzerinden bir $H^{k+1}$ iliþkisi
ortaya çýkartmak istiyoruz ki çözüp bir sonuç elde edebilelim. Ek
denklemler elde etmek için þu akla yatkýn þartlarý öne sürebiliriz, $m$ ve
$f$ gradyanlarý birbirine uysun. Yani,

$$
\nabla m_k(x) = \nabla f(x_k)
$$

O zaman, ``Newton-umsu metot þartý'' da denen iki üstteki denklemle
beraber, ve açýlýmda $x$ herhangi bir $x$ olabileceði için onun yerine
$x^k$ kullanarak,

$$
\nabla f(x^{k+1}) + F_{k+1}^{-1} (x^k-x^{k+1}) = \nabla f(x^k)
$$

$$
H_{k+1}^{-1} (x^k-x^{k+1}) = \nabla f(x^k) - \nabla f(x^{k+1}) 
$$

$$
H_{k+1}^{-1} (x^{k+1}-x^k) = \nabla f(x^{k+1}) - \nabla f(x^k) 
$$

Üsttekine sekant denklemi adý veriliyor, þu figürle alakalý, 

\includegraphics[width=20em]{func_50_bfgs_01.png}

Yani sekant denklemine göre $F_{k+1}^{-1}$ deðeri, yatay kordinattaki
$x^{k+1}-x^k$ deðiþimini, gradyan deðiþimi $\nabla f(x^{k+1}) - \nabla
f(x^k)$'e taþýyor / eþliyor [4].

Kýsaltma amaçlý,

$$
F_{k+1}^{-1} \underbrace{(x^{k+1}-x^k)}_{y_t} = 
\underbrace{\nabla f(x^{k+1}) - \nabla f(x^k)}_{s_t}
$$

$$
F_{k+1}^{-1} y_k = s_k
\mlabel{1}
$$

Ve $F_{k}^{-1}$ yerine $H_k$ diyelim. 

Özyineli baðlamda bir $H_0$'dan baþlayarak ufak deðiþimlerle sonuca
ulaþýlmaya uðraþýlýr. Deðiþimlerin ufak olmasý gerekliliði üzerinden ve bu
deðiþimlerin kerte 1 eki ile olmasý sonucu [4]'teki matris normu ile
beraber aslýnda birazdan türeteceðimiz güncelleme denklemi
alýnabiliyor. Kerte 1 eki konusu için bkz [5]. Biz farklý bir yönden, eðer
ufak deðiþim kerte 1 ve 2 ile yapýlsa nereye varýlacaðýna bakacaðýz [1,
sf. 111].

Kerte 1 eki ile $H_k$'yi $H_{k+1}$ yapmak demek aslýnda

$$
H_{k+1} = H_k + czz^T 
$$

demektir. Bunu iki üstteki formül içine koyarsak

$$
s_k = (H_k + czz^T) y_k = H_k y_k + cz (z^T y_k)
$$

$z^T y_k$ bir skalar olduðu için 

$$
cz = \frac{s_k - H_k y_k}{z^T y_k}
$$

Bu denklemi çözen en basit $c,z$ seçenekleri

$$
z = s_k - H_ky_k
$$

$$
c = \frac{1}{z^Ty_k}
$$

Bu bize kerte 1 güncelleme formülünü verir,

$$
H_{k+1} = H_k + \frac{ (s-H_ky_k) (s-H_ky_k)^T }{(s-H_ky_k) y_k}
$$

Ne yazýk ki kerte 1 güncelemesinin bazý problemleri var. Bunlardan en
önemlisi güncelleme sonrasý elde edilen yeni $H_{k+1}$'in pozitif kesin
olmasýnýn garanti olmamasý, bu sebeple bir sonraki döngüde elde edilecek
yön $d_k = -H_k \nabla f(x_k)$'nin bir iniþ yönü olmasýnýn garantisinin de
tehlikeye girmesi. 

Çözüm olarak $H_{k+1}$'in pozitif kesin kalmasýný garantileyecek kerte 2
güncellemesi keþfedilmiþtir. Yani

$$
H_{k+1} = H_k + c_1z_1z_1^T + c_2z_2z_2^T 
$$

Pozitif kesinliðin ispatý için [2, sf. 206].

Yine (1)'deki Newton-umsu metot þartýyla beraber

$$
s_k = H_k y_k + c_1 z_1 (z_1^Ty_k) + c_2z_2 (z_2^Ty_k)
$$

$z_1$ ve $z_2$ için özgün çözüm olmamasýna raðmen üstteki denklemi tatmin
edecek seçenekler bulunabilir,

$$
z_1 = s_k, \quad 
z_2 = H_k y_k, \quad
c_1 = \frac{1}{z_1^Ty_k}, \quad
c_2 = \frac{1}{z_2^Ty_k}
$$

Ve böylece kerte 2 güncellemesi þu hale gelir, 

$$
H_{k+1} = H_k + \frac{y_ky_k^T}{s_k^Ty_k} - \frac{(H_k y_k)(H_k y_k)^T}{(H_ky_k)^Ty_k}
$$

Bu formüle Davidon-Fletcher-Powell (DFP) formülü adý verilir. 

BFGS

DFP ile kerte 2 güncellemesi oluyor böylece $H_{k+1}$ pozitif kesin
kalýyor, güzel. Fakat DFP'nin hala sayýsal olarak bazý problemleri
var. Burada problem Hessian'ýn deðil Hessian'ýn tersinin yaklaþýklamasýnýn
güncelleniyor olmasý. Daha iyi bir seçim Hessian'ýn {\em kendisinin}
yaklaþýklamasýnýn güncellenmesi ve onun üzerinden bir terslik elde
edilmesi olmaz mýydý? Evet.

Devam etmeden önce iþimize yarayacak baþka bir konu, ikizlik konusundan
bahsedelim. Eðer DFP formülünün tersinin alýrsak belli bir sonuç elde
ederiz (bunun benzerini yapacaðýz). Ama biz bu noktaya (1)'deki

$$
H_{k+1} y_k = s_k
$$

ile geldiðimizi biliyoruz, ve üstteki formülde ufak bir takla atarsak

$$
y_k = B_{k+1} s_k
$$

ki $B_k$ $F_k$'nin yaklaþýk hali, sonucuna gelebileceðimizi de
biliyoruz. Dikkat edersek bu yeni Newton-umsuluk kuralý form olarak bir
öncekine çok benziyor, sadece $H_k$ yerine $B_k$ var ve $y_k,s_k$ yerleri
deðiþti!  Bundan istifade edebiliriz, ve þimdiye kadar yapýlan tüm türetme
iþlemlerini kullanarak ve sadece $y_k,s_k$ yerini deðiþtirerek $B_k$ için
bir güncelleme formülü elde edebiliriz.

$$
B_{k+1} = B_k + \frac{s_ks_k^T}{y_k^Ts_k} - \frac{(B_k s_k)(B_k s_k)^T}{(B_ks_k)^Ts_k}
$$

Ýþte $B_k$'nin BFGS güncellemesi budur, isim Broyden, Fletcher, Goldfarb,
and Shannon adlý araþtýrmacýlardan geliyor. Þimdi {\em üsttekinin tersini}
alýrsak arka planda yapýlan ve daha stabil olan $H_k$'nin güncellenmesinden
faydalanmýþ oluyoruz, ama hala her adýmda bizim ilgilendiðimiz matris
tersine eriþmiþ oluyoruz. Üsttekinin tersi için [6]'daki Sherman-Morrison
tekniðini kullanacaðýz. SM formülü neydi?

$$
(A+uv^T)^{-1} = A ^{-1} - \frac{(A^{-1} u)(v^TA^{-1})}{1 + v^T A^{-1} u}
$$

ki eðer $1+v^TA ^{-1} y \ne 0$ ise.

Simdi eger ana guncelleme formulunu 

$$
B_{k+1} = A_0 + u_0v_0^T + u_1v_1^T
$$

formuna getirebilirsek SM kullanabiliriz. Þu eþitlikleri kullanalým, 

$$
A_0 = B_k, \quad u_o = \frac{s_k}{s_k^Ty_k}, \quad v_0^T = s_k^T
$$
 
$$
A_1 = B_k + \frac{s_k s_k^T}{s_k^Ty_k} = A_0 + u_0v_0^T, \quad 
u_1 = -\frac{B_k y_k}{y_k^TB_ky_k}
$$

$$
v_1^T = y_k^T B_k
$$

Böylece 

$$
B_{k+1} = A_0 + u_0v_0^T + u_1v_1^T
$$

formülüne eriþmiþ olduk. Bu $B_{k+1}$ üzerinden bir ters elde etmek için,
ki bu sonuca $H_{k+1}^{BFGS}$ diyelim, 

$$
H_{k+1}^{BFGS} = B_{k+1}^{-1} 
$$

$$
= (A_1 + u_1v_1^T)^{-1} 
$$

SM açýlýmýna göre,

$$
= A_1^{-1} - \frac{A_1^{-1}u_1v_1^TA_1^{-1}}{1+v_1^TA_1^{-1}u_1 }
$$

Üstteki $A_1^{-1}$ için bir daha SM uygulamak lazým,

$$
H_{k+1}^{BFGS} = 
A_0^{-1} - \frac{A_0^{-1} u_0v_0^T A_0^{-1}}{1+v_0^TA_0^{-1}u_0 } -
\frac
  {
    (A_0^{-1} - \frac{A_0^{-1} u_0v_0^T A_0^{-1}}{1+v_0^TA_0^{-1}u_0})
     u_1v_1^T
    (A_0^{-1} - \frac{A_0^{-1} u_0v_0^T A_0^{-1}}{1+v_0^TA_0^{-1}u_0})
   }
  {1 + v_1^T (A_0^{-1} - \frac{A_0^{-1} u_0v_0^T A_0^{-1}}{1+v_0^TA_0^{-1}u_0})u_1}
$$

Dikkat edersek $A_0 = B_k$. O zaman $A_0^{-1} = B_k^{-1} = H_k$. Bu
eþitliði ve ilk baþta gösterdiðimiz notasyonu kullanarak, 

$$
H_{k+1}^{BFGS} = H_k - \frac{H_ks_ks_k^TH_k}{y_k^Ts_k + s_k^TH_ks_k} -
\frac
  {(H_k - \frac{H_ks_ks_k^TH_k}{y_k^Ts_k + s_k^TH_ks_k})
    (\frac{-B_k y_ky_k^TB_k}{y_k^TB_ky_k}) 
  }
  {1+y_k^TB_k (H_k - \frac{H_ks_ks_k^TH_k}{y_k^Ts_k + s_k^TH_ks_k}(\frac{-B_k y_ky_k^TB_k}{y_k^TB_ky_k})))}
$$
$$
\times (H_k - \frac{H_ks_ks_k^TH_k}{y_k^Ts_k + s_k^TH_ks_k})
$$

Bazý çarpýmlarý yaptýktan sonra ve $H_k = B_k^{-1}$ olduðunu hesaba
katarak, yani

$$
H_kB_k = B_kH_k=I_n
$$

diyerek, alttakini elde ediyoruz,

% \delta x -> y, g -> s

$$
H_{k+1}^{BFGS} = H_k - \frac{H_ks_ks_k^TH_k}{s_k^Ty_k + s_k^TH_ks_k} -
\frac
{ 
  (1 - \frac{H_k s_ks_k^T}{s_k^T y_k + s_k^TH_ks_k})
  (-y_ky_k^T)
  (1 - \frac{s_ks_k^TH_k}{s_k^Ty_k + s_k^TH_ks_k})
}
{y_k^T B_k y_k + y_k^T (B_k - \frac{s_k^Ts_k}{s_k^Ty_k + s_k^TH_ks_k})(-y_k)  }
$$

Sembolik islemlerimize devam ediyoruz. $y_k$ ve $y_k^T$ carparak alttakini
elde ediyoruz, 

$$
H_{k+1}^{BFGS} =  H_k - \frac{H_ks_ks_k^TH_k}{s_k^Ty_k + s_k^TH_ks_k} - 
\frac
{ 
  (\frac{H_k s_ks_k^Ty_k}{s_k^T y_k + s_k^TH_ks_k} - y_k)
  (x_k^T - \frac{y_k^T s_ks_k^TH_k}{s_k^Ty_k + s_k^TH_ks_k})
}
{
y_k^TB_ky_k - y_k^T B_k y_k + 
\frac{y_k^T s_ks_k^Ty_k}{s_k^Ty_k + s_k^TH_ks_k}
}
$$


























 

[devam edecek]

\begin{minted}[fontsize=\footnotesize]{python}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import numpy.linalg as lin

eps = np.sqrt(np.finfo(float).eps)

def rosen(x):
    return 100*(x[1]-x[0]**2)**2+(1-x[0])**2

def rosen_real(x):
    gy =[-400*(x[1]-x[0]**2)*x[0]-2*(1-x[0]), 200*(x[1]-x[0]**2)]
    return rosen(x), gy

def linesearch_secant(f, d, x):
    epsilon=10**(-5)
    max = 500
    alpha_curr=0
    alpha=10**-5
    y,grad=f(x)
    dphi_zero=np.dot(np.array(grad).T,d)

    dphi_curr=dphi_zero
    i=0;
    while np.abs(dphi_curr)>epsilon*np.abs(dphi_zero):
        alpha_old=alpha_curr
        alpha_curr=alpha
        dphi_old=dphi_curr
        y,grad=f(x+alpha_curr*d)
        dphi_curr=np.dot(np.array(grad).T,d)
        alpha=(dphi_curr*alpha_old-dphi_old*alpha_curr)/(dphi_curr-dphi_old);
        i += 1
        if (i >= max) and (np.abs(dphi_curr)>epsilon*np.abs(dphi_zero)):
            print('Line search terminating with number of iterations:')
            print(i)
            print(alpha)
            break
        
    return alpha

def bfgs(x, func):
    
    H = np.eye(2)
    tol = 1e-20
    y,grad = func(x)
    dist=2*tol
    epsilon = tol
    iter=0;

    while lin.norm(grad)>1e-6:
        value,grad=func(x)
        p=np.dot(-H,grad)
        lam = linesearch_secant(func,p,x)
        iter += 1
        xt = x
        x = x + lam*p
        s = lam*p
        dist=lin.norm(s)
        newvalue,newgrad=func(x)
        y = np.array(newgrad)-grad
        rho=1/np.dot(y.T,s)
        s = s.reshape(2,1)
        y = y.reshape(2,1)
        tmp1 = np.eye(2)-rho*np.dot(s,y.T)
        tmp2 = np.eye(2)-rho*np.dot(y,s.T)
        tmp3 = rho*np.dot(s,s.T)
        H= np.dot(np.dot(tmp1,H),tmp2) + tmp3
        #print ('lambda:',lam)

    print (xt)
    print ('iter',iter)
\end{minted}


\begin{minted}[fontsize=\footnotesize]{python}
x=np.array([-1.0,0])
bfgs(x,rosen_real)    
\end{minted}

\begin{verbatim}
[1. 1.]
iter 19
\end{verbatim}

\begin{minted}[fontsize=\footnotesize]{python}
def _approx_fprime_helper(xk, f, epsilon):
    f0 = f(xk)
    grad = np.zeros((len(xk),), float)
    ei = np.zeros((len(xk),), float)
    for k in range(len(xk)):
        ei[k] = 1.0
        d = epsilon * ei
        df = (f(xk + d) - f0) / d[k]
        if not np.isscalar(df):
            try:
                df = df.item()
            except (ValueError, AttributeError):
                raise ValueError("The user-provided "
                                 "objective function must "
                                 "return a scalar value.")
        grad[k] = df
        ei[k] = 0.0
    return grad


def rosen_approx(x):
    g = _approx_fprime_helper(x, rosen, eps)
    return rosen(x),g

bfgs(x,rosen_approx)
\end{minted}

\begin{verbatim}
[0.99999552 0.99999104]
iter 19
\end{verbatim}


Kaynaklar 

[1] Dutta, {\em Optimization in Chemical Engineering}

[2] Zak, {\em An Introduction to Optimization, 4th Edition}

[3] Bayramli, {\em Hesapsal Bilim, Sayýsal Entegrasyon ve Sonlu Farklýlýklar ile Sayýsal Türev}

[4] Chen, {\em ELE522 - Large Scale Optimization Lecture, Princeton},
    \url{http://www.princeton.edu/~yc5/ele522_optimization/}

[5] Bayramli, {\em Lineer Cebir, Ders 8, Kerte Konusu}

[6] Bayramli, {\em Lineer Ceir, Ekler, Sherley-Morrison Formülü}

\end{document}





