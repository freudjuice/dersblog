<h1>Log-Bariyer Yöntemi</h1>
<!DOCTYPE html>
<html>
  <head>
    <title>Log-Bariyer Yöntemi
</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {inlineMath: [["$","$"]]}
      });
    MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
      MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
        cancel: ["Extension","cancel"],
        bcancel: ["Extension","cancel"],
        xcancel: ["Extension","cancel"],
        cancelto: ["Extension","cancel"]
      });
    });
    </script>
<script type="text/javascript"
   src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML-full">
</script>
</head>

<p>[Newton yöntemi özeti atlandı]</p>
<p>Şimdiye kadar kısıtlanmamış Newton yönteminden bahsettik [1, 53:18]. Şimdi
lineer olarak kısıtlanmış, $Ax=b$ ile, olan duruma bakalım, çünkü ileride
faydalı olacak.</p>
<p>En bariz yaklaşım $Ax=b$'nin bir doğrusal (affine) uzay yaratması, ve
optimizasyonun bu daha ufak uzayda iş yapması. Bir değişken değişimi
yaparız, $x = Fy + x_0$, ki $F$, $A$'nin sıfır uzayını kapsıyor, ardından
optimizasyon problemini $y$ bazlı bir probleme indirge. Bu yaklaşıma
"indirgenmiş uzay yaklaşımı" deniyor. Tarif edilen gayet doğal, basit bir
yaklaşım aslında, fakat bir bedeli de var. Eğer orijinal problem seyrek ise
değişim sonrası çok miktarda yapı kaybı (yaı seyreklik) olacak.</p>
<p>Bir diğer seçenek eşitlikle kısıtlanmış Newton (equality constrained
Newton). Bu yöntemle $x^+ = x + t v$ adımında $v$ karesel
yaklaşıksallamanın çözümü, fakat bu çözüm sınırlanmış bir problemin çözümü,
bir sonraki $x^+$'yi olurlu halde tutacak bir sınırlama bu.</p>
<p>Çözülen problem </p>
<p>$$
v = \arg\min_{Az = 0} \nabla f(x)^T (z-x) + \frac{1}{2} (z-x)^T \nabla^2 f(x)(z-x)
$$</p>
<p>Üstteki $x^+$'i olurlu kümede tutar çünkü </p>
<p>$$
Ax^+ = Ax + tAv = b + 0 = b
$$</p>
<p>KKT koşulları üzerinden çözüm</p>
<p>$$
\left[\begin{array}{cc}
\nabla^2 f(x) &amp; A^T \\
A &amp; 0
\end{array}\right]
\left[\begin{array}{c}
v \\
w
\end{array}\right] = 
- 
\left[\begin{array}{c}
\nabla f(x) \\
Ax - b
\end{array}\right]
$$</p>
<p>ile belirtilebilir. KKT bölümünde karesel problem örneğindeki gördüğümüz
$Q$ burada $\nabla^2 f(x)$ oluyor, atılan adım sonrası gelinen yerin ne
olacağı da kısıtlama içinde görülebilir.</p>
<p>Olurlu noktadan basliyorsak, ozel durum $Ax = b$, tabii o zaman </p>
<p>$$
\left[\begin{array}{cc}
\nabla^2 f(x) &amp; A^T \\
A &amp; 0
\end{array}\right]
\left[\begin{array}{c}
v \\
w
\end{array}\right] = 
- 
\left[\begin{array}{c}
\nabla f(x) \\
0
\end{array}\right]
$$</p>
<p>O zaman bariyer metotu nedir? Bu metotla yine Newton metotunu uzatacağız,
ve eşitsizlik olan problemlerle uğraşacağız. </p>
<p>$$
\min_x f(x) \quad \textrm{oyle ki}
$$
$$
Ax = b
$$
$$
h_i(x) \le 0, \quad i=1,..,m
$$</p>
<p>Kriter dışbükey, sınırlamalar lineer, ve dışbükey eşitsizlik
sınırlamaları, $h_i(x) \le 0$ ile.</p>
<p>Bariyer metotu eşitsizliklerle başetmenin bir yolu. Eşitsizlik içeren
programların en büyük problemi sınırlarda ne yapılacağına karar vermek,
yani olurlu kümenin sınırlarında. Başetmek için olurlu küme 
$C \equiv { x: h_i(x) \le 0 , i=1,..,m}$ </p>
<p>$$
\min_x f(x) + I_C(x)
$$
$$
Ax = b
$$</p>
<p>ile kritere dahil edilir, ki $I_C$ göstergeç fonksiyonudur. Ana fikir sınır
noktalarında işler zorlaşıyorsa niye oralardan uzak durmuyoruz? Fakat
göstergeç fonksiyonu ile çalışmak zor, onu da yaklaşıksal olarak temsil
ederiz, işte bariyer fonksiyonu budur. Öyle bir fonksiyon seçeriz ki
sınırlarda aşırı büyük değerler vererek bizi minimizasyon bağlamında
oralardan "geri iter". Sanki sınırlara bir manyetik alan koyuyoruz,
oralara yaklaşınca geri itiliyoruz. </p>
<p>Tabii bir yandan Newton metotu da kullanabilmek istiyoruz, ideal olarak
pürüzsüz bir metot olursa elimizde iyi olur. Log bariyer fonksiyonu böyle
bir fonksiyon</p>
<p>$$
\phi(x) = -\sum_{i=1}^{m} \log(-h_i(x))
\qquad (1)
$$</p>
<p>Böylece ana problemi şu hale getirebiliriz,</p>
<p>$$
\min_x f(x) + \frac{1}{t} \phi(x) \quad \textrm{öyle ki}
$$
$$
Ax = b
$$</p>
<p>ki $t&gt;0$. Görülen $1/t$ dışarıdan bizim tanımladığımız bir parametre,
optimizasyonu ayarlamak için kullanıyoruz onu. Eğer $t$ küçükse, sıfıra
yakınsa o zaman bariyer baskın haldedir (daha büyüktür), tabii o zaman
sınırlardan kaçmak optimizasyon için daha önemli hale gelir. Eğer $t$'yi
büyütürsek bu "orijinal kriter $f$'ye daha fazla önem ver" demektir.</p>
<p>Dersin geri kalanında üstteki format yerine alttaki ile çalışacağız,
üstteki ile aynı, </p>
<p>$$
\min_x t f(x) + \phi(x) \quad \textrm{öyle ki}
$$
$$
Ax = b
$$</p>
<p>Log-Bariyer Calculus</p>
<p>(1) için türev</p>
<p>$$
\nabla \phi(x) = - \sum_{i=1}^{m} \frac{1}{h_i(x)} \nabla h_i(x)
$$</p>
<p>Hessian</p>
<p>$$
\nabla^2 \phi(x) = 
\sum _{i=1}^{m} \nabla h_i(x) \nabla h_i(x)^T - 
\sum _{i=1}^{m} \frac{1}{h_i(x)} \nabla^2 h_i(x)
$$</p>
<p>Örnek</p>
<p>Pek çok yerde kullanılan bir eşitsizlik görelim, mesela bütün $x_i &lt; 0$
olduğu bir durum, yani $h_i(x) = -x$. O zaman bariyer neye benzer? </p>
<p>$$
\phi(x) = - \sum _{i=1}^{n} \log x_i
$$</p>
<p>$$
\nabla \phi(x) = 
- \left[\begin{array}{c}
1/x_1  \\ 
\vdots \\
1/x_n
\end{array}\right] 
= - X^{-1} \textbf{1} 
$$</p>
<p>Burada $X$ matrisi </p>
<p>$$
X = \mathrm{diag}(x) = 
\left[\begin{array}{ccc}
x_1 &amp; &amp; \\
    &amp; \ddots &amp; \\
    &amp; &amp; x_n
\end{array}\right]
$$</p>
<p>ve $\textbf{1}$ sembolu tamamen 1'lerden oluşan matris.</p>
<p>[devam edecek]</p>
<p>Kaynaklar</p>
<p>[1] Tibshirani, <em>Convex Optimization, Lecture Video 14</em>, 
<a href="https://www.youtube.com/channel/UCIvaLZcfz3ikJ1cD-zMpIXg">https://www.youtube.com/channel/UCIvaLZcfz3ikJ1cD-zMpIXg</a>   </p>
<p>[2] Tibshirani, <em>Convex Optimization, Lecture Video 15</em>, 
<a href="https://www.youtube.com/channel/UCIvaLZcfz3ikJ1cD-zMpIXg">https://www.youtube.com/channel/UCIvaLZcfz3ikJ1cD-zMpIXg</a>   </p>