<h1>Log-Bariyer Yöntemi</h1>
<!DOCTYPE html>
<html>
  <head>
    <title>Log-Bariyer Yöntemi
</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {inlineMath: [["$","$"]]}
      });
    MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
      MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
        cancel: ["Extension","cancel"],
        bcancel: ["Extension","cancel"],
        xcancel: ["Extension","cancel"],
        cancelto: ["Extension","cancel"]
      });
    });
    </script>
<script type="text/javascript"
   src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML-full">
</script>
</head>

<p>Bir dışbükey probleme bakalım şimdi, artık tanıdık olan genel form bu,</p>
<p>$$
\min_x f(x) \quad \textrm{öyle ki}
$$
$$
h_i(x) \le 0, \quad i=1,..,m
$$
$$
Ax = b
$$</p>
<p>Tüm bu fonksiyonların dışbükey ve iki kere türevi alınabilir olduğunu farz
ediyoruz. Şimdi log bariyer metotu uygulayacağız, bu ilk göreceğimiz
iç-nokta yöntemi olacak [1, 14:00]. </p>
<p>Bu yöntem ile önce eşitsizlik kısıtlamalarına tekabül eden bir log bariyer
fonksiyonu tanımlamak gerekiyor. Bu fonksiyon, </p>
<p>$$
\phi(x) = -\sum_{i=1}^{m} \log(-h_i(x))
$$</p>
<p>Tabii $\log$'un negatif değerler üzerinde işletilemeyeceğini biliyoruz, o
sebeple üstteki eksi ile çarpım var (kısıtlamalara göre $h_i$'ler eksi
olmalı, onu da biliyoruz). Bu fonksiyon ile yapmaya uğraştığımız gösterge
(indicator) fonksiyonunu yaklaşıklamak. $\phi$'nin tanım kümesi $h$'ye göre
harfiyen olurlu olan $x$'ler. </p>
<p>Şimdi log bariyerin yaklaşıklamayı nasıl yaptığına gelelim. Eşitlik
kısıtlamalarını atlarsak, üstteki minimizasyon problemi şu şekilde de
gösterilebilir [1, 15:53],</p>
<p>$$
\min_x f(x) + \sum_{i=1}^{m} I_{h_i(x) \le 0}(x)
$$</p>
<p>$I$ her $h_i$'nin sıfırdan küçük olup olmadığına göre 0 ya da çok büyük
değerler verir, bu yüzden üstteki gibi bir temsil, eşitsizlik
kısıtlamalarını kullanmakla eşdeğerdir. Çünkü minimizasyon problemi doğal
olarak çok büyük değerlerden kaçacak, ve böylece kısıtlamalar dolaylı
yoldan problem çözümüne dahil olmuş olacak. Altta kesikli çizgiyle
göstergeç fonksiyonu görülüyor, </p>
<p><img alt="" src="func_59_barr_01.png" /></p>
<p>Diğer kavisli çizgiler ise $-\log(-u) \frac{1}{t}$, her $t$ için farklı bir
eğri. $t$ büyütüldükçe log bariyer fonksiyonunu göstergeci daha da iyi
yaklaşık temsil etmeye başlıyor / ona yaklaşıyor [1, 17:08]. </p>
<p>Altta farklı $\mu$ değerleri için
$-\mu \log(-u)$ fonksiyonun değerlerini görüyoruz. Fonksiyon görüldüğü gibi
$I$'ya oldukca yakın.</p>
<pre><code class="python">def I(u): 
   if u&lt;0: return 0.
   else: return 10.0

u = np.linspace(-3,1,100)
Is = np.array([I(x) for x in u])

import pandas as pd
df = pd.DataFrame(index=u)

df['I'] = Is
df['$\mu$=0.5'] = -0.5*np.log(-u)
df['$\mu$=1.0'] = -1.0*np.log(-u)
df['$\mu$=2.0'] = -2.0*np.log(-u)

df.plot()
plt.savefig('func_59_barr_02.png')
</code></pre>

<p><img alt="" src="func_59_barr_02.png" /></p>
<p>Herhalde simdi en yapacagimiz tahmin edilebilir, gostergec fonksiyonlariyla
ile calismak zor, o zaman göstergeç toplamları log toplamları olarak
yaklaşıksallanabilir,</p>
<p>$$
\min_x f(x) + \frac{1}{t} \sum_{i=1}^{m} \log(-h_i(x))
$$</p>
<p>ki $t$ büyük olacak şekilde çünkü o zaman log, göstergeci iyi yaklaşık
olarak temsil ediyor, ardından bu yeni pürüzsüz problemi çözüyoruz,
eşitsizlik şartlarına ihtiyaç duymadan. </p>
<p>Log-Bariyer Calculus</p>
<p>$\phi$ fonksiyonunun bazı özelliklerini dökmek faydalı olur, ileride Newton
metotundan bahsettiğimizde bu özellikler faydalı olacak. $\phi$ için gradyan ve
Hessian,</p>
<p>$$
\nabla \phi(x) = - \sum_{i=1}^{m} \frac{1}{h_i(x)} \nabla h_i(x)
$$</p>
<p>Hessian</p>
<p>$$
\nabla^2 \phi(x) = 
\sum _{i=1}^{m} \nabla h_i(x) \nabla h_i(x)^T - 
\sum _{i=1}^{m} \frac{1}{h_i(x)} \nabla^2 h_i(x)
$$</p>
<p>Merkezi gidiş yolu (central path)</p>
<p>Optimizasyon problemimizi $1/t$ yerine $t$ carpimi ile de gosterebiliriz,
yani</p>
<p>$$
\min_x t f(x) + \phi(x) \quad \textrm{öyle ki}
$$
$$
Ax = b
$$</p>
<p>Herneyse, merkezi yol $x^*(t)$, $t&gt;0$'nin bir fonksiyonudur, yani her $t$
için eldeki çözümlerin ortaya çıkarttığı yoldur bir bakıma. Her $t$ için
problemin çözümünü KKT koşulları ile karakterize edebiliriz. </p>
<p>$$
Ax^*(t) = b, \quad h_i(x^*(t)) &lt; 0, \quad i=1,..,m
$$</p>
<p>$$
t \nabla f(x^*(t)) - \sum \frac{1}{h_i(x^*(t))} \nabla h_i(x^*(t)) + A^T w= 0
$$</p>
<p>Bu koşullar $x^*(t)$'nin optimal olmasının ne demek olduğunu
tanımlıyor. İki denklemdeki ilk denklem ana olurluktan geliyor, eşitlik
sınırlamalarına tekabül eden tek ikiz değişken var, $w$, onun işareti
üzerinde kısıtlama yok çünkü eşitlik kısıtlaması. Durağanlık koşulu ikinci
denklemde, ona nasıl eriştik? Problemin Lagrangian'i</p>
<p>$$
t f(x) + \phi(x) + w^T (Ax - b)
$$</p>
<p>Eğer $x$'e göre gradyan alıp sıfıra eşitlersek durağanlığı elde
ederim. Gradyan yeterli çünkü buradaki tüm fonksiyonlar dışbükey ve
pürüzsüz [1, 24:04]. </p>
<p>Eğer üstteki problemi bir $w$ için çözersem o zaman merkezi yoldaki bir
çözümü belli bir $t$ için karakterize etmiş / tarif etmiş
oluyorum. Umudumuz o ki $t$'yi sonsuzluğa doğru büyüttükçe üstteki KKT
koşullarıyla temsil edilen çözümler orijinal problemimdeki çözüme
yaklaşmaya başlayacak. Bu olabilir değil mi? $t$'yi büyüttükçe log
bariyerin nasıl göstergeç fonksiyonuna benzemeye başladığını biraz önce
gördük. Bu tür log bariyerlerden oluşan optimizasyon problemi için de
benzer bir durum olacağını tahmin edebiliriz. </p>
<p>Bu kavramları lineer programlar için yakından görebiliriz. Tüm bu
yaklaşımlar bu arada ilk başta LP'ler için ortaya atılmıştır. </p>
<p>Önemli bir örnek,</p>
<p>$$
\min_x t c^T x - \sum _{i=1}^{m} \log(e_i - d_i^T x)
$$</p>
<p>Bu bir standart LP'nin bariyerleştirilmiş hali. Eşitlik kısıtlaması yok, ve
bariyer fonksiyonu çokyüzlü kısıtlama  $D x \le e$ ifadesine tekabül
ediyor. Bu problemi belli bir $t$ için çözersem, $t$'yi büyütürsem, bunu
ardı ardına tekrar edersem umudum orijinal LP'nin çözümüne yaklaşmak. </p>
<p><img alt="" src="func_59_barr_03.png" /></p>
<p>Resimde görüldüğü gibi, ortadan başlıyoruz, $t=0$'da diyelim, ve $t$'yi
büyüttükçe yolda ilerliyoruz, ve sonuca erişiyoruz. Gidiş pürüzsüz, ve
LP'lerin karakterinden biliyoruz ki nihai sonuç çokyüzlümün (polyhedra)
ekstrem noktalarının birinde olmalı. Yarı yolda $t=10$'daki bir nokta
gösteriliyor, nihai sonuç belki $t=100$'da [2, 26:59]</p>
<p>KKT koşulu üzerinden durağanlığı temiz bir şekilde gösterebiliyoruz, ya da
iç nokta ve ortada, merkezde bir yol takip edilmesini zorlama bağlamında,
merkezlik şartı da deniyor buna, gradyan alınınca</p>
<p>$$
0 = tc - \sum _{i=1}^{m} \frac{1}{e_i - d_t ^T x^(t)} d_i
$$</p>
<p>Bu demektir ki gradyan $\nabla (x^*(t))$, $-c$'ye paralel olmalıdır, ya da
${ x: c^T x = c^T x^*(t) }$ hiper düzlemi $\phi$'nin $x^*(t)$'deki
konturuna teğet durmalıdır [1, 28:12].</p>
<p>Ikiz noktalar</p>
<p>Birazdan merkezi yoldan ikiz noktalar alabileceğimizi göreceğiz. Bu çok
faydalı olacak çünkü bu ikiz noktaları bir ikiz boşluğu hesaplamak için
kullanacağız. Merkezi yoldayken bu yoldaki noktalar $x*(t)$'leri
kullanarak olurlu ikiz noktalar hesaplayabiliriz. Orijinal probleme
tekrar bakarsak, bu problem için ikiz değişkenleri elde etmek için her
eşitsizlik için bir $u_i$'ye, her eşitlik şartı için bir $v_i$'ya ihtiyacım
var. Onları nasıl tanımlarım? Merkezi yol üzerindeki çözümler üzerinden,</p>
<p>$$
u_i^*(t) = \frac{1}{t h_i(x^*(t))}, \quad i=1,..,m, \quad v^*(t) = w/t
$$</p>
<p>$w$ bariyer problemi için KKT koşullarını çözerken elde ettiğim değişken
idi. </p>
<p>Niye üsttekiler orijinal problem için olurlu? Bunu görmek kolay, ilk önce,
$u_i^*(t)$'nin her ögesi harfiyen pozitif, çünkü $h_i(x^*(t))$'nin her
ögesi harfiyen negatif. Bu bariyer probleminin ana olurluk şartından
geliyor. Ayrıca $(u^*(t),v^*(t))$ Lagrange ikiz fonksiyonu $g(u,v)$'nin
tanım kümesinde (domain). Hatırlarsak Lagrange ikizi formülize ettiğimizde
tanım kümesinde bazı dolaylı sınırlamalar elde ediyorduk. Tarif itibariyle</p>
<p>$$
\nabla f(x^*(t)) + \sum _{i=1}^{m} u_i (x^*(t)) \nabla h_i(x^*(t)) + 
A^T v^*(t) = 0
$$</p>
<p>Yani $x^*(t)$, Lagrangian $L(x,u^*(t),v^*(t))$'i tüm $x$'ler üzerinden
minimize edeceği için $g(u^*(t),v^*(t)) &gt; -\infty$. Bu direk durağanlık
şartından geliyor işte. O kadar bariz birşey ki aslında bazen kafa
karıştırıyor. Merkezi yol probleminden çözdüğümüz durağanlık koşulu
şöyleydi,</p>
<p>$$
t \nabla f(x^*(t)) - \sum \frac{1}{h_i(x^*(t))} \nabla h_i(x^*(t)) + A^T w= 0
$$</p>
<p>Bir $x^*$ çözümü ve $w$ olduğunu farz ediyoruz. Tüm formülü $t$ ile
bölersem,</p>
<p>$$
\nabla f(x^*(t)) - \sum \frac{1}{t h_i(x^*(t))} \nabla h_i(x^*(t)) + A^T \frac{w}{t}= 0
$$</p>
<p>Tek yaptığımız "üstteki orijinal problemdeki durağanlık şartına çok
benziyor" demek, değil mi, çünkü </p>
<p>$$
\nabla f(x^*(t)) +
\sum \underbrace{\frac{-1}{t h_i(x^*(t))}}_{u_i} \nabla h_i(x^*(t)) + 
A^T \underbrace{\frac{w}{t}}_{v}= 0 
\qquad (4)
$$</p>
<p>desem, orijinal problemin durağanlık şartına benzeyen bir ifade elde etmiş
olurum [2, 33:48]. Demiştik ki üstteki $u_i,v$ tanımları üzerinden orijinal
problem için olurlu ikiz noktaları alabiliyoruz. </p>
<p>Soru: niye orijinal problem için optimal noktaları elde etmedim? Biraz önce
gördük, durağanlık koşulunu tatmin ettim, ana, ikiz olurluk
var.. ama.. tamamlayıcı gevşeklik tatmin edilmedi. Çok önemli. Onun yerine
ne var? $u_i h_i (x) = 0$ olması lazım, onun yerine ne var? Tanım
itibariyle $u_i = -1/t ..$ var. O zaman sıfıra yakınsak çok yakınsak bölüm
büyür, dolaylı olarak $t$'yi büyüttükçe orijial problemin KKT koşullarını
yaklaşıksallamış oluruz. O zaman log bariyer problemini çözmüş olmamıza
rağmen belli bir $t$ değer için orijinal problem için çözüm olmamasının
sebebi tamamlayıcı gevşekliğin tatmin edilmiyor olması.</p>
<p>Ama elimizdekiler hala çok faydalı, çünkü herhangi bir anda merkez yol
üzerinde $t$'nin fonksiyonu olarak ne kadar alt optimal olduğumuzu
sınırlamak mümkün oluyor. Bunun için sadece ikiz boşluğunu hesaplıyoruz, o
kadar. O zaman $u^*,v^*$'da Lagrange ikiz fonksiyonu hesaplıyorum, ve $f^*$
ile bu Lagrange farkını buluyorum, ve ikiz boşluğu hesaplanmış oluyor. 
(4)'te gördük ki $x^*$ Lagrangian'i $u^*,v^*$'da minimize eder, o zaman
ikizi alttaki gibi hesaplayarak </p>
<p>$$
g(u^*(t),v^*(t)) = 
f(x^*(t)) + \sum_{i=1}^{m} u_i^*(t) h_i(x^*(t)) + v^*(t)^T (Ax^*(t) - b)
$$</p>
<p>Büyük toplamdaki ikinci terim sıfır, çünkü merkezi yolda $Ax^*$ her zaman
$b$'ye eşittir. Birince terimde, $u_i$'i $-1/t$'ye eşitledik, ve bu $m$
kere toplanacak, sonuç</p>
<p>$$
= f(x^*(t)) - m/t
$$</p>
<p>Yani göstermiş olduk ki merkezi yolun optimallikten olan uzaklığı en fazla
$m/t$ olacaktır,</p>
<p>$$
f(x^*(t)) - f^* \le m/t
$$</p>
<p>Üstteki bariyer metorun işlediğine dair ispata en yakın sonuç, bize diyor
ki eğer herhangi bir $t$ için bariyer problemini çözersem optimalliğe
yakınlık her zaman $m/t$'den küçük olur. $t$'yi isteğe bağlı olarak
büyüttükçe o ölçüde optimalliğe yaklaşmış olurum. </p>
<p>Merkezi yolu yorumlamanın bir diğer yolu "sarsıma uğratılmış KKT
koşulları" denen bir teknik üzerinden. Şimdiye kadar gördük ki merkez yol
ve ona tekabül eden ikiz değerler (4)'teki durağanlık şartını çözüyor. 
$u_i \ge 0$, $h_i(x) \le 0$, ve $Ax = b$. Tamamlayıcı gevşeklik haricinde tüm
koşullar tatmin. Esas KKT koşullarında </p>
<p>$$
u_i^*(t) \cdot h_i^*(t) = 0, \quad i=1,..,m
$$</p>
<p>olurdu, biz onun yerine </p>
<p>$$
u_i^*(t) \cdot h_i^*(t) = -1/t, \quad i=1,..,m
$$</p>
<p>dedik. Yani bir anlamda log bariyer fonksiyonunu unutuyoruz, onun yerine şu
probleme bakıyoruz,</p>
<p>$$
\nabla f(x^*(t)) + \sum_{i=1}^{m} u_i(x^*(t))\nabla h_i(x^*(t)) + A^T v^*(t) = 0
$$</p>
<p>$$
u_i^*(t) \cdot h_i^*(t) = -1/t, \quad i=1,..,m
$$</p>
<p>$$
h_i(x^*(t)) \le 0, \quad i=1,..,m, \quad Ax^*(t) = b
$$</p>
<p>$$
u_i(x^*(t)) \ge 0
$$</p>
<p>Ve $t$'yi büyüterek üstteki problemi çözüyorum. Yani olurluğu, durağanlığı
tam olarak, tamamlayıcı gevşekliği ise yaklaşık olarak çözmüş oluyorum, ve
yaklaşıksallığı gittikçe büyüyen $t$'ler üzerinden daha sıkı hale getiriyorum. 
Yani log bariyer tekniği ile sarsıma uğratılmış KKT koşulları sınırlı
problemleri çözmenin iki yolu. </p>
<p>Niye ufak $t$ ile başlayıp büyütüyorum [2, 02:19]? Çünkü pratikte bu iyi
işliyor. Niye işlediğini görmek zor değil, $t$ küçükken tüm fonksiyon
oldukca pürüzsüz, ve onun üzerinde Newton adımları rahat işler. Ama $t$'yi
büyüttükçe onun kontrol ettiği fonksiyon kısımlarını gittikçe daha az
pürüzsüz yapmaya başlıyorum, ama bu çok kötü değil çünkü bu noktada çözüm
bölgesine kabaca yaklaşmış olmalıyım. </p>
<p>Yakınsama analizi (convergence analysis) </p>
<p>Teori </p>
<p>Diyelim ki merkezleştirme adımlarını kesin olarak çözebiliyoruz, yani,
diyelim ki ne zaman Newton metotunu uygularsam mükemmel bir sonuç
alıyorum. Tabii ki bu gerçekte olmuyor ama farz edelim. O zaman, sadece
$t$'yi her adımda $\mu$ ile çarpmamızın doğal sonucu olarak ve ikizlik
boşluğununun $m / t$ olması sebebiyle $k$ adım sonrası alttakini görürdük, </p>
<p>$$
f(x^(k)) - f^* \le \frac{m}{\mu^k t^{0}}
$$</p>
<p>Bu ifade diyor ki istenen $\epsilon$ seviyesinde bir doğruluğa erişmek için </p>
<p>$$
\frac{\log ( m / (t^{(0)} \epsilon) )}{\log \mu}
$$</p>
<p>tane merkezleştirme adımına ihtiyacımız var. </p>
<p>Olurluk metotu (feasibility method)</p>
<p>Bariyer metotunun bir noktadan başlaması gerekir ve bu nokta olurlu
olmalıdır. Olurlu derken </p>
<p>$$
h_i(x) &lt; 0, \quad i=1,..,m, \quad Ax = b
$$</p>
<p>şartlarına uyan bir noktadan bahsediyorum. Fakat ya öyle bir nokta elimizde
yoksa? Başta olurlu olan bir noktayı bulmanın kendisi de zor bir
problem. Böyle bir noktayı elde etmek için olurluk metotu denen bir yöntem
kullanmak gerekecek. Boyd'un kitabı [4, Bölüm 11] bu metota "1. Faz (Phase
İ)" ismi veriyor, problemin kendisini çözmeye "2. Faz" diyor. Pratikte
bariyer metotunu kullanmak isteyenler bunu hatırlamalı. </p>
<p>Harfiyen olurlu bir noktayı nasıl buluruz? Kulağa biraz dolambaçlı gibi
gelebilir ama bu noktayı bulmak için ayrı, farklı bir optimizasyon problemi
daha kurarız, onu da bariyer metotu ile çözeriz. Tabii illa bariyer metotu
olması gerekmez, ana-çift iç-nokta yöntemi de olabilir, ama her halükarda
alttaki problemi çözeriz. </p>
<p>Bu problemde elimizde iki tane değişken grubu var, $x,s$. Problem [2, 23:02], </p>
<p>$$
\min_{x,s} s \quad \textrm{öyle ki}
$$
$$
h_i(x) \le s, i=1,..,m
$$
$$
Ax = b
$$</p>
<p>Amaç harfiyen negatif bir $s$ elde etmek, böylece $h_i(x) \le s$ üzerinden
ana problemin eşitsizlik şartları tatmin olacak, ayrıca $Ax = b$'e uygun
bir başlangıç noktası elde edilmiş olacak ki bu da ana problem için
gerekli. </p>
<p>Bu problemi bariyer metotu ile çözmek oldukca kolay, ana problemin kendisi
kadar zor değil. Niye? İki sebep: ilki, üstteki problemi çözmek için de
harfiyen olurlu bir başlangı noktası lazım, ama bu noktayı bulmak aslında
çok kolay. Bana tek gereken eşitlik kısıtlaması $Ax = b$'yi tatmin eden bir
$x$ bulmak, ama bu lineer bir sistem çözümü, her lineer cebir paketi bunu
çözer. Ardından elde edilen $x$ ile $h_i(x)$'i hesaplamak, ve bunların en
büyüğünü artı mesela 0.01 diyerek kullanmak [2, 24:00]. Elde edeceğimiz
sonuç üstteki problem için harfiyen olurludur, eşitsizlik kısıtlamalarına
harfiyen uygun. Şimdi elimde bir başlangıç $x$'i ve $s$'i var, ve buradan
başlayarak bariyer metotunun adımlarını uygulayabilirim. İşin güzel tarafı
durma şartımız çok basit, $s$'in her ögesinin negatif olduğunu gördüğüm
anda şak diye durabilirim, yani üstteki programın "optimal" olmasıyla
ilgilenmiyorum sonuçta bana tek gereken ana problemim için olurlu bir
başlangıç noktası. Çoğunlukla yapılan tarif edilen şekilde $x,s$ bulmak ve
bunu ardı ardına yapmak ta ki tamamen negatif elde edilene kadar ve o
noktada durulur, ana probleme dönülür.</p>
<p>Alternatif olarak şu problem de çözülebilir,</p>
<p>$$
\min_{x,s} 1^T s, \quad \textrm{öyle ki}
$$
$$
h_i(x) \le s_i, i=1,..,m
$$
$$
Ax = b, s \ge 0
$$</p>
<p>Bu metotun avantajı eğer sistem olurlu değilse hangi kısıtlamanın harfiyen
yerine getirilemediğini bize söyler. Dezavantaj, üstteki çözmesinin biraz
daha zor olabilmesi.</p>
<p>Ekler</p>
<p>Pek çok yerde kullanılan bir eşitsizlik görelim, mesela bütün $x_i &lt; 0$
olduğu bir durum, yani $h_i(x) = -x$. O zaman bariyer neye benzer? </p>
<p>$$
\phi(x) = - \sum _{i=1}^{n} \log x_i
$$</p>
<p>$$
\nabla \phi(x) = 
- \left[\begin{array}{c}
1/x_1  \\ 
\vdots \\
1/x_n
\end{array}\right] 
= - X^{-1} \textbf{1} 
$$</p>
<p>Burada $X$ matrisi </p>
<p>$$
X = \mathrm{diag}(x) = 
\left[\begin{array}{ccc}
x_1 &amp; &amp; \\
    &amp; \ddots &amp; \\
    &amp; &amp; x_n
\end{array}\right]
$$</p>
<p>ve $\textbf{1}$ sembolu tamamen 1'lerden oluşan matris. </p>
<p>Hessian</p>
<p>$$
\nabla^2 \phi (x) = \left[\begin{array}{ccc}
1/x_1^2 &amp; &amp; \\
    &amp; \ddots &amp; \\
    &amp; &amp; 1/x_n^2
\end{array}\right] = X^2 
$$</p>
<p>Kod</p>
<p>Standard LP çözen bariyer metot temelli kod alttadır [3]. </p>
<p>Not: Bu metot başlangıç noktası bulmak için</p>
<p>$$
\min t, \quad \textrm{öyle ki}
$$
$$
Ax = b, \quad x \ge (1-t) 1, \quad t \ge 0
$$</p>
<p>problemini çözüyor. </p>
<p>Örnek olarak</p>
<p>$$
\min_x -x_1 - 5x_2 \quad \textrm{öyle ki}
$$
$$
x_1 + x_2 + x_3  = 5 
$$
$$
x_1 + 3 x_2 + x_4 = 7
$$
$$
x_1,x_2,x_3,x_4 \ge 0
$$</p>
<p>LP'si üzerinde bu programı işlettik.</p>
<pre><code class="python"># https://github.com/junjiedong/LP-Solver
import numpy as np

class LPCenteringSolver():
    &quot;&quot;&quot;
    A helper module for LPSolver

    Solves LP centering problem using infeasible start Newton method:
    minimize    c'x - sum( log(x_i) )
    subject to  Ax = b

    Hyperparameters
        alpha: the factor by which we weaken the slope of first-order approximation
               in backtracking line search (0 &lt; alpha &lt; 0.5)
        beta: multiplicative step size in backtracking line search (0 &lt; beta &lt; 1)
        max_iter: maximum number of Newton steps before quitting

    Attributes
        status: 'optimal' or None
        value: optimal value of objective function
        x_opt, v_opt: primal optimal point and a dual optimal point
        num_steps: Number of Newton steps executed
        residual_norm: array of ||r(x,v)||_2 (length num_steps+1)
    &quot;&quot;&quot;

    def __init__(self, alpha=0.25, beta=0.5, max_iter=100):
        self.alpha = alpha
        self.beta = beta
        self.max_iter = max_iter
        self.convergence_threshold = 1e-6  # stop when ||r||_2 &lt;= convergence_threshold
        self.status = None


    def solve(self, A, b, c, x0=None):
        m, n = A.shape
        self.status = None
        self.residual_norm = []

        # Initialize primal and dual variables (x, v)
        x = x0 if x0 is not None else np.random.rand(n) + 0.01
        v = np.zeros(m)

        for self.num_steps in range(self.max_iter):
            # Compute primal and dual residuals
            r_dual = c - 1 / x + np.dot(A.T, v)
            r_primal = A.dot(x) - b
            r_norm = np.sqrt(np.sum(r_dual ** 2) + np.sum(r_primal ** 2))
            self.residual_norm.append(r_norm)

            # Convergence check
            if r_norm &lt;= self.convergence_threshold:
                self.status = 'optimal'
                self.x_opt, self.v_opt = x, v
                self.value = np.sum(c * x) - np.sum(np.log(x))
                break

            # Compute primal and dual Newton steps (dx, dv) via block elimination
            h_inv =  x ** 2  # diagonal entries of inv(Hessian)
            dv = np.linalg.solve( np.dot(A * h_inv, A.T), r_primal - \
                                  np.dot(A * h_inv, r_dual) )
            dx = -h_inv * (r_dual + np.dot(A.T, dv))

            # Backtracking line search on ||r||_2
            t = 1
            while np.min(x + t * dx) &lt;= 0:  # prevent x from going below 0
                t = self.beta * t
            while True:
                x_new = x + t * dx
                v_new = v + t * dv
                r_dual_new = c - 1 / x_new + np.dot(A.T, v_new)
                r_primal_new = A.dot(x_new) - b
                r_norm_new = np.sqrt(np.sum(r_dual_new ** 2) + \
                                     np.sum(r_primal_new ** 2))

                if r_norm_new &lt;= (1 - self.alpha * t) * r_norm:
                    x = x_new
                    v = v_new
                    break
                else:
                    t = self.beta * t

        self.residual_norm = np.array(self.residual_norm)
        if self.status != 'optimal':
            self.num_steps = self.max_iter
            self.value, self.x_opt, self.v_opt = None, None, None


class LPSolver():
    &quot;&quot;&quot;
    LPSolver solves the standard form LP problem:
    minimize    c'x
    subject to  Ax = b, x &gt;= 0

    The solver uses the barrier method.
    It traverses the central path by solving a centering problem at each step:
    minimize    t * c'x - sum( log(x_i) )
    subject to  Ax = b

    The solving procedure gradually increases t after each step, and stops when
    the duality gap n / t &lt; tol, where n is the length of x

    Hyperparameters
        mu: multiplicative step size for barrier method (t := mu * t)
        tol: suboptimality tolerance (used for convergence check)

    Attributes:
        status: 'optimal', 'infeasible', or 'failure'
        value: optimal value of objective function
        x_opt, v_opt: primal optimal point and a dual optimal point
        num_steps: number of centering steps executed
        duality_gaps: array - duality gap after every centering step
        newton_steps: array - number of Newton steps for solving centering problems
    &quot;&quot;&quot;

    def __init__(self, mu=10, tol=1e-3) :
        self.mu = mu  # multiplicative step size for barrier method
        self.tol = tol  # maximum acceptable duality gap
        self.max_iter = 100
        self.centering_solver = LPCenteringSolver(alpha=0.25, beta=0.5, max_iter=100)


    def solve_feasible_start(self, A, b, c, x):
        &quot;&quot;&quot;
        Solves an LP problem given a strictly feasible starting point x.
        This is used as a helper method for solving an LP problem from scratch,
        no matter whether the problem is strictly feasible or not
        &quot;&quot;&quot;
        m, n = A.shape
        status, x_opt, v_opt, value = None, None, None, None
        duality_gaps = []  # duality gap n/t after every iteration
        newton_steps = []  # number of Newton steps for solving centering problems

        t = 1
        for num_steps in range(1, self.max_iter+1):
            # Solve LP Centering problem
            self.centering_solver.solve(A, b, t*c, x)
            if self.centering_solver.status != 'optimal':
                status = 'failure'  # Centering solver failed
                break

            x, v = self.centering_solver.x_opt, self.centering_solver.v_opt
            duality_gap = n / t
            newton_steps.append(self.centering_solver.num_steps)
            duality_gaps.append(duality_gap)

            # Convergence check
            if duality_gap &lt; self.tol:
                status = 'optimal'
                x_opt, v_opt = x, v
                value = np.sum(c * x)
                break
            else:
                t = self.mu * t  # Increase t

        return status, x_opt, v_opt, value, num_steps, \
               np.array(duality_gaps), np.array(newton_steps)


    def solve(self, A, b, c):
        &quot;&quot;&quot;
        Checks feasibility of an LP problem, and solves the problem if it's feasible
        The solving procedure consists of two phases:
        -- Phase I: Check strict feasibility and obtain a strictly feasible starting
                    point by using solve_feasible_start() to solve the following LP,
                    for which we can always find a feasible starting point (x, t):
                    minimize    t
                    subject to  Ax = b, x &gt;= (1 - t) * 1_n

        -- Phase II: If strictly feasible, directly invoke solve_feasible_start()
                     with the starting point x found in Phase I
        &quot;&quot;&quot;
        m, n = A.shape

        # Phase I: Check strict feasibility and obtain a strictly
        # feasible starting point
        x = np.linalg.pinv(A).dot(b)
        if np.min(x) &lt;= 0:  # otherwise, x is already a strictly feasible point
            # Construct an LP for checking feasibility
            A1 = np.hstack(( A, -A.dot(np.ones((n, 1))) ))
            b1 = b - A.dot(np.ones(n))
            c1 = np.zeros(n+1)
            c1[-1] = 1

            t = 2 - np.min(x)  # makes sure the starting (x, t) pair is feasible
            z0 = np.concatenate(( x + (t-1)*np.ones(n), [t] ))
            status, z_opt, _, value, _, _, _ = self.solve_feasible_start(A1, b1, c1, z0)
            if status == 'optimal' and value &lt; 1:  # strictly feasible
                x = z_opt[:n] - (z_opt[-1] - 1) * np.ones(n)
            else:
                self.status = &quot;infeasible&quot;
                self.x_opt, self.v_opt, self.value = None, None, None
                self.duality_gaps, self.newton_steps = None, None
                return

        # Phase II: Given a strictly feasible starting point x, solve the problem
        self.status, self.x_opt, self.v_opt, self.value, self.num_steps, \
        self.duality_gaps, self.newton_steps = self.solve_feasible_start(A, b, c, x)
</code></pre>

<pre><code class="python">import numpy as np
from scipy.optimize import linprog
import barr

A = np.array([[1,  1, 1, 0],
              [1,  3, 0, 1]])

b = np.array([5,7])

c = np.array([-1, -5, 0, 0 ])

solver = barr.LPSolver(mu=10, tol=1e-4)

solver.solve(A, b, c)

print ('log bariyer ==========')
print (solver.x_opt)

res = linprog(c, A_eq=A, b_eq=b, options={&quot;disp&quot;: True})
print ('linprog ===============')
print (res)
</code></pre>

<pre><code>log bariyer ==========
[1.49999116e-05 2.33332633e+00 2.66665867e+00 5.99999936e-06]
Primal Feasibility  Dual Feasibility    Duality Gap         Step             Path Parameter      Objective          
1.0                 1.0                 1.0                 -                1.0                 -6.0                
0.1105388427842     0.1105388427842     0.1105388427842     0.8919387648961  0.1105388427842     -10.34625028215     
0.001400532337055   0.00140053233704    0.00140053233704    0.9918943193656  0.00140053233704    -11.65623916548     
7.115191880125e-08  7.11519194345e-08   7.115191920093e-08  0.9999491966235  7.115192025851e-08  -11.66666613752     
3.556266503391e-12  3.557079864332e-12  3.557332206583e-12  0.9999500067836  3.557595982315e-12  -11.66666666664     
Optimization terminated successfully.
         Current function value: -11.666667  
         Iterations: 4
linprog ===============
     con: array([1.18571819e-11, 1.18527410e-11])
     fun: -11.66666666664022
 message: 'Optimization terminated successfully.'
     nit: 4
   slack: array([], dtype=float64)
  status: 0
 success: True
       x: array([1.15454732e-13, 2.33333333e+00, 2.66666667e+00, 3.96953400e-12])
</code></pre>

<p>Kaynaklar</p>
<p>[1] Tibshirani, 
    <em>Convex Optimization, Lecture Video 15, Part 1</em>, 
    <a href="https://www.youtube.com/channel/UCIvaLZcfz3ikJ1cD-zMpIXg">https://www.youtube.com/channel/UCIvaLZcfz3ikJ1cD-zMpIXg</a>   </p>
<p>[2] Tibshirani, 
    <em>Convex Optimization, Lecture Video 15, Part 2</em>, 
    <a href="https://www.youtube.com/channel/UCIvaLZcfz3ikJ1cD-zMpIXg">https://www.youtube.com/channel/UCIvaLZcfz3ikJ1cD-zMpIXg</a>   </p>
<p>[3] Dong, <em>LP-Solver, Github</em>, 
    <a href="https://github.com/junjiedong/LP-Solver">https://github.com/junjiedong/LP-Solver</a></p>
<p>[4] Boyd, <em>Convex Optimization</em></p>