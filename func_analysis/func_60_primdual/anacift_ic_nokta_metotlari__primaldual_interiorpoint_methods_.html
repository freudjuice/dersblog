<h1>Ana-Çift İç Nokta Metotları (Primal-Dual Interior-Point Methods)</h1>
<!DOCTYPE html>
<html>
  <head>
    <title>Ana-Çift İç Nokta Metotları (Primal-Dual Interior-Point Methods)
</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {inlineMath: [["$","$"]]}
      });
    MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
      MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
        cancel: ["Extension","cancel"],
        bcancel: ["Extension","cancel"],
        xcancel: ["Extension","cancel"],
        cancelto: ["Extension","cancel"]
      });
    });
    </script>
<script type="text/javascript"
   src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML-full">
</script>
</head>

<p>Şimdiye kadar gördüğümüz problem tiplerini hatırlayalım şimdi ve çözme
zorluğu açısından sıralayalım. En üstte, en basit olan karesel
problemlerdi, </p>
<p>$$
\min_x \frac{1}{2}  x^T Q x + c^T x
$$</p>
<p>formunda oluyorlardı. Bu problemler en basiti, eğer $Q$ pozitif yarı-kesin
ise. Basit çünkü gradyani alıp sıfıra eşitliyorum, pat diye sonucu
buluyorum. </p>
<p>Sonraki seviye, biraz daha zor, üstteki probleme $Ax=b$ formunda eşitlik
kısıtlaması getirmek. Bu problemi de kapalı / analitik halde (closed-form)
çözebiliriz, KKT koşullarını kullanarak. $Ax=b$ ifadesini ek değişkenler
üzerinden kritere ekleriz, Lagrangian'ı oluştururuz, KKT koşulunda iki tane
öğe olur, durağanlık, ve ana olurluk, bu iki öğeyi eşzamanlı olarak çözerek
sonuca ulaşırız, koca bir lineer sistemdir bu. </p>
<p>Bir sonraki pürüzsüz minimizasyon, yani üstteki kriterin yerine $f(x)$
kullanmak ki $f(x)$ bir pürüzsüz fonksiyon. Bu durumda Newton metotu
kullanıyoruz, bu metot $Ax=b$ kısıtlamasında $f(x)$ minimizasyonunu birkaç
adımda çözmeye uğraşıyor, bunu her adımda $f(x)$'e bir karesel yaklaşıklama
yaparak başarıyor. </p>
<p>Sonraki seviye ise iç nokta metotları, eşitlik kısıtlamalarına ek olarak
$h_i(x) \le 0, i=1,..,m$ formunda eşitsizlik kısıtlamaları eklemek.  Bu
derste bu tür problemleri ana-çift yöntemi ile çözeceğiz, daha önceki bir
derste bariyer yöntemi iç nokta metotu ile çözdük. </p>
<p>Genel olarak yaptığımız herhangi bir seviyedeki problemi çözmeye
uğraştığımızda onu bir önce seviyedeki probleme indirgemek, problemi belli
adımlara bölerek her adımda nasıl işlediğini bildiğimiz önceki seviyedeki
tekniği uygulamak. Bariyer metotunda öyle oldu mesela, eşitsizlik
problemini bariyer terimini kritere ekleyerek bilinen Newton adımlarıyla
onu çözmeye uğraştık. </p>
<p>Ana-çift metotu biraz daha farklı olacak. Onu öğrendiğimizde göreceksiniz
ki bir problemi açık bir şekilde başka bir probleme indirgemediğini
göreceksiniz [1, 5:55]. Bu tekniği sarsıma puğratılmış KKT koşulları
ışığında ele almak lazım, ana prensibi bu. </p>
<p>[bariyer metot ozeti atlandi]</p>
<p>Ana-çift metotu, bariyer metodundan farklı olarak, $t$ parametresinin
güncellemeden önce sadece tek Newton adımı atar. Yani ana-çift metotunda da
yaklaşıksallamanın kuvvetini kontrol eden bir $t$ var, ama o belli bir $t$
üzerinden yakınsama oluncaya kadar Newton adımı atmak yerine her $t$ için
tek Newton adımı atılıyor. Bu demektir ki dış döngü, iç döngü farkı yok,
her şey tek bir döngü içinde.</p>
<p>Bir diğer fark ana-çift döngüsünde giderken üretilen (ziyaret edilen)
noktalar illa olurlu olmayabiliyor. Yapısı itibariyle metot döngüsü
sırasında eşitsizlik kısıtlamalarını tatmin eder, fakat her zaman eşitlik
kısıtlamalarını tatmin etmeyebilir. Hatta bazen ikiz olurlu noktalar bile
mevcut olmayabilir, bu daha ciddi bir durum. Hatırlarsak bariyer metotunda
ikiz olurlu nokta her zaman vardı ve bu noktayı bir ikiz boşluğu hesaplamak
için kullanabiliyorduk. Bu boşluğu hesaplamak kolaydı, her noktada
$m / t &lt; \epsilon$ değerindeydi.</p>
<p>O zaman ana-çift ile bu hesap yoksa, ne zaman duracağımızı tam bilmiyoruz
demektir, demek ki akıllıca uydurma (heuristic) yaparak bir durma şartı
bulmamız lazım.</p>
<p>Pozitif bağlamda ana-çift metotları daha verimli çalışır. İspatına
girmeyeceğiz ama ana-çift yakınsaması lineerden daha iyidir.</p>
<p>Negatif olarak ana-çift metotlarını kabaca, sezgisel kavramak bariyer
metotu kadar direk olmayabilir. Şahsen bu alanda araştırmacı olan ben bile
ana-çift metot adımlarının temelii hatırlamakta bazen zorlanıyorum, bariyer
metotunu hatırlamak basit, kısıtlamanın log'unu alıp kritere ekliyorsunuz,
sonra Newton metotu uyguluyorsunuz [13:35].</p>
<p>Detaylara gelelim. KKT koşul sarsımını hatırlarsak, bariyer metotunu KKT
koşullarında bir sarsım olarak görebiliyorduk, şu ifadelerde</p>
<p>$$
\nabla f(x) + \sum_{i=1}^{m} u_i\nabla h_i + A^T v = 0
\qquad (1)
$$</p>
<p>$$
u_i \cdot h_i(x) = (-1/t) 1, \quad i=1,..,m
$$</p>
<p>$$
h_i(x) \le 0, \quad i=1,..,m, \quad Ax = b
$$</p>
<p>$$
u_i \ge 0
$$</p>
<p>Normal sartlarda bloktaki ikinci ifade yerine </p>
<p>$$
u_i \cdot h_i(x) = 0, \quad i=1,..,m
\qquad (2)
$$</p>
<p>olacaktı. Değişen tamamlayıcı gevşeklik yani.</p>
<p>Ana problem neydi?</p>
<p>$$
\min_x f(x) \quad \textrm{öyle ki}
$$
$$
Ax = b
$$
$$
h_i(x) \le 0, \quad i=1,..,m
$$</p>
<p>Bu problemin KKT şartları görülen blokta, durağanlık için gradyan alıp
sıfıra eşitlenir, (1) elde edilir, tabii $f,h_i$'in pürüzsüz ve dışbükey
olduğu farz edilir, o sebeple gradyan yeterli, altgradyana gerek yok, vs.
Tek değiştirdiğimiz tamamlayıcı gevşeklik ve onun artık sıfıra eşit
olmasını şart koşmuyorum, ufak başka bir değere, ve doğru işarete sahip
olan başka bir değere eşit olmasını zorluyorum, $u_i \cdot h_i(x) = (-1/t)$
şartı bu. $1/t$ gibi bir değerin sebebi aslında $\log(x)$'in türevinin
$1/\log(x)$ olmasıyla alakalı, çünkü log bariyerleştirilmiş kriterin
türevini alıp sıfıra eşitleyince ve ikiz değişkenleri uygun şekilde
tanımlayınca log bariyer metotunun orijinal KKT koşulları yerine üstteki
şekilde bir problemi çözülebildiğini görmüştük [1, 16:19], ve $t$
büyütüldükçe görülen değiştirilmiş tamamlayıcı gevşeklik esas versiyonuna
daha da yaklaşıyordu.</p>
<p>Ana-çift metotlarına erişmenin bir diğer yolu sarsımın ortaya çıkarttığı
denklemleri birarada çözmek ve Newton adımını ona göre atmak [1, 22:55]. </p>
<p>Denklemler ayrı ayrı olarak </p>
<p>$$
r_{dual}= \nabla f(x) + Dh(x)^T u + A^T v 
\qquad (6)
$$</p>
<p>$$
r_{cent} = -\mathrm{diag}(u)h(x) - 1/t 
$$</p>
<p>$$
r_{prim} = Ax - b
$$</p>
<p>Sarsım denklem sistemini sıfıra eşitlemek amacıyla matris formunda
düzenlersek, </p>
<p>$$
r(x,u,v) = 
\left[\begin{array}{c}
\nabla f(x) + Dh(x)^T u + A^T v \\
-\mathrm{diag}(u)h(x) - 1/t \\
Ax - b
\end{array}\right]
\qquad (3)
$$</p>
<p>ki</p>
<p>$$
h(x) = \left[\begin{array}{c}
h_1(x) \\
\dots \\
h_m(x)
\end{array}\right]
\quad
D h(x) = \left[\begin{array}{c}
D h_1(x)^T \\
\dots \\
D h_m(x)^T
\end{array}\right]
\qquad (4)
$$</p>
<p>$r(x,u,v)$'yu sıfıra eşitliyoruz, yani bir anlamda </p>
<p>$$
0 = r (x+\Delta x, u + \Delta u, v + \Delta v)
$$</p>
<p>çözülecek, bunu 1. derece Taylor açılımı ile yaklaşıklarım,</p>
<p>$$
\approx r(x,u,v) + D r(x,u,v) \left[\begin{array}{c}
\Delta x \\ \Delta u \\ \Delta v
\end{array}\right]
$$</p>
<p>Üstteki denklemde (3) ve (4) öğelerini kullanarak özyineli şekilde dönersem
gayrı-lineer denklemi çözmüş olurum. Notasyonu biraz degistirirsek,
$y = (x,u,v)$ ile,</p>
<p>$$
0 = r(y + \Delta y) \approx r(y) + D r(y) \Delta y
$$</p>
<p>ve $\Delta y$ için çözmek istiyoruz. </p>
<p>Ya da, genel bir $F$ için $F(y) = 0$ çözümü, yani "kök bulmak" amacıyla
her döngü adımında bir $\Delta y$ hesaplayabilmek istiyoruz. Şu şekilde</p>
<p>$$
F(y + \Delta y) \approx F(y) + D F(y) \Delta y
$$</p>
<p>yaklaşıklarsak, ve kök amaçlı $F(y)=0$ olmalı ama $F(y + \Delta y) = 0$ da
denebilir,</p>
<p>$$
0 \approx F(y) + D F(y) \Delta y
$$</p>
<p>$$
-F(y) =  D F(y) \Delta y
$$</p>
<p>$$
\Delta y = -(DF(y))^{-1} F(y) 
$$</p>
<p>Ya da</p>
<p>$$
DF(y) \Delta y = -F(y) 
$$</p>
<p>Bu problemde $F$ yerine $r$ var. </p>
<p>$$
D r(y) \Delta y = -r(y) 
$$</p>
<p>O zaman (3)'teki $r(y)$'nin türevi, yani Jacobian'ı gerekiyor. Üsttekini
şöyle yazıyoruz, </p>
<p>$$
\left[\begin{array}{ccc}
\nabla^2 f(x) + \sum _{i=1}^{m} u_i \nabla^2 h_i(x) &amp; D h(x)^T &amp; A^T \\
-\mathrm{diag}(u) D h(x) &amp; -\mathrm{diag}(h(x)) &amp; 0 \\
A &amp; 0 &amp; 0
\end{array}\right]
\left[\begin{array}{c}
\Delta x \\ \Delta y \\ \Delta v
\end{array}\right] = 
\left[\begin{array}{c}
r_{dual} \\ r_{cent} \\ r_{prim}
\end{array}\right]<br />
\qquad (5)
$$</p>
<p>Büyük Jacobian'ı nasıl elde ettik? Mesela (3)'ün ilk satırına bakalım, </p>
<p>$$
\nabla f(x) + Dh(x)^T u + A^T v 
$$</p>
<p>var, onun $x,u,v$'ye göre türevlerini almak bize iki üstteki matrisin
1. satır 1. 2. ve 3. kolonunu veriyor, mesela $x$'e göre türev alınca bir
üstteki ifadede 1. ve 2. terimin türevi alınır, $A^T v $ yokolur, bu bize
$\nabla^2 f(x) + \sum _{i=1}^{m} u_i \nabla^2 h_i(x)$ verir [1, 28:43].
Aynı şekilde devam edersek görülen matrisi elde ederiz. Tüm sistemi
$\Delta y$ için çözünce de istediğimiz Newton yönünü elde ederiz.</p>
<p>Bu yönteme ana-çift denmesinin sebebi üstte görülüyor aslında, çünkü dikkat
edersek hem ana hem ikiz değişkenleri aynı sistemde, aynı anda
çözüyoruz. Değil mi? Denklem sistemi KKT koşularının formülize edilmesinden
geldi, ve bu koşullara ana ve ikiz değişkenler aynı yerde mevcuttur, ve
çözerken tüm $x,u,v$ için çözüyoruz. </p>
<p>Not: Bu yaklaşımla bariyer metotuna erişmek mümkün, o durumda sistemden $u$
çıkartılır, ve geri kalanlar çözülür. </p>
<p>Metotu algoritmik olarak görmeden önce bir konudan daha bahsetmek
istiyorum; alternatif ikizlik boşluğu. Bu gerçek ikizlik boşluğu değil,
çünkü daha önce belirttiğimiz gibi bu metotta ikiz değişkenler her zaman
olurlu olmayabiliyor.</p>
<p>Bariyer metotu için ikizlik boşluğu basitti, $m/t$ çünkü
$u_i = -1 / (t h_i(x))$, $i=1,..,m$ tanımlamıştık ve bu ikiz olurlu idi.
Alternatif boşluk için sanki ikiz olurluk varmış gibi yapıyoruz, ve </p>
<p>$$
\eta = -h(x)^T u = - \sum _{i=1}^{m} u_i h_i(x)
$$</p>
<p>hesabını yapıyoruz. Eğer üstteki hesabı bariyer problemi için yapıyor
olsaydık, $u_i = -1/t$ tanımlamış olacaktık ve o zaman bariyer metotu için
olan boşluğu elde edecektik. Ana-çift yönteminde böyle değil tabii, sistemi
çözerken $u_i$ için de çözüm yapıyoruz, onu önceden tanımlamıyoruz, fakat
üstteki formu kullanarak alternatif ikizlik boşluğunu elde edebiliriz.<br />
$\eta$ her zaman pozitif olacak, çünkü kendimizi her zaman $h_i(x) \le 0$
olacak şekilde kısıtlayacağız, ve $u_i \ge 0$ zaten, o zaman çarpımlarının
ekşi ile çarpılması pozitif sonuç verir.</p>
<p>Tüm bunları durma şartı için nasıl kullanırız? Her ne kadar $u_i$'lar
olurlu olmayabilse bile yine de boşluğu hesaplıyoruz, ardından ikiz
değişkenlerin olurluğa ne kadar yakın olduğunu ayrı bir yerde
hesaplıyoruz. Yani eğer alternatif boşluk az, ve olurluğa yakınlık varsa,
akıllıca bir uydurma ile kullanarak durma / durmama kararı verebiliriz.
Gerçi bu teknik uydurmadan biraz daha iyi aslında, ana-çift metotunun
yakınsadığına dair matematiksel ispatlar var, fakat terminolojik olarak bu
boşluk hesabı gerçek bir boşluk hesabı değil.</p>
<p>Artık metotu tanımlayabiliriz. Bir harfiyen olurlu $x^{(0)}$ ile başla,
yani bu nokta $h_i(x^{(0)}) &lt; 0 $, ve $A x^{(0)}= b$. Ayrıca $u^{(0)} &gt; 0$,
$v^{(0)}$ herhangi bir değer. Alternatif ikizlik boşluğu
$\eta^{(0)} = -h(x^{(0)})^T u^{(0)}$ olarak tanımla [1, 45:21].</p>
<p>$t$'yi büyütmek için $\mu &gt; 1$ kullanıyoruz. Her döngü sonunda eski $t$'yi
$\mu$ ile çarpıp yeni $t$ elde edeceğiz.  </p>
<p>Adımlar</p>
<p>1) $t = \mu m / \eta^{(k-1)}$ tanımla. </p>
<p>2) Ana-çift güncelleme yönü $\Delta y$'yi hesapla (nasıl yapılacağını
gördük, (5)'teki lineer sistemi çözerek).</p>
<p>3) Geri iz sürme (backtracking) tekniği ile adım büyüklüğü $s$'yi hesapla
(birazdan nasıl yapılacağını göreceğiz)</p>
<p>4) $y^{(k)} = y^{(k-1)} + s \cdot \Delta y$ ile $y$'yi güncelle. Yani bu
hesapla tüm ana, ikiz değişkenleri güncellemiş oluyoruz, $x,u,v$. </p>
<p>5) Alternatif ikizlik boşluğunu hesapla $\eta^{(k)} = -h(x^{(k)})^T
u^{(k)}$. </p>
<p>6) Ana ve ikiz artıklar ufak ise, yani eğer $\eta^{(k)} &lt; \epsilon$ ise ve 
$(|| r_{prim} ||_2^2 + ||r_{dual}||_2^2)^{1/2} &lt; \epsilon$ ise dur. </p>
<p>$r_{prim}$ hatırlarsak eşitlik sınırlamasından ne kadar uzak
olduğumüz. $r_{dual}$ ise durağanlık şartıydı, onun sıfırdan ne kadar uzak
olduğuydu. Niye ona "ikiz (dual)" etiketi verdik? Bunun ikiz olurluk ile
ne alakası var? Burada biraz nüans var.. </p>
<p>Not: artık kelimesini kullandık daha önce  $r_{dual}$ ikiz artık,
$r_{prim}$ ana artık. </p>
<p>Hatırlarsak $u,v$ üzerindeki kısıtlamalar nelerdi? $u \ge 0$, ve $v$
herhangi bir şey olabilir. Ama dolaylı bir kısıtlama daha var aslında, o da
$u,v$'nin Lagrange iki fonksiyonunun tanım alanında olma zorunluluğu.. Ve
bu kısıtlamalar işte (6)'dan başlayan üç denklemde aslında belirtiliyor. 
Yani, çünkü eğer alttaki sıfır ise</p>
<p>$$
\nabla f(x) + D h(x)^T u + A^T v = 0
$$</p>
<p>bu sadece ve sadece doğru olabilir $x$ eğer $x$ üzerinden $L(x,u,v)$'i
minimize ediyorsa. Ki bu durumda </p>
<p>$$
g(u,v) = L(x,u,v)
$$</p>
<p>doğru olur. Yani $x$ Lagrangian'i minimize ediyorsa, tanım itibariyle
$L(x,u,v)$ ekşi sonsuzluk değildir. Değil mi? Çünkü eksi sonsuzluğa gidiş
olmasın diye ikizde spesifik kısıtlamalar getirdik. Ve bu da demektir ki
$u,v$ Lagrangian'in tanım alanında olmalı.</p>
<p>Geri İz Sürme</p>
<p>Üstteki algoritmada #4 adımında bir adım atıldığını gördük, fakat bu adım
atılırken $s$'nin nasıl bulunacağını anlatmadık. Adım atılırken
$y^+ = y + s \Delta y$ ile, $h_i(x) \le 0$, ve $u_i(x) &gt; 0$ şartlarının
hala geçerli olmasını garantilemek istiyoruz, ve $s$'yi bu olacak şekilde
seçeceğiz. Tabii $y^+ = y + s \Delta y$ derken</p>
<p>$$
x^+ = x + s \Delta y
$$</p>
<p>$$
u^+ = u + s \Delta y
$$</p>
<p>$$
v^+ = v + s \Delta y
$$</p>
<p>demek istiyoruz. Bu seçim şöyle yapılabilir, önce $s$'yi her öge için
$u_i &gt; 0, i=1,..,m$ olacak sekide mümkün en büyük adımdan başlarız. Bu
çözüm kolaydır, çünkü her $u_i$ için her $\Delta u_i$ bizi sıfıra
yaklaştırıyor mu, eğer yaklaştırıyorsa sıfıra gelmeden ne kadar uzağa
gidebiliriz sorusunuz sorabiliriz, ve tüm bu uzaklıklar arasından en ufak
olanı $s$ seçimi için başlayacağımız en büyük uzaklık olacaktır. Tabii
harfiyen olurluk istiyoruz, yani $u &gt; 0$ o zaman bulunan büyüklüğün 0.999'ü
kadarını alırız. </p>
<p>Bu değeri alınca oradan "geriye iz sürmeye" başlarız, yani küçülte
küçülte bu sefer $h$ şartlarını da tatmin eden bir $s$ aramaya
başlayabiliriz. Bu aramayı yaparken $u$ şartını tatmin edeceğimizden
eminizdir çünkü en büyük $s$'yi özellikle $u$ için ayarladık.</p>
<pre><code class="python">import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import numpy.linalg as lin
import scipy.linalg as slin

MAXITERS = 200;
TOL = 1e-6;
m=3;n = 3
RESTOL = 1e-8;
MU = 10;
ALPHA = 0.01;
BETA = 0.5;
x = np.zeros((n,1));

b = np.ones((n,1))*10.
q = np.ones((n,1))*3.

A = np.array( [[2, 3, 5],
               [3, 4, 5],
               [4, 5, 3]] )

P = np.array( [[1, 2, 4],
               [2, 4, 4],
               [1, 1, 1]] )

s = b-np.dot(A,x);
z = 1./s;

for iters in (range(MAXITERS)):
  gap = np.dot(s.T,z)
  res = np.dot(P,x) + q + np.dot(A.T,z)
  if (gap &lt; TOL) &amp; (lin.norm(res) &lt; RESTOL):
      print ('breaking')
      break
  tinv = gap/(m*MU)

  tmp1 = np.hstack((P, A.T))
  tmp2 = np.hstack((A, np.diag(  (-s/z).T[0]  )))
  tmp3 = -np.vstack((tmp1,tmp2))
  tmp4 = np.vstack(( np.dot(P,x)+q+np.dot(A.T,z), -s+tinv*(1.0/z) )) 
  sol = lin.solve(tmp3, tmp4)
  dx = sol[0:n]
  dz = sol[n:n+m]
  ds = -np.dot(A,dx)
  tmp1 = np.dot(P,x)+q+np.dot(A.T,z)
  tmp2 = z*s-tinv
  r = np.vstack((tmp1, tmp2))
  step = np.min([1.0, 0.99/np.max(-dz/z)]);
  while (np.min(s+step*ds) &lt;= 0):
    step = BETA*step
    print (step)

  newz = z+step*dz
  newx = x+step*dx
  news = s+step*ds

  tmp1 = np.dot(P,newx)+q+np.dot(A.T,newz)
  tmp2 = newz*news-tinv
  newr = np.vstack((tmp1,tmp2))
  while (lin.norm(newr) &gt; (1-ALPHA*step)*lin.norm(r)):
    step = BETA*step;
    newz = z+step*dz
    newx = x+step*dx
    news = s+step*ds
    tmp1 = np.dot(P,newx)+q+np.dot(A.T,newz)
    tmp2 = newz*news-tinv
    newr = np.vstack((tmp1,tmp2))

  x = x+step*dx
  z = z +step*dz
  s = b-np.dot(A,x)

print (x)

</code></pre>

<p>[devam edecek]</p>
<p>Kaynaklar</p>
<p>[1] Tibshirani, <em>Convex Optimization, Lecture Video 16 (Part 1)</em>, 
<a href="https://www.youtube.com/channel/UCIvaLZcfz3ikJ1cD-zMpIXg">https://www.youtube.com/channel/UCIvaLZcfz3ikJ1cD-zMpIXg</a>   </p>
<p>[2] Tibshirani, <em>Convex Optimization, Lecture Video 16 (Part 2)</em>, 
<a href="https://www.youtube.com/channel/UCIvaLZcfz3ikJ1cD-zMpIXg">https://www.youtube.com/channel/UCIvaLZcfz3ikJ1cD-zMpIXg</a>   </p>