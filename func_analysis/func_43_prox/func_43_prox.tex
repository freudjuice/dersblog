\documentclass[12pt,fleqn]{article}\usepackage{../../common}
\begin{document}
Proksimal / Yakýnsal Gradyan Metotu (Proximal Gradient Method) 

Bu metot, herhangi bir pürüzsüz olmayan fonksiyonu optimize etmeye uðraþmak
yerine belli bir yapýya uyan çetrefil fonksiyonlarý optimize etmeye
uðraþýr. Bu yapý

$$
f(x) = g(x) + h(x) 
\mlabel{1}
$$

formundadýr [1, 45:59]. $g,h$'in ikisi de dýþbükey. $g$'nin pürüzsüz,
türevi alýnabilir olduðu farz edilir (çoðunlukla oldukca çetrefil olabilir)
ve $\dom(g) = \mathbb{R}^n$, $h$ ise pürüzsüz olmayabilir. Tabii pürüzsüz
artý pürüzsüz olmayan iki fonksýsyon toplamý kriterin tamamýný pürüzsüz
olmayan hale çevirir. Ama bu toplam daha basittir denebilir, onun üzerinde
proksimal operatörü uygulanabilir.

Hatýrlarsak eðer $f$ türevi alýnabilir olsaydý gradyan iniþ güncellemesi 

$$
x^{+} = x - t \cdot \nabla f(x)
$$

Bu formüle eriþmenin bir yöntemi karesel yaklaþýklama üzerinden idi,
$f$'nin $x$ etrafýndaki yaklaþýklamasýnda $\nabla^2f(x)$ yerine
$\frac{1}{t} I$ koyunca,

$$
x^{+} = \arg\min_x f(x) + 
\underbrace{
  \nabla f(x)^T (z-x) + \frac{1}{2t} ||z-x||_2^2 
}_{f_t(z)}
$$

elde ediliyordu. Yani gradyan iniþi sanki ardý ardýna geldiði her noktada
bir karesel yaklaþýklama yapýyor, ve onu adým atarak minimize etmeye
uðraþýyor.

Ama (1)'deki $f$ pürüzsüz deðil, o sebeple üstteki mantýk ise
yaramayacak. Fakat, belki de karesel yaklaþýklamanýn bir kýsmýný hala
kullanabiliriz, ve minimizasyonu sadece $g$'ye uygularýz çünkü pürüzsüz
olan kýsým o. Yani niye $g$'nin yerine karesel yaklaþýklama koymayalým?
Þimdi bunu yapacaðýz [1, 50:10]. 

$$
x^{+} = \arg\min_z \tilde{g}_t(z) + h(z)
$$

ki $\tilde{g}_t(z)$ $g$'nin $x$ noktasý etrafýndaki karesel yaklaþýklamasý
oluyor.  Eðer elimizde $h$ olmasaydý üstteki sadece bir gradyan
güncellemesine indirgenebilirdi. Neyse açýlýmý yaparsak

$$
= \arg\min_z g(x) + \nabla g(x)^T (z-x) + \frac{1}{2t} ||z-x||_2^2 + h(z)
$$

Daha uygun bir formda yazabiliriz, $z$'nin $g$ üzerindeki deðiþikliðe
karesel uzaklýðý olarak,

$$
= \arg\min_z \frac{1}{2t} 
\big|\big| z - \big( x-t\nabla g(x) \big) \big|\big|_2^2 + 
h(z)
$$

Yani söylenmek istenen, hem sadece $g$ olsaydý atacaðýmýz gradyan adýmýna
yakýn durmaya çalýþmak, hem de $h$'nin kendisini ufak tutmak. Dýþarýdan
ayarlanan $t$ parametresi $g$ gradyan adýmý ile $h$ arasýnda bir denge
kurmak gibi görülebilir, eðer $t$ ufak ise o zaman gradyan adýmýna yakýn
durmaya daha fazla önem atfetmiþ olacaðýz, $h$'nin ufak tutulmasýna daha
az. $t$ çok büyük ise tam tersi olacak. 

Bu aslýnda kabaca Proksimal Gradyan Ýniþi yöntemini tarif etmiþ
oluyor. Þimdi proksimal eþlemesi operatörünü göstereceðiz, ki bu
algoritmalarý daha temiz olarak yazmamýza yardýmcý olacak. 

Bir $h$ fonksiyonu için bir $\prox$ operatörü tanýmlýyoruz, 

$$
\prox_{t} (x) = \arg\min_z \frac{1}{2t} ||x-z||_2^2 + h(z)
\mlabel{3}
$$

Üstteki $x$'in bir fonksiyonu olarak $\arg\min$'de gösterilen kriteri
minimize eden $z$'yi buluyor. Operatör $t$'ye baðlý, dýþarýdan verilen
parametre. Tabii ki $h$'ye de baðlý, ki bazen üstteki operatörü
$\prox_{h,t}$ olarak gösteren de oluyor. 

Üstteki ifadenin eþleme olduðunu kontrol edelim. Ciddi tanýmlý bir
fonksiyon üstteki deðil mi? Ona bir $x$ veriyorsunuz, o da size tek bir
sonuç döndürüyor. Ayrýca bu eþleme / fonksiyonun kendisi bir
minimizasyon. Peki bu minimizasyon problemi dýþbükey mi? Evet. Peki bu
problemin özgün bir sonucu var mýdýr? Vardýr çünkü üstteki problem harfiyen
dýþbükey. Deðil mi? Eðer $h$ harfiyen dýþbükey olmasa bile ifadenin tümü
harfiyen dýþbükey olurdu çünkü $\frac{1}{2t} ||x-z||_2^2 $ harfiyen
dýþbükey [1, 55:09], $z-x$'in karesi var [ayrýca $x$ deðiþkeni $h$'ye
geçilmiyor].

Yani harfiyen dýþbükey, o zaman özgün sonuç var. Biz de bu özgün sonucu
alarak bir iniþ algoritmasý yazýyoruz [1, 56:28], 

$$
x^{(k)} = \prox_{t_k} \big( 
x^{(k-1)} - t_k \nabla g(x^{(k-1)})
\big), \quad k=1,2,...
\mlabel{2}
$$

Güncelleme adýmýný tanýdýk þekilde yazmak için 

$$
x^{(k)} = x^{(k-1)} - t_k \cdot G_{t_k} (x^{(k-1)})
$$

ki $G_t$'ye $f$'nin genelleþtirilmiþ gradyaný denebilir,

$$
G_t(x) = \frac{x - \prox_{t}(x - t \nabla g(x))}{t}
$$

$G$'nin dýþbükey fonksiyonlarýn alýþýlageldik gradyanlarýna benzer pek çok
özelliði vardýr, ki bu özellikler proksimal metotlarýn yakýnsadýðýyla
alakalý ispatlarda kullanýlabilir.

Þimdiye kadar anlattýklarýmýza bakanlara bu komik bir hikaye gibi
gelebilir. Bir $g + h$ toplamýný minimize etmek istiyordum, bunu yapabilmek
için (2) formunda adýmlar atacaðým, bir $\prox$ operatörüm var, ama bu
operatör bir sonuç döndürüyor aslýnda, ve bu sonuç bir baþka
minimizasyondan geliyor. Yani $g + h$ türü bir toplam minimizasyonu yerine
her adýmda, bir sürü minimizasyonlarý koymuþ oldum. Bu nasýl daha iyi bir
sonuç verecek ki?

Þunu belirtmek lazým, sadece eðer proksimal gradyanlarý analitik, ya da
hýzlý bir þekilde hesaplayabiliyorsak onlarý çözüm için düþünürüz. Yani her
ne kadar her adýmda (3) turu optimizasiyonlar yapýyorsak ta, bunu pürüzsüz
olan kýsým $h$ yeterince basit olduðu zaman yapýyoruz ki tüm (3) için
analitik  ya da hýzlý hesapsal çözüm olsun [1, 58:54]. 

Diðer noktalar, dikkat edersek proksimal operatör $g$'ye baðlý deðil,
tamamen $h$ bazlý. Eðer $h$ basit ise ama $g$ müthiþ çetrefil ise bu
proksimal hesaplarýný çok zorlaþtýrmýyor. Eðer o çok çetrefil (ve pürüzsüz)
$g$ için gradyan hesaplanabiliyorsa, durumu kurtardýk demektir. 

Tekrar bir Lasso problemi göreceðiz. Bu Lasso için gördüðümüz ikinci
algoritma, ve belirtmek gerekir ki Proksimal metot altgradyan metotuna göre
çok daha verimlidir, hýzlýdýr.

Verili bir $y \in \mathbb{R}^n$, $X \in \mathbb{R}^{n \times p}$ için Lasso
kriterini hatýrlarsak, 

$$
f(\beta) = \frac{1}{2} || y - X \beta ||_2^2 + \lambda ||\beta||_1 
$$

Kriterdeki ilk terim en az kareler kayýp fonksiyonu, ikinci terim bir ayar
parametresi üzerinden katsayýlarýn 1. normu. Bu kriteri pürüzsüz, ve
pürüzsüz olmayan ama basitçe olan iki kýsma ayýracaðýz, yani zaten oldukca
bariz, pürüzsüz kýsým 1. terim, $g(\beta)$ diyelim, olmayan 2. terim,
$h(\beta)$ diyelim. Proksimal gradyan iniþi için bize iki þey gerekiyor,
birincisi $g$'nin gradyaný, ikincisi $h$ için $\prox$ operatörünü
hesaplayabilmek.

$g$'nin gradyaný oldukca basit, onu bu noktada uykumuzda bile bulabiliyor
olmamýz lazým. $h$'nin $\prox$ operatörü,

$$
\prox_t(\beta) = \arg\min_z \frac{1}{2t} ||\beta-z||_2^2 + \lambda ||z||_1
$$

Her þeyi $t$ ile çarparsam,

$$
\prox_t(\beta) = \arg\min_z \frac{1}{2} ||\beta-z||_2^2 + \lambda t ||z||_1
$$

Üstteki minimizasyonun çözümünü daha önce altgradyanlar üzerinden
görmüþtük, 

$$
= S_{\lambda t}(\beta)
$$

Yine yumuþak eþikleme (soft-threshold) operatörüne gelmiþ olduk, 

$$
[ S_{\lambda} (\beta) ]_i = 
\left\{ \begin{array}{ll}
\beta_i - \lambda & \textrm{eðer } \beta_i > \lambda_i \\
0 & \textrm{eðer } -\lambda \ge \beta_i \ge \lambda, \quad i=1,..,n \\
\beta_i + \lambda & \textrm{eðer } \beta_i < -\lambda_i 
\end{array} \right.
$$

Tüm algoritma neye benziyor? Önce $g$'ye göre bir graydan güncellemesi
yaparým, gradyan

$$
\nabla g(\beta) = -X^T (y-X\beta)
$$

O zaman güncelleme 

$$\beta + t X^T (y-X\beta)$$ 

olur. Buna $\prox$ uygularsak,

$$
\beta^+ = S_{\lambda t}(\beta + t X^T (y-X\beta))
$$

Yumuþak eþikleme ne yapar? Her ögeye teker teker bakar, eðer mutlak deðeri
çok ufaksa onu ya sýfýra eþitler, ya da onu $\lambda \cdot t$ kadar sýfýra
yaklaþtýrýr. 

Üstteki Lasso algoritmasina ISTA adý da verilir. 

Geriye Çizgisel Ýz Sürme  (Backtracking line search)

Graydan iniþinde görmüþtük ki adým büyüklüklerini dinamik olarak
seçebiliyorduk, her adýmdaki duruma adapte olabiliyorduk, tipik olarak
Lipschitz sabitini bilmiyoruz çünkü. O sebeple geriye iz sürme pratikte
iyi iþlemesiyle beraber, teorik olarak yakýnsamayý da garantiliyordu. 

Proksimal gradyanlarý için benzer bir kavram geçerli. Ayrýca proksimal
durumda geriye doðru iz sürmenin birden fazla yolu var [1, 1:10:23]. Ben
sadece $g$ üzerinde iþlem yapan bir yöntem seçtim, çünkü bu metotu
hatýrlamasý daha kolay. Gradyan iniþi için iz sürmeyi hatýrlarsak, $x$
noktasýndayýz diyelim ve $x-t \nabla g$ yönünde gitmek istiyoruz, o zaman
alttakinin doðru olup olmadýðýný kontrol ediyorduk, 

$$
f(x - t\nabla f(x) ) > f(x) - \frac{t}{2} ||\nabla f||_2^2 
$$

Proksimal gradyan için benzer bir yöntem [1:11:57], ve $G$ üzerinde iþlem
yapýyoruz dikkat, $h$ deðil, yani $G$ üzerinden yeterince ``iniþ'' yapmaya
uðraþacaðýz, $g+h$ deðil. Normal iz sürmede üsttekinin doðruluðunu kontrol
ediyoruz, doðru ise $t$'yi belli bir ölçüde ufaltýyoruz. Doðru deðilse yani
yeterince iniþ yaptýysak, o zaman eldeki deðerlerle güncellemeyi yapýyoruz.

$$
g(x - t G_t (x) ) > g(x) - t \nabla g(x)^T G_x(x) + \frac{t}{2} ||G_x(x)||_2^2
$$

Yani gradyan güncellemesi 1. derece Taylor güncellemesinden geldi. Üstteki
ifadede de eðime ek olarak 1. derece Taylor güncellemesine göre de
yeterince iniþ yapmýþ olmak istiyorum. Dikkat edersek eðer $G$ yerine
$g$'nin gradyanini koyarsam, iki üstteki formüle benzer bir formül elde
ederim.

Geriye iz sürme genel algoritmasi þöyle, bir $0 < \beta < 1$ paremetresi
var (dýþarýdan ayarlanan bir parametre). $t=1$ ile baþlýyoruz, ve üstteki
formülü iþletiyoruz, eðer gerekiyorsa $t = \beta t$ ile küçültme
yapýyoruz. Ýz sürme bitince bulduðumuz $t$ ile güncelleme yapýyoruz [3,
07:35].

[bazi ek detaylar atlandi]

Matris Tamamlamasi

Þimdi prox operatörü sofistike olan bir örnek görelim [3, 11:38]. Buradan
çýkan algoritma ilginç olacak. 

Bize bir $Y \in \mathbb{R}^{m \times n}$ matrisi veriliyor ama biz sadece
bu matrisin bazý öðelerini görebiliyoruz, bu öðeler
$Y_{i,j}, (i,j) \in \omega$ ile belirtiliyor ki $\omega$ belli bir indis
kümesidir. Bu problem bir tavsiye sistemi olabilir, matrisin tamamý bir
ideal müþteri / ürün eþlemesidir, biz sadece bu matrisin belli bir kýsmýný
görüyoruz (tipik olarak mevcut müþterilerin tarihi veride yaptýðý alýmlar,
ürünler üzerindeki beðendi/beðenmedi yorumlarý matrisin ``görünen'' kýsmýný
temsil edebilir). 

Bu tür problemleri iz norm regülarizasyonu (trace norm regularization)
problemi olarak görmenin iyi iþlediði görülmüþtür. Bu problem aslýnda daha
önce gördüðümüz Lasso problemine benzer, onun matrisler için olan formudur
bir açýdan. Problem,

$$
\min_B \frac{1}{2} \sum _{(i,j) \in \omega} (Y_{ij} - B_{ij})^2 + \lambda ||B||_{tr}
\mlabel{4}
$$

ki $||B||_{tr}$ iz (ya da nükleer) normudur, 

$$
||B||_{tr} = \sum _{i=1}^{r} \sigma_i(B)
$$

ile gösterilir, $r$ kertedir, $r = \rank(B)$, ve herhangi bir $X$ matrisi
için $\sigma_1(X) \ge .. \ge \sigma_r(X) \ge 0$, $X$'in eþsiz (singular)
deðerleridir.

Minimizasyon ifadesindeki ilk toplam, $B$'deki deðerleri zaten görülen,
bildiðimiz deðerlere $Y$'ye karesel kayýp yakýn tut diyor, ve ona bir ayar
parametresi üzerinden $B$'nin iz normunu ekliyoruz, bu istatistiki baðlamda
bir tür regülarizasyon yapmýþ oluyor. Eðer bu ek olmsaydý problem kötü
konumlanmýþ (ill-posed) olurdu [3, 15:06]. Eðer o terim olmasaydý ve
$\lambda$ sýfýr olsaydý o zaman optimizasyon $B$'yi $Y$'ye eþitlerdik, oldu
bitti derdik, ama o zaman hiçbir iþ yapmamýþ olurduk. Saðdaki terim ekiyle
yapmaya uðraþtýðýmýz $Y$'ye olabildiðince yakýn bir $B$ seçmek ve bu
$B$'nin düþük kerte olmasýný zorlamak. Bu bir regülarizasyon yöntemi,
diyelim $B$ þu þekilde 

$$
\underbrace{B}_{m \times n} = 
\underbrace{U}_{m \times k}
\underbrace{V^T}_{k \times n}
$$

yani $B$'yi güya oluþturan birer $U,V$'nin boyutlarýndaki $k$'nin
olabildiðince düþük olmasýný istiyoruz. 

Bir diðer bakýþ açýsý, daha önceki Lasso'yla ilintilendirmek baðlamýnda, iz
normu L1-normun matrisler için olan versiyonu olarak görmek. Eðer elimde
bir köþegen matris olsaydý iz köþegendeki öðelerin (tek öðeler onlar)
toplamý olurdu, ki bu bir vektörün 1-norm'unu almak gibi deðil mi? Eðer
köþegeni bir vektör gibi görürsem, ve matriste bu vektörden baþka bir þey
yoksa.. baðlantýyý görüyoruz herhalde. 

Bir anlamda $||B||_{tr}$ $B$ matrisinin kertesini yaklaþýksal olarak temsil
ediyor çünkü kerte bir matrisin sýfýr olmayan eþsiz deðerlerinin
sayýsýdýr. $||B||_{tr}$ tabii ki eþsiz deðerlerinin sayýsý deðil onlarýn
kendilerinin toplamý, ama yaklaþýksal olarak kullanabileceðimiz bir þey bu
y [3, 17:49] , çünkü deðer toplamý dýþbükey [altgradyan, türevi
alýnabildiði için bu yaklaþýklýk seçilmiþ herhalde].  (4) problemi tamamen
dýþbükey bu arada, ilk terim dýþbükey, ve ikinci terim düzgün bir norm'dur
o zaman dýþbükeydir.

Bilimciler proksimal gradyan kullanmadan önce bu problem zor bir
problemdi. Optimizasiyon problemi bir yarý kesin (semidefinite) program
olarak tanýmlanýr. Eskiden bu tür problemleri iç nokta teknikleri ile
çözüyorduk, ve bu teknikler üstteki gibi problemler üzerinde oldukca
yavaþtýr. 

Devam edelim, problemi þöyle hazýrlarýz, bir yansýtma operatörü $P_\Omega$
tanýmlayalým, gözlenen kümeye yansýtma yapacaðýz, 

$$
[P_\Omega (B) ]_{ij} = 
\left\{ \begin{array}{ll}
B_{ij} & (i,j) \in \Omega \\
0 & (i,j) \notin \Omega 
\end{array} \right.
$$

Bu operatörün yaptýðý verilen bir matrisin gözlem olmayan her öge için
deðeri sýfýr yapmak, yoksa olduðu gibi býrakmak. Simdi kriteri yazabiliriz, 

$$
f(B) = \frac{1}{2} || P_\Omega (Y) - P_\Omega(B) ||_F^2 + \lambda ||B||_{tr}
$$










Ekler

Örnek kod, Lasso problem çözümü [2], pür proksimal gradyan iniþi, iz sürme yok

\begin{minted}[fontsize=\footnotesize]{python}
import pandas as pd
diabetes = pd.read_csv("../../stat/stat_120_regular/diabetes.csv",sep=';')
y = np.array(diabetes['response'].astype(float)).reshape(442,1)
X = np.array(diabetes.drop("response",axis=1))
N,dim = X.shape
print (N,dim)

lam = 1/np.sqrt(N);
w = np.matrix(np.random.multivariate_normal([0.0]*dim, np.eye(dim))).T

L = (np.linalg.svd(X)[1][0])**2
print(L)
max_iter = 500

def obj(w):
    r = X*w-y;
    return np.sum(np.multiply(r,r))/2 +  lam * np.sum(np.abs(w))

def f_grad(w):
    return  X.T*(X*w-y) 

def soft_threshod(w,mu):
    return np.multiply(np.sign(w), np.maximum(np.abs(w)-mu,0))  

w = np.matrix([0.0]*dim).T
for t in range(0, max_iter):
    obj_val = obj(w)
    w = w - (1/L)* f_grad(w)
    w= soft_threshod(w,lam/L)    
    if (t % 50==0):
        print('iter= {},\tobjective= {:3f}'.format(t, obj_val.item()))

print (w)
\end{minted}

\begin{verbatim}
442 10
4.0242141761466925
iter= 0,	objective= 6425460.500000
iter= 50,	objective= 5751070.568959
iter= 100,	objective= 5750285.357193
iter= 150,	objective= 5749670.506866
iter= 200,	objective= 5749177.635558
iter= 250,	objective= 5748779.527464
iter= 300,	objective= 5748457.810485
iter= 350,	objective= 5748197.804952
iter= 400,	objective= 5747987.670443
iter= 450,	objective= 5747817.840900
[[  -8.71913404]
 [-238.35531517]
 [ 522.93302022]
 [ 323.11825944]
 [-526.09642955]
 [ 265.58097894]
 [ -17.84381222]
 [ 143.15165377]
 [ 652.14114865]
 [  68.55685031]]
\end{verbatim}

Kaynaklar

[1] Tibshirani, {\em Convex Optimization, Lecture Video 8}, 
\url{https://www.youtube.com/channel/UCIvaLZcfz3ikJ1cD-zMpIXg}

[2] He, {\em IE 598 - Big Data Optimization},  
    \url{http://niaohe.ise.illinois.edu/IE598_2016/}

[3] Tibshirani, {\em Convex Optimization, Lecture Video 9}, 
\url{https://www.youtube.com/channel/UCIvaLZcfz3ikJ1cD-zMpIXg}


\end{document}




