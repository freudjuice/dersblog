\documentclass[12pt,fleqn]{article}\usepackage{../../common}
\begin{document}
Autograd ile Optimizasyon

Otomatik türevin nasýl iþlediðini [1] yazýsýnda gördük. Programlama dilinde
yazýlmýþ, içinde \verb!if!, \verb!case!, hatta döngüler bile içerebilen
herhangi bir kod parçasýnýn türevini alabilmemizi saðlayan otomatik türev
almak pek çok alanda iþimize yarar. Optimizasyon alaný bunlarýn baþýnda
geliyor. Düþünürsek, eðer sembolik olarak türev almasý çok çetrefil bir
durum varsa, tasaya gerek yok; bir fonksiyonu kodlayabildiðimiz anda onun
türevini de alabiliriz demektir.

Önce bazý genel optimizasyon konularýný iþleyelim. 

Sýnýrlanmamýþ optimizasyonda (unconstrained optimization) $f(x)$
fonksiyonunu minimum deðerde tutacak $x$ deðerini bulmaya uðraþýyoruz, ki
$x$ tek boyutlu skalar, ya da çok boyutlu $x \in \mathbb{R}^n$
olabilir. Yani yapmaya uðraþtýðýmýz

$$
\min_x f(x)
$$

iþlemi. Peki minimumu nasýl tanýmlarýz? Bir nokta $x^*$ global minimize
edicidir eðer tüm $x$'ler için $f(x^*) \le f(x)$ ise, ki
$x \in \mathbb{R}^n$, en azýndan $x$ modelleyeni ilgilendiren tüm küme
öðeleri için.

Fakat çoðu zaman bir global $f$'i kullanmak mümkün olmayabilir, fonksiyon
çok çetrefil, çok boyutlu, bilinmez durumdadýr, ve elimizde sadece yerel
bilgi vardýr. Bu durumda üstteki tanýmý ``bir $N$ bölgesi içinde'' olacak
þekilde deðiþtiririz ki bölge, $x^*$ etrafýndaki, yakýnýndaki bölgedir.

Üstteki tanýmý okuyunca  $x^*$'in yerel minimum olup olmadýðýný anlamanýn
tek yolunun yakýndaki diðer tüm noktalara teker teker bakmak olduðu anlamý
çýkabilir, fakat eðer $f$ pürüzsüz bir fonksiyon ise yerel minimumu
doðrulamanýn çok daha hýzlý bir yöntemi vardýr. Hatta ve hatta eðer
fonksiyon $f$ iki kez türevi alýnabilir haldeyse $x^*$'in yerel minimum
olduðunu ispatlamak daha kolaylaþýr, $\nabla f(x^*)$ ve Hessian $\nabla^2
f(x^*)$'e bakarak bunu yapabiliriz.

Pürüzsüz fonksiyonlarýn minimize edicilerini incelemekte kullanacaðýmýz
araç Taylor'un teorisi olacak. 

Taylor'un Teorisi

$f: \mathbb{R}^n \to \mathbb{R}$'nin sürekli türevi alýnabilir olduðunu farz
edelim ve $p \in \mathbb{R}^n$ olacak þekilde,

$$
f(x + p) = f(x) + \nabla f(x + tp)^T p
$$

ki $t \in (0,1)$. Ayrýca eðer $f$ iki kez türevi alýnabilir halde ise,

$$
\nabla f(x+p) = \nabla f(x) + \int_{0}^{1}\nabla^2 f(x+tp) p \ud t
$$

ve 

$$
f(x+p) = f(x) + \nabla f(x)^T p + \frac{1}{2} p^T \nabla^2 f(x+tp) p
$$


[devam edecek]

Kaynaklar 

[1] Bayramlý, Ders Notlarý, {\em Otomatik Türev Almak (Automatic Differentiation -AD-)}

[3] \url{https://nikstoyanov.me/post/2019-04-14-numerical-optimizations}

[4] \url{https://rlhick.people.wm.edu/posts/mle-autograd.html}

[5] \url{http://kitchingroup.cheme.cmu.edu/blog/2018/11/03/Constrained-optimization-with-Lagrange-multipliers-and-autograd/}


\end{document}
