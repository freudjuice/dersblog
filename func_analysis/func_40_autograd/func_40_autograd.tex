\documentclass[12pt,fleqn]{article}\usepackage{../../common}
\begin{document}
Autograd ile Optimizasyon

Otomatik türevin nasýl iþlediðini [1] yazýsýnda gördük. Programlama dilinde
yazýlmýþ, içinde \verb!if!, \verb!case!, hatta döngüler bile içerebilen
herhangi bir kod parçasýnýn türevini alabilmemizi saðlayan otomatik türev
almak pek çok alanda iþimize yarar. Optimizasyon alaný bunlarýn baþýnda
geliyor. Düþünürsek, eðer sembolik olarak türev almasý çok çetrefil bir
durum varsa, tasaya gerek yok; bir fonksiyonu kodlayabildiðimiz anda onun
türevini de alabiliriz demektir.

Önce bazý genel optimizasyon konularýný iþleyelim. 

Sýnýrlanmamýþ optimizasyonda (unconstrained optimization) $f(x)$
fonksiyonunu minimum deðerde tutacak $x$ deðerini bulmaya uðraþýyoruz, ki
$x$ tek boyutlu skalar, ya da çok boyutlu $x \in \mathbb{R}^n$
olabilir. Yani yapmaya uðraþtýðýmýz

$$
\min_x f(x)
$$

iþlemi. Peki minimumu nasýl tanýmlarýz? Bir nokta $x^*$ global minimize
edicidir eðer tüm $x$'ler için $f(x^*) \le f(x)$ ise, ki
$x \in \mathbb{R}^n$, en azýndan $x$ modelleyeni ilgilendiren tüm küme
öðeleri için.

Fakat çoðu zaman bir global $f$'i kullanmak mümkün olmayabilir, fonksiyon
çok çetrefil, çok boyutlu, bilinmez durumdadýr, ve elimizde sadece yerel
bilgi vardýr. Bu durumda üstteki tanýmý ``bir $N$ bölgesi içinde'' olacak
þekilde deðiþtiririz ki bölge, $x^*$ etrafýndaki, yakýnýndaki bölgedir.

Üstteki tanýmý okuyunca  $x^*$'in yerel minimum olup olmadýðýný anlamanýn
tek yolunun yakýndaki diðer tüm noktalara teker teker bakmak olduðu anlamý
çýkabilir, fakat eðer $f$ pürüzsüz bir fonksiyon ise yerel minimumu
doðrulamanýn çok daha hýzlý bir yöntemi vardýr. Hatta ve hatta eðer
fonksiyon $f$ iki kez türevi alýnabilir haldeyse $x^*$'in yerel minimum
olduðunu ispatlamak daha kolaylaþýr, $\nabla f(x^*)$ ve Hessian $\nabla^2
f(x^*)$'e bakarak bunu yapabiliriz.

Minimallik için 1. ve 2. derece þartlar var. 1. derece gerekli þart (ama
yeterli deðil) $\nabla f = 0$ olmasý. Bu standard Calculus'tan bildiðimiz
bir þey, minimum ya da maksimumda birinci türev sýfýrdýr. Ama türevin sýfýr
olup minimum ya da maksimum olmadýðý durum da olabilir, mesela
$f(x) = x^3$. $f'(0) = 0$'dir fakat $x=0$ ne maksimum ne de
minimumdur. Daha iyi bir termioloji $\nabla f = 0$ noktalarýný {\em kritik
  nokta} olarak tanýmlamaktýr. $x=0$ noktasýnda bir deðiþim oluyor, bu
deðiþim kritik bir deðiþim, her ne kadar minimum ya da maksimum olmasa da.

\begin{minted}[fontsize=\footnotesize]{python}
x = np.linspace(-3,3,100)
plt.plot(x,x**3)
plt.grid(True)
plt.savefig('func_40_autograd_01.png')
\end{minted}

\includegraphics[height=6cm]{func_40_autograd_01.png}

Bir kritik noktanýn yerel maksimum ya da yerel minimum olup olmadýðýný
anlamak için fonksiyonun ikinci türevine bakabiliriz. Bir
$f: \mathbb{R}^n \to \mathbb{R}$ var ve $x^*$ noktasýnýn kritik nokta
olduðunu düþünelim, yani $\nabla f(x^*) = 0$. Þimdi çok ufak bir $h$ adýmý
için $f(x^* + h)$'a ne olduðuna bakalým. Burada Taylor açýlýmý
kullanabiliriz [2], 

$$
f(x + h^*) = 
f(x^*) + \nabla f(x^*) h + 
\frac{1}{2} h^T f(x^*) \nabla^2 (x^*) f(x^*) h + 
O(3)
$$

$\nabla^2 (x^*)$  bir matristýr içinde $f$'nin ikinci derece türevleri
vardýr [6]. 


[devam edecek]

Kaynaklar 

[1] Bayramlý, Ders Notlarý, {\em Otomatik Türev Almak (Automatic Differentiation -AD-)}

[2] Schrimpf, \url{http://faculty.arts.ubc.ca/pschrimpf/526/526.html}

[3] \url{https://nikstoyanov.me/post/2019-04-14-numerical-optimizations}

[4] \url{https://rlhick.people.wm.edu/posts/mle-autograd.html}

[5] \url{http://kitchingroup.cheme.cmu.edu/blog/2018/11/03/Constrained-optimization-with-Lagrange-multipliers-and-autograd/}

[6] Bayramli, Cok Boyutlu Calculus, {\em Vektör Calculus, Kurallar, Matris Türevleri}


\end{document}
